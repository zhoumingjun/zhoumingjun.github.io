{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","description":"personal blog"}}},"pageContext":{"posts":[{"internal":{"content":"\n# Introduction\nThis is following the idea of https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#generating-names-with-a-character-level-rnn\n\n# Task definition\nIn training phase, It's a kind of many-to-many task.\n![task]https://i.imgur.com/JH58tXY.png\n\nThe `(input, label)`  is `(name X, X[1:]+\"<EOS>\")`, here we use '-' as `<EOS>`   \ne.g.    \n`(Cotton, otton-)`    \n`(Alex, lex-)`\n`(Doolin, oolin-)`\n\nIn prediction phase, we use the result of the last iteration as the input of the current iteration.\n\n# Details\n\n## dataset & dataloader\n```python\n\n# dataset\nclass NamesDataset(Dataset):\n    def __init__(self, data_path, transforms=[]):\n        inputs = []\n        filepath = path.join(data_path)\n\n        with open(filepath) as f:\n            lines = f.readlines()\n            inputs += [line.strip() for line in lines]\n\n        self.inputs = inputs\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input = self.inputs[idx]\n        for transform in self.transforms:\n            input = transform(input)\n        return input\n\n\nclass UnicodeToAscii(object):\n    def __init__(self, letters):\n        self.letters = letters\n\n    def __call__(self, s):\n        return ''.join(\n            c for c in unicodedata.normalize('NFD', s)\n            if unicodedata.category(c) != 'Mn'\n            and c in self.letters\n        )\n\n\nnamesDataset = NamesDataset('./data/names/English.txt', transforms=[UnicodeToAscii(all_letters)])\n\n# dataloader\nclass NamesClassifier(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NamesClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\n        self.linear = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        x, hidden = self.lstm(x, hx=hidden)\n        x = x.view(-1, hidden_size)\n        x = self.linear(x)\n        return F.log_softmax(x, dim=1), hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\n\ndataloader = DataLoader(namesDataset, batch_size=1, shuffle=True, num_workers=4, drop_last=True)\n\n```\n\n## sample  generation\njust use the original code with minor changes\n\n``` python\nmax_length = 20\ndef sample(start_letter='A'):\n    with torch.no_grad():  # no need to track history in sampling\n        input = name2tensor(start_letter).view(-1, 1, input_size)\n        output_name = start_letter\n        hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1,1,hidden_size))\n\n        for i in range(max_length):\n            output, hidden = model(input,hidden)\n            topv, topi = output.topk(1)\n            topi = topi[0][0]\n            if topi == n_letters - 1:\n                break\n            else:\n                letter = all_letters[topi]\n                output_name += letter\n\n            input = name2tensor(letter).view(-1, 1, input_size)\n\n        return output_name\n\n\n# Get multiple samples from one category and multiple starting letters\ndef samples(start_letters='ABC'):\n    for start_letter in start_letters:\n        print(sample(start_letter))\n\n```\n## model\nThe model is different from the last post.\nHere we need to return the hidden for the next iteration.\n\n```python\nclass NamesClassifier(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NamesClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\n        self.linear = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden):\n        x, hidden = self.lstm(x, hx=hidden)\n        x = x.view(-1, hidden_size)\n        x = self.linear(x)\n        return F.log_softmax(x, dim=1), hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\n```\n## training\nThe training code is straightforward\n```python\n\ninput_size = n_letters\nhidden_size = 50\noutput_size = input_size\n\n# define model/optimizer/criterion\nmodel = NamesClassifier(input_size, hidden_size, output_size)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.NLLLoss()\n\nfor epoch in range(20):\n\n    loss_sum = 0;\n    nRound = 0\n    for i_batch, batch in enumerate(dataloader):\n\n        inputs = batch\n\n        for idx, input in enumerate(inputs):\n            label = input[1:] + all_letters[-1]\n\n            optimizer.zero_grad()\n\n            input = name2tensor(input).view(-1, 1, input_size)\n            label = [all_letters.index(x) for x in label]\n\n            output, _ = model(input,  None)\n            loss = criterion(output, torch.LongTensor(label))\n\n            loss.backward()\n            optimizer.step()\n\n            loss_sum += loss\n            nRound += 1\n\n    print(\"epoch {} i_batch {} loss {}\".format(epoch, i_batch, loss_sum / nRound))\n    samples()\n\n\"\"\"\nepoch 0 i_batch 3667 loss 2.161202907562256\nAlling\nBoule\nColler\nepoch 1 i_batch 3667 loss 1.931467890739441\nAlling\nBrand\nCoode\nepoch 2 i_batch 3667 loss 1.8524298667907715\nAlling\nBarring\nCower\nepoch 3 i_batch 3667 loss 1.7972185611724854\nAllins\nBart\nCorrin\nepoch 4 i_batch 3667 loss 1.7545592784881592\nAllam\nBarrer\nCorright\nepoch 5 i_batch 3667 loss 1.7233166694641113\nAllam\nBreatt\nCrowe\nepoch 6 i_batch 3667 loss 1.7019587755203247\nAllam\nBarris\nColley\nepoch 7 i_batch 3667 loss 1.6757022142410278\nAllam\nBarrett\nCorner\nepoch 8 i_batch 3667 loss 1.6573164463043213\nAllan\nBarkin\nColler\nepoch 9 i_batch 3667 loss 1.6444448232650757\nAllam\nBarr\nCorrigan\nepoch 10 i_batch 3667 loss 1.629043459892273\nAllam\nBray\nColley\nepoch 11 i_batch 3667 loss 1.618332028388977\nAllam\nBarr\nColling\nepoch 12 i_batch 3667 loss 1.6057300567626953\nAllam\nBarty\nColley\nepoch 13 i_batch 3667 loss 1.600183129310608\nAllington\nBrand\nColling\nepoch 14 i_batch 3667 loss 1.5862054824829102\nAllam\nBurne\nCrowther\nepoch 15 i_batch 3667 loss 1.5789239406585693\nAllan\nBurner\nColl\nepoch 16 i_batch 3667 loss 1.5749832391738892\nAllam\nBarron\nColley\nepoch 17 i_batch 3667 loss 1.5670925378799438\nAllam\nBarr\nCower\nepoch 18 i_batch 3667 loss 1.56125009059906\nAllam\nBarr\nCorre\nepoch 19 i_batch 3667 loss 1.5633057355880737\nAllam\nBarr\nCollam\n\n\"\"\"\n```\n\n# The source code \n\nThe soure code can be found on [github](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/5.char-rnn-generation.py)\n\n## 散文生成 \n\n[fiction genration](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/6.fiction-generation.py)\n学习<<荷塘月色>>的结果\n\nepoch 150  loss 0.0017014490440487862\n> 恐从了很，可有，而六朝时上，这时候荷塘塘。这路浮起在这是的叶子和花上一路过热闹的，我爱热闹，也爱冷静；爱群居，也爱独处。像今晚上，一个人，背着艳歌去的。有着一些颜色；而叶子却更见风致了。月光如隔了树梢上\n\nepoch 200  loss 0.0010049326810985804\n> 恐也是一个人静树，在这满月的光里，总该另有一番样子吧。月亮渐渐地升高了，墙外马路上孩子们的欢笑，已经听不见了；妻在屋里拍着闰儿，迷迷糊糊地哼着眠歌。我悄不为月光留下的。树色一例是阴阴，，乍看像一团烟雾；\n\nepoch 250  loss 0.0006684782565571368\n> 恐始荷塘的，有看像今晚上，一旁，像着睡痕的。路上一旁，是一，的光，花也有一样；妻在屋里拍。塘的，是个人。树色一例是阴阴的，乍看像一团烟雾；但杨柳的丰姿，便在烟雾里也辨得出。树梢上蝉声与水的的蛙声；但热闹\n\n"},"fields":{"fpath":"/series/pytorch/rnn/3-charnn-name-generation/","permalink":"/2018/09/11/generating-names-with-a-character-level-rnn","category":"series"},"frontmatter":{"title":"Generating Names with a Character-Level RNN","date":"2018-09-11","tags":["machine learning","pytorch","rnn"],"desc":null,"slug":null}},{"internal":{"content":"\n# Introduction\nThe pytorch's official tutorial introduces the character-level RNN\nhttps://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n\nHere I will implement it by using lstm\n\n# Points\n1. Classifying Names is a kind of many-to-one RNN task\n2. The Inputs' length are variable  \n3. It's better to handle the data in batch to boost the training\n4. Use dataset and dataloader to handle the raw data\n   \n\n# Task definition\nTraining data is collection of (name, language)\nThe task is to preidct the language of the given name\n\n# Details\n\n## Raw data\nDownload data from https://download.pytorch.org/tutorial/data.zip\n\n## Unicode & ascii\nplease read my previous post about [unicode](https://zhoumingjun.github.io/note/unicode)    \nwe will transform the names into NFD (Normalization Form Decomposition) and filter all non-ascii characters\n\n## Dataset and dataloader\nThe dataset is a helper class to load data.\n\n```python\nclass NamesDataset(Dataset):\n    def __init__(self, data_dir, transforms=[]):\n        self.data_dir = data_dir\n\n        all_langs = []\n        inputs = []\n        labels = []\n\n        for filepath in glob.glob(path.join(data_dir, \"*\")):\n            lang = os.path.splitext(os.path.basename(filepath))[0]\n            if not lang in all_langs:\n                all_langs.append(lang)\n\n            label = all_langs.index(lang)\n\n            with open(filepath) as f:\n                lines = f.readlines()\n                inputs += [line.strip() for line in lines]\n                labels += [label] * len(lines)\n\n        self.all_langs = all_langs\n        self.inputs = inputs\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n\n        item = self.inputs[idx]\n        for transform in self.transforms:\n            item = transform(item)\n\n        return item, self.labels[idx]\n\n    def getLangs(self):\n        return self.all_langs\n```\n\n## Model\nThe model is straitforward.\n1. use lstm to compute the output\n2. use linear to map the output's feature to language classes\n3. use crossentrophy(log_softmax + NLLLoss) to do classification\n   \n*attention, here we use packed sequece as the input* \n\n```python\nclass NamesClassifier(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NamesClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\n        self.linear = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x, hidden = self.lstm(x)\n        output_unpacked, unpack_outputs_length = pad_packed_sequence(x)\n\n        seqs = unpack_outputs_length - 1\n        batch = [x for x in range(len(unpack_outputs_length))]\n        last_state = output_unpacked[seqs, batch, :].view(-1, self.hidden_size)\n\n        x = self.linear(last_state)\n        return F.log_softmax(x, dim=1)\n```\n\n## Training\nHere we follow the pattern sort-> pack -> rnn to train the model in batch \nrefer to [fundamental](https://zhoumingjun.github.io/series/pytorch/rnn/1-Fundamental/)\n\n```python\nfor epoch in range(10):\n    \n    # train\n    loss_sum = 0;\n    nRound = 0\n    for i_batch, batch in enumerate(dataloader):\n\n        # zero\n        optimizer.zero_grad()\n\n        inputs, labels = batch\n        # pre-process\n        inputs = [name2tensor(name) for name in inputs]\n\n        inputs_length = [x.size(0) for x in inputs]\n        _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\n        _, indices_restore = torch.sort(indices_sorted)\n\n        # sort\n        inputs_sorted = [inputs[x] for x in indices_sorted]\n        labels_sorted = labels[indices_sorted]\n\n        # pack inputs\n        pack = pack_sequence(inputs_sorted)\n\n        # rnn\n        outputs = model(pack)\n\n        # loss/bp/step\n        loss = criterion(outputs, labels_sorted)\n\n        loss.backward()\n        optimizer.step()\n\n        loss_sum += loss\n        nRound += 1\n        if i_batch % 50 == 0:\n            print(\"epoch {} i_batch {} loss {}\".format(epoch, i_batch, loss_sum / nRound))\n            \n    # validate\n    with torch.no_grad():\n        acc = 0\n        for i_batch, batch in enumerate(dataloader):\n            inputs, labels = batch\n            # pre-process\n            inputs = [name2tensor(name) for name in inputs]\n\n            inputs_length = [x.size(0) for x in inputs]\n            _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\n            _, indices_restore = torch.sort(indices_sorted)\n\n            # sort\n            inputs_sorted = [inputs[x] for x in indices_sorted]\n            labels_sorted = labels[indices_sorted]\n\n            # pack inputs\n            pack = pack_sequence(inputs_sorted)\n\n            # rnn\n            outputs = model(pack)\n\n            top_v, topi = torch.topk(outputs, 1)\n            acc += (topi.view(1, -1) == labels_sorted).sum().item()\n```\n\nHere is the training output\n\n```\nepoch 0 i_batch 0 loss 2.942514419555664\nepoch 0 i_batch 50 loss 1.760701298713684\nepoch 0 i_batch 100 loss 1.5069572925567627\nepoch 0 i_batch 150 loss 1.3581050634384155\nepoch 0 acc:14758/20074 \nepoch 1 i_batch 0 loss 0.8439935445785522\nepoch 1 i_batch 50 loss 0.8764864802360535\nepoch 1 i_batch 100 loss 0.8679843544960022\nepoch 1 i_batch 150 loss 0.836237907409668\nepoch 1 acc:15769/20074 \nepoch 2 i_batch 0 loss 0.6725496649742126\nepoch 2 i_batch 50 loss 0.6784597635269165\nepoch 2 i_batch 100 loss 0.6801468729972839\nepoch 2 i_batch 150 loss 0.6713106632232666\nepoch 2 acc:16419/20074 \nepoch 3 i_batch 0 loss 0.7285385131835938\nepoch 3 i_batch 50 loss 0.5921918749809265\nepoch 3 i_batch 100 loss 0.5902780294418335\nepoch 3 i_batch 150 loss 0.5864360928535461\nepoch 3 acc:16829/20074 \nepoch 4 i_batch 0 loss 0.5711963176727295\nepoch 4 i_batch 50 loss 0.5279501676559448\nepoch 4 i_batch 100 loss 0.521149218082428\nepoch 4 i_batch 150 loss 0.5309893488883972\nepoch 4 acc:17058/20074 \nepoch 5 i_batch 0 loss 0.5366474390029907\nepoch 5 i_batch 50 loss 0.46456900238990784\nepoch 5 i_batch 100 loss 0.47316044569015503\nepoch 5 i_batch 150 loss 0.4669898450374603\nepoch 5 acc:17373/20074 \nepoch 6 i_batch 0 loss 0.4217735528945923\nepoch 6 i_batch 50 loss 0.4200931489467621\nepoch 6 i_batch 100 loss 0.43043628334999084\nepoch 6 i_batch 150 loss 0.4396715462207794\nepoch 6 acc:17447/20074 \nepoch 7 i_batch 0 loss 0.31758126616477966\nepoch 7 i_batch 50 loss 0.38675254583358765\nepoch 7 i_batch 100 loss 0.399675190448761\nepoch 7 i_batch 150 loss 0.4018913507461548\nepoch 7 acc:17867/20074 \nepoch 8 i_batch 0 loss 0.3606557548046112\nepoch 8 i_batch 50 loss 0.33634153008461\nepoch 8 i_batch 100 loss 0.34979167580604553\nepoch 8 i_batch 150 loss 0.36078399419784546\nepoch 8 acc:18013/20074 \nepoch 9 i_batch 0 loss 0.31942570209503174\nepoch 9 i_batch 50 loss 0.30287066102027893\nepoch 9 i_batch 100 loss 0.31636151671409607\nepoch 9 i_batch 150 loss 0.33173689246177673\nepoch 9 acc:18151/20074 \n```\n\n## Predict\n\nAfter training, we can predict names' language using the model.\n``` python\n# do some preidct\nfor i_batch, batch in enumerate(dataloader):\n    input, label = batch\n    for idx, input in enumerate(input):\n        lang, lang_id = predict(input)\n        print(\"input {}, label {}, predict {}, result: {}\".format(\n            input, all_langs[label[idx].item()],\n            lang,\n            lang_id == label[idx].item()))\n\n    break\n```\n\nHere is the result\n```\ninput Purse, label English, predict English, result: True\ninput Teague, label Irish, predict English, result: False\ninput Daher, label Arabic, predict Arabic, result: True\ninput Soma, label Japanese, predict Japanese, result: True\ninput Mikhail, label Arabic, predict Arabic, result: True\ninput Pawluk, label Russian, predict Russian, result: True\ninput Bakaleinikov, label Russian, predict Russian, result: True\ninput Zhaivoronok, label Russian, predict Russian, result: True\ninput Fujiwara, label Japanese, predict Japanese, result: True\ninput Jachikov, label Russian, predict Russian, result: True\ninput Sardelis, label Greek, predict Greek, result: True\ninput Yatzenko, label Russian, predict Russian, result: True\ninput Kabalevsky, label Russian, predict Russian, result: True\ninput Godo, label Japanese, predict Japanese, result: True\ninput Imran, label English, predict English, result: True\ninput Kokkali, label Greek, predict Greek, result: True\ninput Adam, label Russian, predict English, result: False\ninput Todorov, label Russian, predict Russian, result: True\ninput Bastian, label Russian, predict Russian, result: True\ninput Samson, label French, predict French, result: True\ninput Bereznitsky, label Russian, predict Russian, result: True\ninput Bakhmetiev, label Russian, predict Russian, result: True\ninput Hadad, label Arabic, predict Arabic, result: True\ninput Sung, label Korean, predict Chinese, result: False\ninput Oishi, label Japanese, predict Japanese, result: True\ninput Baidin, label Russian, predict Russian, result: True\ninput Pavlyuchkov, label Russian, predict Russian, result: True\ninput Sarkis, label Arabic, predict Arabic, result: True\ninput Kirwin, label English, predict English, result: True\ninput Pokhis, label Russian, predict Russian, result: True\ninput Granitov, label Russian, predict Russian, result: True\ninput Wirner, label German, predict German, result: True\ninput Jamburg, label Russian, predict Russian, result: True\ninput Lowe, label German, predict English, result: False\ninput Nespola, label Italian, predict Italian, result: True\ninput Maclean, label Scottish, predict English, result: False\ninput St martin, label French, predict French, result: True\ninput Langer, label German, predict English, result: False\ninput Shaikin, label Russian, predict Russian, result: True\ninput Podsevalov, label Russian, predict Russian, result: True\ninput Romano, label Italian, predict Italian, result: True\ninput Hadad, label Arabic, predict Arabic, result: True\ninput Hoshino, label Japanese, predict Japanese, result: True\ninput Morra, label Italian, predict Italian, result: True\ninput Ashwell, label English, predict English, result: True\ninput Awad, label Arabic, predict Arabic, result: True\ninput Okubo, label Japanese, predict Japanese, result: True\ninput Ametistov, label Russian, predict Russian, result: True\ninput Jachmenev, label Russian, predict Russian, result: True\ninput Lodygin, label Russian, predict Russian, result: True\ninput Vikulov, label Russian, predict Russian, result: True\ninput Prokoshin, label Russian, predict Russian, result: True\ninput Deulin, label Russian, predict Russian, result: True\ninput Zhdankov, label Russian, predict Russian, result: True\ninput Antyufeev, label Russian, predict Russian, result: True\ninput Babayan, label Russian, predict Russian, result: True\ninput Khouri, label Arabic, predict Arabic, result: True\ninput Dizhbak, label Russian, predict Russian, result: True\ninput Ogterop, label Dutch, predict Dutch, result: True\ninput Minitsky, label Russian, predict Russian, result: True\ninput Dobrynsky, label Russian, predict Russian, result: True\ninput Mikheev, label Russian, predict Russian, result: True\ninput Kelly, label Irish, predict English, result: False\ninput Martoyas, label Russian, predict Greek, result: False\ninput Koning, label Dutch, predict German, result: False\ninput Manfredi, label Italian, predict Italian, result: True\ninput Bakh, label Russian, predict Russian, result: True\ninput Foran, label English, predict English, result: True\ninput Nelson, label English, predict English, result: True\ninput Hapitsky, label Russian, predict Russian, result: True\ninput To The First Page, label Russian, predict Russian, result: True\ninput Basara, label Arabic, predict Arabic, result: True\ninput Coffey, label English, predict English, result: True\ninput Kenyon, label English, predict English, result: True\ninput Ichisada, label Japanese, predict Japanese, result: True\ninput Alves, label Portuguese, predict Spanish, result: False\ninput Awdiysky, label Russian, predict Russian, result: True\ninput Lecce, label Italian, predict Italian, result: True\ninput Gosselin, label French, predict French, result: True\ninput Rettig, label German, predict German, result: True\ninput Hudoshin, label Russian, predict Russian, result: True\ninput Yushkevich, label Russian, predict Russian, result: True\ninput Anderson, label Scottish, predict English, result: False\ninput Bassin, label Russian, predict Russian, result: True\ninput Bavilin, label Russian, predict Russian, result: True\ninput Altshuler, label Russian, predict German, result: False\ninput Hafizov, label Russian, predict Russian, result: True\ninput Holland, label English, predict English, result: True\ninput Lodyjensky, label Russian, predict Russian, result: True\ninput Matsura, label Japanese, predict Japanese, result: True\ninput Kalb, label Arabic, predict Arabic, result: True\ninput Daniau, label French, predict French, result: True\ninput Napoletani, label Italian, predict Italian, result: True\ninput Renov, label Russian, predict Russian, result: True\ninput Traversini, label Italian, predict Italian, result: True\ninput Ba, label Arabic, predict Arabic, result: True\ninput Nakamura, label Japanese, predict Japanese, result: True\ninput Gimondi, label Italian, predict Italian, result: True\ninput Lohanov, label Russian, predict Russian, result: True\ninput Lezhenko, label Russian, predict Russian, result: True\n\n```\n\n# The source code \n\nThe soure code can be found on [github](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/4.char-rnn.py)"},"fields":{"fpath":"/series/pytorch/rnn/2-charnn/","permalink":"/2018/09/10/character-level-rnn","category":"series"},"frontmatter":{"title":"Character-Level RNN","date":"2018-09-10","tags":["machine learning","pytorch","rnn"],"desc":null,"slug":null}},{"internal":{"content":"\n# Pytorch"},"fields":{"fpath":"/series/pytorch/","permalink":"/2018/09/07/pytorch","category":"series"},"frontmatter":{"title":"Pytorch","date":"2018-09-07","tags":["machine learning","pytorch"],"desc":null,"slug":null}},{"internal":{"content":"\n# RNN\n\n "},"fields":{"fpath":"/series/pytorch/rnn/","permalink":"/2018/09/07/rnn","category":"series"},"frontmatter":{"title":"RNN","date":"2018-09-07","tags":["machine learning","pytorch","rnn"],"desc":null,"slug":null}},{"internal":{"content":"\n\n# python3\n* star operator\n* map zip lambda\n* NDArray Indexing\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\n\n# pytorch\n\n\n## RNN\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\n\nAndrej Karpathy’s diagram shows the different pattern in RNN        \n![RNN](rnn.jpg)\n\n### Sequence \nThe following code show the concept about sequence. \ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \n\n```python\nimport torch\n\ninput_size = 10\nhidden_size = 20\nnum_layers = 1\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\n\n# data\ninput = torch.ones(4, 1, 10)\n\n# option1: sequence\noutput, hidden = model(input)\n\n# option2: step by step\ninput_0 = input[0,:,:].view(1,1,10)\ninput_1 = input[1,:,:].view(1,1,10)\ninput_2 = input[2,:,:].view(1,1,10)\ninput_3 = input[3,:,:].view(1,1,10)\n\noutput_0, hidden_0 = model(input_0)\noutput_1, hidden_1 = model(input_1, hidden_0)\noutput_2, hidden_2 = model(input_2, hidden_1)\noutput_3, hidden_3 = model(input_3, hidden_2)\n\n\nprint(hidden)\nprint(output)\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\nprint(output_0, output_1, output_2,output_3)\n\n\n# compare option1 & option2\nprint ((output[0]==output_0).sum().item() == hidden_size)\nprint ((output[1]==output_1).sum().item() == hidden_size)\nprint ((output[2]==output_2).sum().item() == hidden_size)\nprint ((output[3]==output_3).sum().item() == hidden_size)\n\n\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"\n# relation between hidden & output\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\n\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"\n```\n\nAs the result of the above code shown   \n1. the output contains all outputs of each iteration.\n2. the output is the collection of hidden state of each iteration\n3. from the last layer of the LSTM (in much layer network, see the official document)\n\n\n### Batch Processing\nIn fact, pytorch handle data in batch.  \nIt's more quickly, and save time.\n\n```python\nimport torch\n\ninput_size = 10\nhidden_size = 20\nnum_layers = 1\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\n\n# data\ninput = torch.randn(4, 4, 10)\n\n# option1: sequence\noutput, hidden = model(input)\n\n# option2: one by one\ninput_0 = input[:, 0, :].view(4,1,10)\ninput_1 = input[:, 1, :].view(4,1,10)\ninput_2 = input[:, 2, :].view(4,1,10)\ninput_3 = input[:, 3, :].view(4,1,10)\n\noutput_0, hidden_0 = model(input_0)\noutput_1, hidden_1 = model(input_1)\noutput_2, hidden_2 = model(input_2)\noutput_3, hidden_3 = model(input_3)\n\n#compare\nprint((output[-1][0]- output_0[-1][0]).sum())\nprint((output[-1][1]- output_1[-1][0]).sum())\nprint((output[-1][2]- output_2[-1][0]).sum())\nprint((output[-1][3]- output_3[-1][0]).sum())\n\"\"\"\ntensor(8.1956e-08, grad_fn=<SumBackward0>)\ntensor(3.2596e-09, grad_fn=<SumBackward0>)\ntensor(9.1270e-08, grad_fn=<SumBackward0>)\ntensor(5.1223e-08, grad_fn=<SumBackward0>)\n\"\"\"\n\n```\nThe above code process inputs in batch. \nOutput of shape is (seq_len, batch, num_directions * hidden_size)   \nWe can get the output according to the seq and batch    \noutput[i,j,:]: get the i iteration output of the sample j   \noutput[-1,j,:] get the last output of the sample j  \n\n### Batch processing with variable length sequences\nMany real cases need to handle variable length sequences.\nE.g. in nlp, the sentence length is variable, the character count of a word is variable.\n\nPytorch introduce several helper functions to handle this.\n\nhelper functions\n* torch.nn.utils.rnn.pack_sequence\n* torch.nn.utils.rnn.pad_sequence\n* torch.nn.utils.rnn.pad_packed_sequence\n* torch.nn.utils.rnn.pack_padded_sequence\n\nhelper structure\n* PackedSequence\n  \n```python\nimport torch\nimport numpy as np\n\nfrom torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n\ninput_size = 2\nhidden_size= 5\nnum_layers = 1\nnClasses = 10\nnSamples = 10\n\na = torch.ones(3, input_size)\nb = torch.ones(5, input_size)\nc = torch.ones(7, input_size)\n\n# pad\npad = pad_sequence([c,b,a])\nprint(\"pad result\", pad.size())\n\n# pack\npack = pack_sequence([c,b,a])\nprint(\"pack result:\", pack.data.size(), pack.batch_sizes)\n\n# pack_padded\npack_padded = pack_padded_sequence(pad, [7,5,3])\nprint(\"pack_padded result:\", pack_padded.data.size(), pack_padded.batch_sizes)\n\n# pad_packed\npad_packed_data, pad_packed_lengths = pad_packed_sequence(pack)\nprint(\"pad_packed result:\", pad_packed_data.size() ,pad_packed_lengths)\n\n# pattern\n\n\"\"\"\nprepare data/model/indices\n\"\"\"\n\n# data\ninputs = []\ntargets = []\nfor idx in range(nSamples):\n    # set random len of input , and set the len as target\n    # input: ones(len, input_size)\n    # target: len\n    len = np.random.randint(nSamples)+1\n    sample = torch.ones(len, input_size)\n    inputs.append(sample)\n    targets.append(len)\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\ndemo = torch.ones(10,1,  input_size)\nprint(\"sample sequence result\", model(demo)[0])\n\n# indices\nsample_length = [x.size(0) for x in inputs]\n_, indices_sorted = torch.sort(torch.LongTensor(sample_length), descending=True)\n_, indices_restore = torch.sort(indices_sorted)\n\nprint(\"sample length:\", sample_length)\n\n\"\"\"\noption1:\npre-process inputs\nsort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)\n\ntargets <-> outputs  \n\"\"\"\nprint(\"option1\")\n\n \n\n# sort inputs\ninputs_sorted = [inputs[x] for x in indices_sorted]\n\n# pack inputs\npack = pack_sequence(inputs_sorted)\n\n# rnn ...\noutputs, hidden = model(pack)\n\n# unpack\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\n\n# unsort\nunsorted_last_state = last_state[indices_restore,:]\nprint([(tup[0].size(0), tup[1], tup[2]) for tup in   zip(inputs, targets, unsorted_last_state)])\n\n\"\"\"\noption2 \npre-process (inputs, targets)\nsort (inputs, targets)-> pack(inputs) -> rnn -> unpack\n\ntargets(sorted) <--> outputs  \n\"\"\"\n\nprint(\"option2\")\nbatch = list(zip(inputs, targets))\n\n# sort inputs\nbatch_sorted = [batch[x] for x in indices_sorted]\n\n# pack inputs\npack = pack_sequence([tup[0] for tup in batch_sorted])\n\n# rnn ...\noutputs, hidden = model(pack)\n\n# unpack\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\n\nprint([(tup[0][0].size(0), tup[0][1], tup[1]) for tup in zip(batch_sorted, last_state)])\n\n\"\"\"\npad result torch.Size([7, 3, 2])\npack result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\npack_padded result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\npad_packed result: torch.Size([7, 3, 2]) tensor([7, 5, 3])\nsample sequence result tensor(\n\t\t[[[ 0.0121,  0.0403, -0.0511, -0.0392,  0.2119]],\n        [[ 0.0203,  0.0604, -0.0728, -0.0546,  0.2866]],\n        [[ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169]],\n        [[ 0.0271,  0.0784, -0.0860, -0.0653,  0.3302]],\n        [[ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362]],\n        [[ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390]],\n        [[ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403]],\n        [[ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409]],\n        [[ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412]],\n        [[ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413]]],\n       grad_fn=<CatBackward>)\nsample length: [9, 7, 10, 8, 6, 8, 5, 3, 5, 5]\noption1\n[(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \n(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), 0\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>))]\n\noption2\n[(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \n(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>))]\n\"\"\"\n```\nThe sample sequence result show the result of sample[1,1,1,1,1,1,1,1,1,1], so we can get the output of each iteration .     \nIn option 1: the outputs' order are restored, and it's the same as the orgin data           \nIn option 2: the outputs' order are sorted(not restored), and it's the same as the sorted data      \n\n# The source code\nThe soure code can be found on github\n\n[basic](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/1.basic.py) \n\n[batch](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/2.batch.py) \n\n[variable length](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/3.variable%20length.py)   \n\n# refs\n\nhttps://www.pythonlikeyoumeanit.com/intro.html\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558"},"fields":{"fpath":"/series/pytorch/rnn/1-Fundamental/","permalink":"/2018/09/07/fundamental","category":"series"},"frontmatter":{"title":"Fundamental","date":"2018-09-07","tags":["machine learning","pytorch","rnn"],"desc":null,"slug":null}},{"internal":{"content":"\n# Key Points\n# Lecture Notes\n\n## Introcution\nModel-free control can solve these problems   \n\n- MDP model is unknown, but experience can be sampled           \n- MDP model is known, but is too big to use, except by samples          \n \nOn/Off policy learning              \n\n- On-policy learning              \n    - “Learn on the job”                \n    - Learn about policy 𝛑 from experience sampled from 𝛑     \n- Off-policy learning         \n    - “Look over someone’s shoulder”                \n    - Learn about policy 𝛑 from experience sampled from μ                       \n\n## On-Policy MC Control\n\n### Generalised Policy Iteration with Action-Value Function\n\nGreedy policy improvement over V(s) requires model of MDP           \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\mathcal{P}_{ss'}^aV(s')    $\n\nGreedy policy improvement over Q(s,a) is model-free                 \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} \\mathcal{Q}(s,a))    $\n\nPolicy evaluation Monte-Carlo policy evaluation, $ Q = q_\\pi$             \nPolicy improvement Greedy policy improvement? (haha ,that is 𝝴-Greddy)\n\n\n### Exploration\n**recall greedy**   \n\n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$ \n\n**𝝴-Greddy Exploration**\n\n- Simplest idea for ensuring continual exploration          \n- All m actions are tried with non-zero probability             \n- With probability 1-𝝴  choose the greedy action           \n- With probability 𝝴 choose an action at random                \n$ \n\\pi(a|s) = \n\\begin{cases}\n    \\epsilon/m +1-\\epsilon , & \\text{if }  a^*= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} Q(s,a))    \\\\\n    \\epsilon/m, & \\text{otherwise}\n\\end{cases}\n$ \n\n**𝝴-Greedy Policy Improvement**          \n\nTheorem         \nFor any 𝝴-Greedy Policy 𝛑 , the 𝝴-Greedy policy 𝛑' with respect to q𝛑 is an improvement $ v_{\\pi'}(s) \\geqslant v_\\pi(s)  $\n$ \n\\begin{align*}\nq_\\pi(s, \\pi'(s)) &= \\sum_{a}\\pi'(a|s)q_\\pi(s,a) \\\\\n                  &= \\epsilon/m \\sum_{a} q_\\pi(s,a) + (1-\\epsilon) \\max_a q_\\pi(s,a) \\\\\n                  &\\geqslant  \\epsilon/m \\sum_{a} q_\\pi(s,a) + + (1-\\epsilon) \\sum_{a} \\frac{\\pi(a|s) - \\epsilon/m}{1-\\epsilon} q_\\pi(s,a) \\\\\n                  &= \\sum_a \\pi(a|s) q_\\pi(s,a) \\\\\n                  &= v_\\pi(s)\n\\end{align*}\n$ \n\nTherefore from policy improvement theorem,  $ v_{\\pi'}(s) \\geqslant v_\\pi(s)  $\n\n### GLIE \n**Definition**          \n_Greedy in the Limit with Infinite Exploration_(GLIE)   \n\n- All state-action pairs are explored infinitely many times,               \n$ \\lim\\limits_{k\\rightarrow \\infty} N_k(s,a)=\\infty $             \n\n- The policy converges on a greedy policy,                 \n$ \\lim\\limits_{k\\rightarrow \\infty} \\pi_k(a|s)=1(a=\\operatorname*{arg\\,max}\\limits_{a'} Q_k(s,a')) $          \n\nFor example, 𝝴-greedy is GLIE if 𝝴 reduces to zero at $ \\epsilon_k = \\frac{1}{k}$\n\n**GLIE Monte-Carlo Control**        \n\n- Sample kth episode using 𝛑: {S1, A1, R2, ..., ST } ∼ 𝛑          \n- For each state St and action At in the episode        \nrecall _Incremental Mean_          \n\n$\n\\begin{align*}\n    N(S_t, A_t) &\\leftarrow N(S_t, A_t) + 1 \\\\\n    Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))   \n\\end{align*}\n$      \n- Improve policy based on new action-value function             \n$\n\\begin{align*}\n    \\epsilon    &\\leftarrow 1/k \\\\\n    \\pi         &\\leftarrow \\epsilon-greedy(Q)\n\\end{align*}\n$  \n\n**Theroem**     \nGLIE Monte-Carlo control converges to the optimal action-value function, $ Q(s,a) \\rightarrow q_*(s,a) $\n\n## On-Policy TD Learning\n\n## Off-Policy Learning\n\n## Summary\n\n# Excises"},"fields":{"fpath":"/series/rl/5_model_free_control/","permalink":"/2017/03/14/5-model-free-control","category":"series"},"frontmatter":{"title":"5 Model Free Control","date":"2017-03-14","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\nhave not been revised yet."},"fields":{"fpath":"/series/rl/","permalink":"/2017/03/14/reinforcement-learning","category":"series"},"frontmatter":{"title":"Reinforcement learning","date":"2017-03-14","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\n# Definition\n- MDP               \nA Markov decision process (MDP) is a Markov reward process with decisions.  \nIt is an environment in which all states are Markov.    \nA Markov Decision Process is a tuple  $ <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $    \n\n    - $ \\mathcal{S} $ is a finite set of states   \n    - $ \\mathcal{A} $ is a finite set of actions\n    - $ \\mathcal{P} $ is a state transition probability matrix,       \n    $ \\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' | S_t=s, A_t = a] $  \n    - $ \\mathcal{R} $ is a reward function,       \n    $ \\mathcal{R}_s^a =  \\mathbb{E}[R_{t+1} | S_t=s, A_t = a] $   \n    - $ \\gamma $ is a discount factor,  $ \\gamma \\in [0,1]$  \n\n- state action relation         \n$\n\\begin{align*}\n    v_\\pi(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s,a)               \\\\\n    q_\\pi(s,a) &= \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s')  \n\\end{align*}\n$    \n\n- state-value function          \n$ v_\\pi(s)  = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') ) $ \n\n- action-value function          \n    $ q_\\pi(s,a)  = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a \\in \\mathcal{A}} \\pi(a'|s') q_\\pi(s',a') $  \n\n- return        \n    $ G_t = R_{t+1} +  R_{t+2} + ... = \\sum\\limits_{k=0} ^\\infty \\gamma^kR_{t+k+1} $              \n$\n\\begin{align*}\nV_\\pi(s) &= \\mathbb{E}_\\pi \\left \\{ {G_t | s_t = s} \\right \\}                                                   &  \\color{red} {MC} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma V_\\pi(s_{t+1}) | s_t = s\\right \\}                          &  \\color{red} {TD(0)} \\\\\n         &= \\sum_a\\pi(s,a)\\sum_{s'}\\mathbb{P}_{ss'}^a [R_{ss'}^a + \\gamma V_\\pi(s')]                            &  \\color{red} {DP}\n\\end{align*}\n$\n\n# Method\n \n## DP           \n$ V(S_t) \\leftarrow \\mathbb{E}_\\pi[ R_{t+1} + \\gamma V(S_{t+1})] $    \n\nThe way to find a policy $ \\pi_*$ over $ \\pi $              \n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$                          \n$  v_*(s) = \\max\\limits_a q_* (s,a)  \\ \\ \\ \\ \\    q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $\n\n## MC       \n $ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t))$       \n\n## TD           \n$ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1})-V(S_t))$            \n"},"fields":{"fpath":"/series/rl/rl_essence/","permalink":"/2017/03/14/rl-essence","category":"series"},"frontmatter":{"title":"RL Essence","date":"2017-03-14","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\n# Key Points\n**Return**      \n$\n\\begin{align*}\nV_\\pi(s) &= \\mathbb{E}_\\pi\\left \\{ \\sum_{k=0}^\\infty \\gamma_k R_{t+k+1} | s_t = s\\right \\}                      &  \\color{red}  {Definition} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma_k R_{t+k+2} | s_t = s\\right \\}    &  \\color{red}  {Unfolding} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma G_{t+1} | s_t = s\\right \\}                                 &  \\color{red}  {Recursive \\ formula} \n\\end{align*}\n$\n\n**Value function**           \n$\n\\begin{align*}\nV_\\pi(s) &= \\mathbb{E}_\\pi \\left \\{ {G_t | s_t = s} \\right \\}                                                   &  \\color{red} {MC} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma V_\\pi(s_{t+1}) | s_t = s\\right \\}                          &  \\color{red} {TD(0)} \\\\\n         &= \\sum_a\\pi(s,a)\\sum_{s'}\\mathbb{P}_{ss'}^a [R_{ss'}^a + \\gamma V_\\pi(s')]                            &  \\color{red} {DP}\n\\end{align*}\n$\n\nMC Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t))$       \nTD Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1})-V(S_t))$            \nDP Backup: $ V(S_t) \\leftarrow \\mathbb{E}_\\pi[ R_{t+1} + \\gamma V(S_{t+1})] $\n\n# Lecture Notes\n[some reading]https://www.tu-chemnitz.de/informatik/KI/scripts/ws0910/ml09_6.pdf\n## introduction\n**Model-free prediction**           \nEstimate the value function of an _unknown_ MDP\n\n## Monte-Carlo Learning\n[MC method](https://en.wikipedia.org/wiki/Monte_Carlo_method) \n\n- MC methods learn directly from episodes of experience\n- MC is model-free: no knowledge of MDP transitions / rewards \n- MC learns from complete episodes: no bootstrapping\n- MC uses the simplest possible idea: value = mean return\n- Caveat: can only apply MC to episodic MDPs \n    - All episodes must terminate\n\n### Some definition\n- Goal: learn $v_\\pi$ from episodes of experience under policy $\\pi$            \n    $ S_1, A_1, R_2, \\dots, S_k \\sim \\pi $\n- Recall that the return is the total discounted reward:            \n    $ G_t = R_{t+1} +  \\gammaR_{t+2} + ... = \\sum_{k=0} ^\\infty \\gamma^kR_{t+k+1} $     \n- Recall that the value function is the expected return:            \n    $  v(s) = \\mathbb{E}[G_t | S_t =s] $    \n- Monte-Carlo policy evaluation uses **empirical mean return** instead of expected return        \n\n\n### MC Policy Evaluation (algorithm)\nThare are two kinds of evaluation method:  _the first time-step_ and _every time-step_     \n\n- The first/Every time-step  t that state s is visited in an episode,       \n- Increment counter ,     $ N(s) \\leftarrow N(s) +1 $       \n- Increment total return, $ S(s) \\leftarrow S(s) +G_t $     \n- Value is estimated by mean return $ V(s) = S(s)/N(s)$     \n- By law of large numbers, $ V(s) \\rightarrow v_\\pi(s) \\text{  as  } N(s) \\rightarrow \\infty$       \n\n\n### Incremental Mean and Incremental MC updates (practice)\n**Incremental Mean**        \nThe Mean $ \\mu_1, \\mu_2, \\dots $ of a sequence $x_1, x_2, \\dots$ can be computed incrementally,                \n$\n\\begin{align*}\n\\mu_k &= \\frac{1}{k}\\sum_{j=1}^kx_j \\\\\n      &= \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1}x_j)  \\\\\n      &= \\frac{1}{k}(x_k + (k-1)\\mu_{k-1})  \\\\\n      &= \\frac{1}{k}x_k  +\\mu_{k-1} - \\frac{1}{k}\\mu_{k-1} \\\\\n      &= \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1}) \\\\\n\\end{align*}\n$\n\n**Incremental MC updates**         \n\n- Update V(s) incrementally after episode $ S_1, A_1, R_2, \\dots, S_k \\sim \\pi $          \n- For each state $S_t$ with return $G_t$            \n$\n\\begin{align*}    \n    N(s) & \\leftarrow N(s) +1 \\\\\n    V(S_t) & \\leftarrow V(S_t) + \\frac{1}{N(s)}(G_t - V(S_t))\n\\end{align*}\n$\n- In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.                   \n$ V(S_t)  \\leftarrow V(S_t) + \\alpha(G_t - V(S_t)) $     \n<span style=\"color:red\">what is stationary/non-stationary problems ???</span>   \n\n## Temporal-Difference Learning\n- TD methods learn directly from episodes of experience\n- TD is **model-free**: no knowledge of MDP transitions / rewards \n- TD learns from incomplete episodes, by bootstrapping\n- TD updates a guess towards a guess\n\n### TD Policy Evaluation\n- Simplest temporal-difference learning algorithm: TD(0) \n    - Update value $V(S_t)$ toward estimated return $ R_{t+1} + \\gamma V(S_{t+1})$  \n        $ V(S_t) \\leftarrow + \\alpha(R_{t+1} + \\gamma V(S_{t+1} - V(S_t))$    \n    - $ R_{t+1} + \\gamma V(S_{t+1}$ is called the TD target       \n    - $ \\delta _t = R_{t+1} + \\gamma V(S_{t+1} - V(S_t)$ is called the TD error     \n\n## MC vs TD\n### Bias\n- TD can learn before knowing the final outcome\n    - TD can learn online after every step\n    - MC must wait until end of episode before return is known\n- TD can learn without the final outcome \n    - TD can learn from incomplete sequences\n    - MC can only learn from complete sequences\n    - TD works in continuing (non-terminating) environments \n    - MC only works for episodic (terminating) environments\n\n- The return value\n    - Return $ G_t = R_{t+1} +  R_{t+2} + ... \\gamma^{T-1}R_T $  is unbiased estimate of $ v_\\pi(S_t)$\n    - True TD target $ R_{t+1} + \\gamma v_\\pi(S_{t+1})$ is is unbiased estimate of $ v_\\pi(S_t)$\n    - TD target $ R_{t+1} + \\gamma V(S_{t+1})$ is biased estimate of vπ(St)\n    - TD target is much lower variance than the return:\n        - Return depends on many random actions, transitions, rewards \n        - TD target depends on one random action, transition, reward\n\n### Variance\n MC | TD\n--- | ---\nMC has high variance, zero bias    |        TD has low variance, some bias                                                                      \nGood convergence properties        |        Usually more efficient than MC                                                 \n(even with function approximation) |        TD(0) converges to $v_\\pi(s)$                                                     \nNot very sensitive to initial value|        (but not always with function approximation)                                                  \nVery simple to understand and use  |        More sensitive to initial value            \n\n### markov property\n- TD exploits Markov property\n    - Usually more efficient in Markov environments\n- MC does not exploit Markov property\n    - Usually more effective in non-Markov environments       \n\nMC Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t))$       \nTD Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1})-V(S_t))$            \nDP Backup: $ V(S_t) \\leftarrow \\mathbb{E}_\\pi[ R_{t+1} + \\gamma V(S_{t+1})] $\n\n## Unified view of reinforcement learning\n- Bootstrapping: update involves an estimate \n    - MC does not bootstrap\n    - DP bootstraps\n    - TD bootstraps\n- Sampling: update samples an expectation \n    - MC samples\n    - DP does not sample \n    - TD samples\n\n![image](/img/content/note/rl/unified_view_of_rl.png)\n\n## TD(λ)\n### n-step return\n- Consider the followingn-step returns for $ n= 1,2,3, \\dots, \\infty$         \n$\n\\begin{align*}\nn=1          \\ \\ \\ \\ \\ \\      &  G_t^{(1)} =  R_{t+1} + \\gamma V(S_{t+1})      \\\\\nn=2          \\ \\ \\ \\ \\ \\      &  G_t^{(2)} =  R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})      \\\\\n\\vdots       \\ \\ \\ \\ \\ \\      &  \\vdots  \\\\\nn=\\infty     \\ \\ \\ \\ \\ \\      &  G_t^{(\\infty)} =   R_{t+1} + \\gamma R_{t+2} + \\dots +\\gamma^{T-1}R_T      \n\\end{align*}\n$\n- Define the n-step return      \n$ G_t^{(n)} =  R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^n V(S_{t+n})   $         \n- n-step TD learning        \n$ V(S_t) \\leftarrow V(S_t) + \\alpha ( G_t^{(n)} - V(S_t)  )$\n\n### Forward view of TD(λ)  \n\n**averaging n-step returns**            \n\n- We can averagen-step returns over different n          \n    e.g.  $ \\frac{1}{2}G^{(2)} + \\frac{1}{2}G^{(4)} $  \n\n**λ-return**            \n\n- The λ-return $G_t^\\lambda$ combines all n-step returns $G_t^{(n)}$        \n- Using weight $(1-\\lambda)\\lambda^{n-1}$       \n    $ G_t^\\lambda  = (1-\\lambda)\\sum_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)} $       \n- Forward-view TD(λ)     \n    $ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^\\lambda - V(S_t)) $\n\n\n\nUpdate value function towards the λ-return          \nForward-view looks into the future to compute $G_t^\\lambda$           \nLike MC, can only be computed from complete episodes            \n\n### Backward View TD(λ)  \n- Forward view provides theory\n- Backward view provides mechanism\n- Update online, every step, from incomplete sequences\n\n# Excises\n"},"fields":{"fpath":"/series/rl/4_model_free_predication/","permalink":"/2017/03/13/4-model-free-predication","category":"series"},"frontmatter":{"title":"4 Model Free Predication","date":"2017-03-13","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\n# Key Points\nDynamic programming assumes full knowledge of the MDP\n\n**value evaluation**        \ngiven MDP and policy $\\pi$, compute the state value $v_\\pi$  \njust follows the bellmen equation , and compute value of each state iteratively   \n$\n\\begin{align*}\nv_{k+1}(s)  &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_k(s') ) \\\\\nv_{k+1} &= \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_k\n\\end{align*}\n$   \n\n**policy optimality**       \ngiven MDP, compute the optimal policy $\\pi_*$           \n\n- Policy iteration       \nPolicy iteration try to find the optimal policy $\\pi_*$ by improving the current policy $\\pi$ step-by-step      \nThe key is greedy algorithm     \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_\\pi(s,a))$      \n\n- Value iteration             \nValue iteration try to find the optimal policy $\\pi_*$ by the known solutions to the sub problems                     \nIt is different to the policy iteration, and there is no explicit policy     \nThe key is compute state value from all the successor states(the known solution of the subproblem)          \n$ \n\\begin{align*} v_{k+1}(s) &= \\max\\limits_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') ) \\\\\n                  v_{k+1} &= \\max\\limits_{a \\in \\mathcal{A}} (\\mathcal{R}^a + \\gamma\\mathcal{P}^av_k) \n\\end{align*} \n$\n\n# Lecture \n## Introduction\nDynamic Programming is a very general solution method forproblems which have two properties:\n\n- Optimal substructure\n    - Principle of optimality applies\n    - Optimal solution can be decomposed into subproblems\n- Overlapping subproblems\n    - Subproblems recur many times\n    - Solutions can be cached and reused\n- Markov decision processes satisfy both properties\n    - Bellman equation gives recursive decomposition\n    - Value function stores and reuses solutions\n\n## Policy Evaluation\n\n### definition\n\n- Problem:  evaluate a given policy       \n- Solution:  iterative application of Bellman expectation backup        \n$ v_1 -> v_2 -> v_3 $             \n- Using synchronous backups,          \n    - At each iterationk+ 1\n    - For all statess $ s \\in \\mathcal{S} $\n    - Update $ v_{k+1}(s) $ from   $ v_{k}(s')$ wheres $s'$ is a sucdessor state of s\n\n### equation\n```dot\ndigraph g { \n   node[shape=\"circle\" , label=\"\", width=0.2, height=0.2]\n   l1[xlabel=\"Vk+1\\(s\\)\"]\n   l21[xlabel=\"a\", width=0.1, height=0.1 , style=filled]\n   l22[width=0.1, height=0.1, style=filled]\n   l31[xlabel=\"Vk\\(s'\\)\"]\n\n   l1 -> l21\n   l1 -> l22\n   l21 -> l31 [xlabel=\"r\"]\n   l21 -> l32\n   l22 -> l33\n   l22 -> l34\n}\n```\n$\n\\begin{align*}\nv_{k+1}(s)  &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_k(s') ) \\\\\nv_{k+1} &= \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_k\n\\end{align*}\n$         \n\n## Policy Iteration\n### definition\n- Given a policy $ \\pi $\n    - Evaluate the policy $ \\pi $ \n    $ v_\\pi(s) = \\mathcal{E}[R_{t+1} + \\gamma$_{t+2} + ... | S_t=s ]$     \n    - Improve the policy by acting greedily with respect to $ v_\\pi $     \n    $ \\pi' = greedy(v_\\pi) $\n\n![ policy iteration ](/img/content/note/rl/policy_iteration.png) \n\n### policy improvement\n\n- Consider a deterministic policy, $ a=\\pi(s)$      \n- We can improve the poilcy by acting greedily        \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_\\pi(s,a))$      \n- This improves the value from any state s over one step      \n$ q_\\pi(s, \\pi'(s)) = \\max\\limits_{a \\in \\mathcal{A}} q_\\pi (s,a)   \\geqslant q_\\pi(s, \\pi(s)) = v_\\pi(s) $       \n- It therefore improves the value function, $ v_\\pi'(s) \\geqslant v_\\pi(s) $      \n$\n\\begin{align*}\nv_\\pi(s)& \\leqslant q_\\pi(s,\\pi'(s))   \\\\\n        & \\leqslant \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s]    \\\\\n        & \\leqslant \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma q_\\pi(S_{t+1}, \\pi'(S_{t+1})) | S_t = s]    \\\\\n        & \\leqslant \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2}, \\pi'(S_{t+2})) | S_t = s]    \\\\\n        & \\leqslant \\mathbb{E}_{\\pi'}[R_{t+1} + \\gamma R_{t+2} + \\dots| S_t = s]    \\\\\n        &= v_{\\pi'}(s)\n\\end{align*}\n$\n- if improvements stop,     \n$ q_\\pi(s, \\pi'(s)) = \\max\\limits_{a \\in \\mathcal{A}} q_\\pi (s,a) = q_\\pi(s, \\pi(s)) = v_\\pi(s) $         \n- Then the Bellman optimality equation has been satisfied           \n$ v_\\pi(s) \\max\\limits_{a \\in \\mathbb{A}} q_\\pi(s,a) $  \n- Therefore $ v_\\pi(s) v_*(s)  \\text{   for all} s \\in \\mathbb(S) $       \n- so $\\pi$ is an optimal policy\n\n### extensions to policy iteration\n\n![ generalised policy iteration ](/img/content/note/rl/generalised_policy_iteration.png) \n\n\n## Value Iteration\n\n### definition\n- Problem:  find optimal policy $\\pi$     \n- Solution:  iterative application of Bellman optimality backup        \n$ v_1 -> v_2 -> \\dots -> v_* $             \n- Using synchronous backups,          \n    - At each iteration k+1\n    - For all statess $ s \\in \\mathcal{S} $\n    - Update $ v_{k+1}(s) $ from   $ v_{k}(s')$ \n- Convergence to $v_*$  \n- **Unlike policy iteration, there is no explicit policy**\n- Intermediate value functions may not correspond to any policy\n\n### Principle of Optimality\nAny optimal policy can be subdivided into two components\n\n- An optimal first action $A_*$\n- Followed by an optimal policy from successor state $\\mathbb{S}'$        \n\n**Theorem**     \nA policy $ \\pi(a|s)$ achieves the optimal value from state s, $v_\\pi(s) = v_*(s)$, if and only if            \n\n- For any state $s'$ reachable from s     \n- $\\pi$ achieves the optimal value from state $s'$, $ v_\\pi(s') = v_*(s') $       \n\nso if we know the subproblem's solution $v_*(s')$,$v_*(s)$ can be found by one-step lookahead       \n$ v_*(s) <- \\max\\limits_{a \\in \\mathbb{A}} R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $\n\nHere is the final algorithm:        \n$\n\\begin{align*}\nv_{k+1}(s) &= \\max\\limits_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s')  ) \\\\ \nv_{k+1}    &= \\max\\limits_{a \\in \\mathcal{A}} (\\mathcal{R}^a + \\gamma\\mathcal{P}^av_k)\n\\end{align*}\n$\n\n\n## Extensions to Dynamic Programming\n## Contraction Mapping\n\n# Excises\n## Policy Evaluation Solution\n[algorithm](https://github.com/zhoumingjun/reinforcement-learning/blob/master/DP/Policy%20Evaluation%20Solution%202.ipynb)          \nThis is a modified version, and I only make the bellman_equation as a function to make it easier to understand.                 \nThe key is the bellman equation .           \n\n$\n\\begin{align*}\nv_{k+1}(s)  &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_k(s') ) \\\\\nv_{k+1} &= \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_k\n\\end{align*}\n$  \n\n```python\ndef bellman_equation(policy, env, V, s, gamma ):\n    \"\"\"\n    compute the state value according to the ballman equation\n\n    Args:\n        policy: [S, A] shaped matrix representing the policy.\n        env: OpenAI env. env.P represents the transition probabilities of the environment.\n            env.P[s][a] is a (prob, next_state, reward, done) tuple.\n        V: the values of the states\n        s: the state,\n        gamma: gamma discount factor.\n\n    Returns:\n        the value of the state s\n    \"\"\"    \n    v = 0\n    for action, action_prob in enumerate(policy[s]):\n        for  state_transition_prob, next_state, reward, done in env.P[s][action]:\n            # Calculate the expected value\n            v += action_prob * state_transition_prob * (reward + gamma * V[next_state])\n    return v\n```\n\nsome explanation:   \n\n- action_prob \n    action_prob is action probability, which is  $\\pi(a|s)$  \n- state_transition_prob    \n    state_transition_prob is state transition probability, which is $P_(ss')^\\pi$\n\n## policy iteration\n[algorithm](https://github.com/zhoumingjun/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution%202.ipynb)          \nThe key is the greedy algorithm, that is    \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_\\pi(s,a))$\n\n```python\ndef greedy(env, policy, V, discount_factor=1.0):\n    policy_stable = True\n    for s in range(env.nS):\n        # The best action we would take under the currect policy\n        chosen_a = np.argmax(policy[s])\n\n        # Find the best action by one-step lookahead\n        # Ties are resolved arbitarily\n        action_values = np.zeros(env.nA)\n        for a in range(env.nA):\n            for prob, next_state, reward, done in env.P[s][a]:\n                action_values[a] += prob * (reward + discount_factor * V[next_state])\n        best_a = np.argmax(action_values)\n\n        # Greedily update the policy\n        if chosen_a != best_a:\n            policy_stable = False\n        policy[s] = np.eye(env.nA)[best_a]\n        \n    return   policy , policy_stable\n```        \n\n## value iteration\nThe key is one_step_lookahead, that is compute the max value of the state from all the successor states\n\n$ \n\\begin{align*} v_{k+1}(s) &= \\max\\limits_{a \\in \\mathcal{A}} (\\mathcal{R}_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') ) \\\\\n                  v_{k+1} &= \\max\\limits_{a \\in \\mathcal{A}} (\\mathcal{R}^a + \\gamma\\mathcal{P}^av_k) \n\\end{align*} \n$\n```python\n    def one_step_lookahead(state, V):\n        \"\"\"\n        Helper function to calculate the value for all action in a given state.\n        \n        Args:\n            state: The state to consider (int)\n            V: The value to use as an estimator, Vector of length env.nS\n        \n        Returns:\n            A vector of length env.nA containing the expected value of each action.\n        \"\"\"\n        A = np.zeros(env.nA)\n        for a in range(env.nA):\n            for prob, next_state, reward, done in env.P[state][a]:\n                A[a] += prob * (reward + discount_factor * V[next_state])\n        return A\n\n    A = one_step_lookahead(s, V)        \n    best_action_value = np.max(A)        \n```        "},"fields":{"fpath":"/series/rl/3_planning_by_dynamic_programming/","permalink":"/2017/03/10/3-planning-by-dynamic-programming","category":"series"},"frontmatter":{"title":"3 Planning by Dynamic Programming","date":"2017-03-10","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\n# Key points\n\n- state action relation \nThis describe the relation between state and action. \n    - $ v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s,a)  $\n    - $ q_\\pi(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') $\n  \n```dot\ndigraph g { \n   node[shape=\"circle\" , label=\"\", width=0.2, height=0.2]\n   l1[xlabel=\"v\\(s\\)\"]\n   l21[xlabel=\"a\", width=0.1, height=0.1 , style=filled]\n   l22[width=0.1, height=0.1, style=filled]\n   l31[xlabel=\"v\\(s'\\)\"]\n\n   l1 -> l21\n   l1 -> l22\n   l21 -> l31 [xlabel=\"r\"]\n   l21 -> l32\n   l22 -> l33\n   l22 -> l34\n}\n```\n\n- Bellman equation\nBellman equation is the key to the MDP, and it describe the relation of the elements in MDP  \n    $ <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $       \n    $ v_\\pi = \\mathcal{R}_\\pi + \\gamma \\mathcal{P}_\\pi v  $       \n\nstate-value function and action-value function can be described recursivly according to the state-action relation equation      \n    - state-value function $ v_\\pi(s)  = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') ) $        \n    - action-value function $ q_\\pi(s,a)  = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a \\in \\mathcal{A}} \\pi(a'|s') q_\\pi(s',a') $     \n\n- Optimal Policy    \nThe way to find a policy $ \\pi_*$ over $ \\pi $  \n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$ \n$  v_*(s) = \\max\\limits_a q_* (s,a)  \\ \\ \\ \\ \\    q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $        \nThen we get the following optimal policy            \n    - state-value function: $ v_*(s) = \\max\\limits_a R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $\n    - action-value function: $ q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\max\\limits_a' q_*(s',a') $  \n\n# Lecture Notes   \n## Markov Processes\nA state $S_t$ is Markov if and only if $ \\mathbb{P}[S_{t+1} | s_t] = \\mathbb{P}[S_{t+1}|S_1, ... , S_t] $   \nA Markov Process (or Markov Chain) is a tuple $ <\\mathcal{S}, \\mathcal{P}> $\n\n- $\\mathcal{S}$ is a (finite) set of states\n- $\\mathcal{P}$ is a state transition probability matrix  \n  $ \\mathcal{P} = \\mathbb{P}[S_{t+1} = s' | S_t = s] $\n \n## Markov Reward Processes\n### definition\nA Markov reward process is a Markov chain with values.  \nA Markov Reward Process is a tuple  $ <\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma> $   \n\n- $ \\mathcal{S} $ is a finite set of states   \n- $ \\mathcal{P} $ is a state transition probability matrix,       \n  $ \\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s' | S_t = s] $  \n- $ \\mathcal{R} $ is a reward function,       \n  $ \\mathcal{R}_s =  \\mathbb{E}[R_{t+1} | S_t=s]$   \n- $ \\gamma $ is a discount factor,  $ \\gamma \\in [0,1]$     \n\n### Return  \nThe return Gt is the total discounted reward from time-step t.  \n$ G_t = R_{t+1} +  R_{t+2} + ... = \\sum_{k=0} ^\\infty \\gamma^kR_{t+k+1} $     \n\n### Value Function\nThe state value function v(s) of an MRP is the expected return starting from state s    \n$  v(s) = \\mathbb{E}[G_t | S_t =s] $    \n\n### Bellman Equation for MRPs\n$ \n\\begin{align*}\n    v(s) &= \\mathbb{E}[G_t | S_t =s]  \\\\ \n         &= \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) | S_t =s]     \\\\     \n         &= \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\n\\end{align*}    \n$  \n\n### Bellman Equation in Matrix Form  \n$ \n\\begin{align*}\nv &= \\mathcal{R} + \\gamma \\mathcal{P}v \\\\\nv &= (1-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{align*}\n$    \n$ \n\\begin{bmatrix}\n    v_{1} \\\\\n    \\vdots \\\\\n    v_{m}  \n\\end{bmatrix} = \n\\begin{bmatrix}\n    \\mathcal{R}_{1} \\\\\n    \\vdots \\\\\n    \\mathcal{R}_{m}  \n\\end{bmatrix} + r\n\\begin{bmatrix}\n    \\mathcal{P}_{11} \\dots \\mathcal{P}_{1n} \\\\\n    \\vdots \\\\\n    \\mathcal{P}_{n1} \\dots \\mathcal{P}_{nn}  \n\\end{bmatrix} \n\\begin{bmatrix}\n    v_{1} \\\\\n    \\vdots \\\\\n    v_{m}  \n\\end{bmatrix} \n$\n\n## Markov Decision Processes\n### definition\nA Markov decision process (MDP) is a Markov reward process with decisions.  \nIt is an environment in which all states are Markov.    \nA Markov Decision Process is a tuple  $ <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $    \n\n- $ \\mathcal{S} $ is a finite set of states   \n- $ \\mathcal{A} $ is a finite set of actions\n- $ \\mathcal{P} $ is a state transition probability matrix,       \n  $ \\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' | S_t=s, A_t = a] $  \n- $ \\mathcal{R} $ is a reward function,       \n  $ \\mathcal{R}_s^a =  \\mathbb{E}[R_{t+1} | S_t=s, A_t = a] $   \n- $ \\gamma $ is a discount factor,  $ \\gamma \\in [0,1]$  \n\n### policy\nA policy $ \\pi $ is a distribution over actions given states,     \n$ \\pi(a|s) =\\mathbb{P}[A_t=a|S_t=s] $     \nPolicies are stationary (time-independent),     \n$ A_t \\sim \\pi(\\cdot|S_t) $\n\n- Given an MDP $ \\mathcal{M} = <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $\n- The state sequence $ S_1, S_2, S_3, \\dots $ is a Markov process $ <\\mathcal{S}, \\mathcal{P^\\pi}> $\n- The state and reward sequence $ S_1, S_2, S_3, \\dots $ is a Markov reward process  $ <\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma> $   \n- where\n$\n\\begin{align*}\n    \\mathcal{P}_{ss'}^\\pi &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  \\mathcal{P}_{ss'}^a \\\\\n    \\mathcal{R}_{s}^\\pi &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  \\mathcal{R}_{s}^a\n\\end{align*}\n$\n\n### value function\nThe state-value function $ v_\\pi(s) $  of an MDP is the expected return starting from state s, and then following policy $ \\pi $    \n$  v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t =s] $    \n\nThe action-value function $ q_\\pi(s,a) $ is the expected return starting from state s, taking action a, and then following policy $ \\pi $   \n$  q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t =s,A_t=a] $    \n\n### Bellman Expectation Equation\n**state-value function**       \n$ \n\\begin{align*}\nv_\\pi(s) &=\\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t =s] \\\\\nv_\\pi(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s,a) \\\\\n         &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') ) \n\\end{align*}\n$  \n\n**action-value function**   \n$\n\\begin{align*}\nq_\\pi(s,a) &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})| S_t =s,A_t=a] \\\\\nq_\\pi(s,a) &= \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') \\\\\n           &= \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a \\in \\mathcal{A}} \\pi(a'|s') q_\\pi(s',a')\n\\end{align*}\n$ \n\n### Bellman Equation in Matrix Form  \n$ \n\\begin{align*}\nv_\\pi &= \\mathcal{R}_\\pi + \\gamma \\mathcal{P}_\\pi v \\\\\nv_\\pi &= (1-\\gamma\\mathcal{P}_\\pi)^{-1}\\mathcal{R}_\\pi\n\\end{align*}\n$    \n\n### Optimal Value Function\n#### definition\nThe optimal state-value function $ v_*(s)$ is the maximum value function over all policies    \n$ v_*(s)=\\max\\limits_{\\pi}v_{\\pi}(s) $    \nThe optimal action-value function q⇤(s,a) is the maximum action-value function over all policies    \n$ q_*(s,a)=\\max\\limits_{\\pi}v_{\\pi}(s,a) $    \n\n- The optimal value function specifies the best possible performance in the MDP.\n- An MDP is “solved” when we know the optimal value fn. \n\n#### Optimal policy\nDefine a partial ordering over policies\n$ \\pi \\geqslant \\pi' \\ if\\ v_\\pi(s) \\geqslant v_\\pi'(s), \\forall s $  \n**Theorem** \nFor any Markov Decision Process \n\n- There exists an optimal policy $ \\pi_*$ that is better than or equal to all other policies,     \n$ \\pi_* \\geqslant \\pi, \\forall \\pi $\n- All optimal policies achieve the optimal value function   \n$ v_{\\pi_*}(s) = v_*(s) $\n- All optimal policies achieve the optimal action-value function    \n$ q_{\\pi_*}(s,a) = q_*(s,a) $\n\n#### Finding an Optimal Policy\n\nAn optimal policy can be found by maximising over $ q_*(s,a) $,   \n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$ \n\n- There is always a deterministic optimal policy for any MDP \n- If we know $ q_*(s,a) $, we immediately have the optimal policy\n\n$  \nv_*(s) = \\max\\limits_a q_* (s,a)  \\ \\ \\ \\ \\    q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') \\\\ \nv_*(s) = \\max\\limits_a R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') \\\\ \nq_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\max\\limits_a' q_*(s',a')\n$  \n\n### Solving the Bellman Optimality Equation\n- Bellman Optimality Equation is non-linear \n- No closed form solution (in general) \n- Many iterative solution methods\n    - Value Iteration \n    - Policy Iteration \n    - Q-learning \n    - Sarsa\n\n## Extensions to MDPs\n...\n"},"fields":{"fpath":"/series/rl/2_markov_decision_processes/","permalink":"/2017/03/09/2-markov-decision-processes","category":"series"},"frontmatter":{"title":"2 Markov Decision Processes","date":"2017-03-09","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}}],"tag":"machine learning","pagesSum":2,"page":0}}