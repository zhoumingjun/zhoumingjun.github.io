{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","description":"personal blog"}}},"pageContext":{"posts":[{"internal":{"content":"\n# Key points\n\n- state action relation \nThis describe the relation between state and action. \n    - $ v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s,a)  $\n    - $ q_\\pi(s,a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') $\n  \n```dot\ndigraph g { \n   node[shape=\"circle\" , label=\"\", width=0.2, height=0.2]\n   l1[xlabel=\"v\\(s\\)\"]\n   l21[xlabel=\"a\", width=0.1, height=0.1 , style=filled]\n   l22[width=0.1, height=0.1, style=filled]\n   l31[xlabel=\"v\\(s'\\)\"]\n\n   l1 -> l21\n   l1 -> l22\n   l21 -> l31 [xlabel=\"r\"]\n   l21 -> l32\n   l22 -> l33\n   l22 -> l34\n}\n```\n\n- Bellman equation\nBellman equation is the key to the MDP, and it describe the relation of the elements in MDP  \n    $ <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $       \n    $ v_\\pi = \\mathcal{R}_\\pi + \\gamma \\mathcal{P}_\\pi v  $       \n\nstate-value function and action-value function can be described recursivly according to the state-action relation equation      \n    - state-value function $ v_\\pi(s)  = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') ) $        \n    - action-value function $ q_\\pi(s,a)  = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a \\in \\mathcal{A}} \\pi(a'|s') q_\\pi(s',a') $     \n\n- Optimal Policy    \nThe way to find a policy $ \\pi_*$ over $ \\pi $  \n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$ \n$  v_*(s) = \\max\\limits_a q_* (s,a)  \\ \\ \\ \\ \\    q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $        \nThen we get the following optimal policy            \n    - state-value function: $ v_*(s) = \\max\\limits_a R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $\n    - action-value function: $ q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\max\\limits_a' q_*(s',a') $  \n\n# Lecture Notes   \n## Markov Processes\nA state $S_t$ is Markov if and only if $ \\mathbb{P}[S_{t+1} | s_t] = \\mathbb{P}[S_{t+1}|S_1, ... , S_t] $   \nA Markov Process (or Markov Chain) is a tuple $ <\\mathcal{S}, \\mathcal{P}> $\n\n- $\\mathcal{S}$ is a (finite) set of states\n- $\\mathcal{P}$ is a state transition probability matrix  \n  $ \\mathcal{P} = \\mathbb{P}[S_{t+1} = s' | S_t = s] $\n \n## Markov Reward Processes\n### definition\nA Markov reward process is a Markov chain with values.  \nA Markov Reward Process is a tuple  $ <\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma> $   \n\n- $ \\mathcal{S} $ is a finite set of states   \n- $ \\mathcal{P} $ is a state transition probability matrix,       \n  $ \\mathcal{P}_{ss'} = \\mathbb{P}[S_{t+1} = s' | S_t = s] $  \n- $ \\mathcal{R} $ is a reward function,       \n  $ \\mathcal{R}_s =  \\mathbb{E}[R_{t+1} | S_t=s]$   \n- $ \\gamma $ is a discount factor,  $ \\gamma \\in [0,1]$     \n\n### Return  \nThe return Gt is the total discounted reward from time-step t.  \n$ G_t = R_{t+1} +  R_{t+2} + ... = \\sum_{k=0} ^\\infty \\gamma^kR_{t+k+1} $     \n\n### Value Function\nThe state value function v(s) of an MRP is the expected return starting from state s    \n$  v(s) = \\mathbb{E}[G_t | S_t =s] $    \n\n### Bellman Equation for MRPs\n$ \n\\begin{align*}\n    v(s) &= \\mathbb{E}[G_t | S_t =s]  \\\\ \n         &= \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) | S_t =s]     \\\\     \n         &= \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\n\\end{align*}    \n$  \n\n### Bellman Equation in Matrix Form  \n$ \n\\begin{align*}\nv &= \\mathcal{R} + \\gamma \\mathcal{P}v \\\\\nv &= (1-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{align*}\n$    \n$ \n\\begin{bmatrix}\n    v_{1} \\\\\n    \\vdots \\\\\n    v_{m}  \n\\end{bmatrix} = \n\\begin{bmatrix}\n    \\mathcal{R}_{1} \\\\\n    \\vdots \\\\\n    \\mathcal{R}_{m}  \n\\end{bmatrix} + r\n\\begin{bmatrix}\n    \\mathcal{P}_{11} \\dots \\mathcal{P}_{1n} \\\\\n    \\vdots \\\\\n    \\mathcal{P}_{n1} \\dots \\mathcal{P}_{nn}  \n\\end{bmatrix} \n\\begin{bmatrix}\n    v_{1} \\\\\n    \\vdots \\\\\n    v_{m}  \n\\end{bmatrix} \n$\n\n## Markov Decision Processes\n### definition\nA Markov decision process (MDP) is a Markov reward process with decisions.  \nIt is an environment in which all states are Markov.    \nA Markov Decision Process is a tuple  $ <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $    \n\n- $ \\mathcal{S} $ is a finite set of states   \n- $ \\mathcal{A} $ is a finite set of actions\n- $ \\mathcal{P} $ is a state transition probability matrix,       \n  $ \\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' | S_t=s, A_t = a] $  \n- $ \\mathcal{R} $ is a reward function,       \n  $ \\mathcal{R}_s^a =  \\mathbb{E}[R_{t+1} | S_t=s, A_t = a] $   \n- $ \\gamma $ is a discount factor,  $ \\gamma \\in [0,1]$  \n\n### policy\nA policy $ \\pi $ is a distribution over actions given states,     \n$ \\pi(a|s) =\\mathbb{P}[A_t=a|S_t=s] $     \nPolicies are stationary (time-independent),     \n$ A_t \\sim \\pi(\\cdot|S_t) $\n\n- Given an MDP $ \\mathcal{M} = <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $\n- The state sequence $ S_1, S_2, S_3, \\dots $ is a Markov process $ <\\mathcal{S}, \\mathcal{P^\\pi}> $\n- The state and reward sequence $ S_1, S_2, S_3, \\dots $ is a Markov reward process  $ <\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma> $   \n- where\n$\n\\begin{align*}\n    \\mathcal{P}_{ss'}^\\pi &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  \\mathcal{P}_{ss'}^a \\\\\n    \\mathcal{R}_{s}^\\pi &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  \\mathcal{R}_{s}^a\n\\end{align*}\n$\n\n### value function\nThe state-value function $ v_\\pi(s) $  of an MDP is the expected return starting from state s, and then following policy $ \\pi $    \n$  v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t =s] $    \n\nThe action-value function $ q_\\pi(s,a) $ is the expected return starting from state s, taking action a, and then following policy $ \\pi $   \n$  q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t =s,A_t=a] $    \n\n### Bellman Expectation Equation\n**state-value function**       \n$ \n\\begin{align*}\nv_\\pi(s) &=\\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t =s] \\\\\nv_\\pi(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s,a) \\\\\n         &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') ) \n\\end{align*}\n$  \n\n**action-value function**   \n$\n\\begin{align*}\nq_\\pi(s,a) &= \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1})| S_t =s,A_t=a] \\\\\nq_\\pi(s,a) &= \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') \\\\\n           &= \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a \\in \\mathcal{A}} \\pi(a'|s') q_\\pi(s',a')\n\\end{align*}\n$ \n\n### Bellman Equation in Matrix Form  \n$ \n\\begin{align*}\nv_\\pi &= \\mathcal{R}_\\pi + \\gamma \\mathcal{P}_\\pi v \\\\\nv_\\pi &= (1-\\gamma\\mathcal{P}_\\pi)^{-1}\\mathcal{R}_\\pi\n\\end{align*}\n$    \n\n### Optimal Value Function\n#### definition\nThe optimal state-value function $ v_*(s)$ is the maximum value function over all policies    \n$ v_*(s)=\\max\\limits_{\\pi}v_{\\pi}(s) $    \nThe optimal action-value function q‚á§(s,a) is the maximum action-value function over all policies    \n$ q_*(s,a)=\\max\\limits_{\\pi}v_{\\pi}(s,a) $    \n\n- The optimal value function specifies the best possible performance in the MDP.\n- An MDP is ‚Äúsolved‚Äù when we know the optimal value fn. \n\n#### Optimal policy\nDefine a partial ordering over policies\n$ \\pi \\geqslant \\pi' \\ if\\ v_\\pi(s) \\geqslant v_\\pi'(s), \\forall s $  \n**Theorem** \nFor any Markov Decision Process \n\n- There exists an optimal policy $ \\pi_*$ that is better than or equal to all other policies,     \n$ \\pi_* \\geqslant \\pi, \\forall \\pi $\n- All optimal policies achieve the optimal value function   \n$ v_{\\pi_*}(s) = v_*(s) $\n- All optimal policies achieve the optimal action-value function    \n$ q_{\\pi_*}(s,a) = q_*(s,a) $\n\n#### Finding an Optimal Policy\n\nAn optimal policy can be found by maximising over $ q_*(s,a) $,   \n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$ \n\n- There is always a deterministic optimal policy for any MDP \n- If we know $ q_*(s,a) $, we immediately have the optimal policy\n\n$  \nv_*(s) = \\max\\limits_a q_* (s,a)  \\ \\ \\ \\ \\    q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') \\\\ \nv_*(s) = \\max\\limits_a R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') \\\\ \nq_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\max\\limits_a' q_*(s',a')\n$  \n\n### Solving the Bellman Optimality Equation\n- Bellman Optimality Equation is non-linear \n- No closed form solution (in general) \n- Many iterative solution methods\n    - Value Iteration \n    - Policy Iteration \n    - Q-learning \n    - Sarsa\n\n## Extensions to MDPs\n...\n"},"fields":{"fpath":"/series/rl/2_markov_decision_processes/","permalink":"/2017/03/09/2-Markov-Decision-Processes","category":"series"},"frontmatter":{"title":"2 Markov Decision Processes","date":"2017-03-09","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\n## About Reinforcement Learning  \n\nWhat makes reinforcement learning different from other machine learning paradigms?   \n\n- There is no supervisor, only a reward signal\n- Feedback is delayed, not instantaneous\n- Time really matters (sequential, non i.i.d data)\n- Agent‚Äôs actions affect the subsequent data it receives\n\n## The Reinforcement Learning Problem\n### Rewards\n\n**Definition (Reward Hypothesis)**   \nAll goals can be described by the maximisation of expected cumulative reward\n\n### Sequential Decision Making\n- Goal: select actions to maximise total future reward\n- Actions may have long term consequences\n- Reward may be delayed\n- It may be better to sacrifice immediate reward to gain more long-term reward\n\n\n## Inside An RL Agent\n\nAn RL agent may include one or more of these components:   \n- Policy: agent‚Äôs behaviour function  \n- Value function: how good is each state and/or action \n- Model: agent‚Äôs representation of the environment \n\n\n### policy\nA policy is the agent‚Äôs behaviour   \nIt is a map from state to action, e.g. Deterministic policy: a = ùúã(s)   \nStochastic policy:   $ \\pi(a|s) = \\mathbb{P}[A_t=t |S_t=s] $\n \n### value function\nValue function is a prediction of future reward    \nUsed to evaluate the goodness/badness of states    \nAnd therefore to select between actions, e.g.\n$$ \nv_{\\pi}(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} +\\gamma^{t+2} R_{t+3} + .. | S_{t} = s] \n$$\n\n### model \nA model predicts what the environment will do next P predicts the next state  \nR predicts the next (immediate) reward, e.g.  \n$ P_{ss{}'}^{\\alpha }=\\mathbb{P}[S_{t+1}=s{}'|S_{t} =s,A_{t} =a] $\n$ R_{s}^{\\alpha }=\\mathbb{E}[R_{t+1} |S_{t} =s,A_{t} =a] $\n\n## Problems within Reinforcement Learning\n"},"fields":{"fpath":"/series/rl/1_introduction_to_reinforcement_learning/","permalink":"/2017/03/08/1-Introduction-to-Reinforcement-Learning","category":"series"},"frontmatter":{"title":"1 Introduction to Reinforcement Learning","date":"2017-03-08","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},{"internal":{"content":"\n# Introduction\nThis is a series of reinforcement learning materials\n\n- book: [Reinforcement Learning: An Introduction (2nd Edition)](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)\n- lectures: [David Silver's Reinforcement Learning Course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) \n- exercises: [Reinforcement learning](https://github.com/dennybritz/reinforcement-learning)\n"},"fields":{"fpath":"/series/rl/0_preface/","permalink":"/2017/03/07/series-rl-preface","category":"series"},"frontmatter":{"title":"0 Preface","date":"2017-03-07","tags":["machine learning","reinforcement learning"],"desc":null,"slug":"series-rl-preface"}}],"tag":"machine learning","pagesSum":2,"page":1}}