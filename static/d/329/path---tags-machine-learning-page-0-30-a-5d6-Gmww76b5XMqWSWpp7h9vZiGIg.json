{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","description":"personal blog"}}},"pageContext":{"posts":[{"internal":{"content":"---\ntitle : \"5 Model Free Control\"\ndate : \"2017-03-14T15:05:04+08:00\"\nseries : [\"reinforcement learning\"]\ntags : [\"machine learning\",\"rl\"]\nmath : true\nviz : true\n---\n\n# Key Points\n# Lecture Notes\n\n## Introcution\nModel-free control can solve these problems   \n\n- MDP model is unknown, but experience can be sampled           \n- MDP model is known, but is too big to use, except by samples          \n \nOn/Off policy learning              \n\n- On-policy learning              \n    - “Learn on the job”                \n    - Learn about policy 𝛑 from experience sampled from 𝛑     \n- Off-policy learning         \n    - “Look over someone’s shoulder”                \n    - Learn about policy 𝛑 from experience sampled from μ                       \n\n## On-Policy MC Control\n\n### Generalised Policy Iteration with Action-Value Function\n\nGreedy policy improvement over V(s) requires model of MDP           \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\mathcal{P}_{ss'}^aV(s')    $\n\nGreedy policy improvement over Q(s,a) is model-free                 \n$ \\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} \\mathcal{Q}(s,a))    $\n\nPolicy evaluation Monte-Carlo policy evaluation, $ Q = q_\\pi$             \nPolicy improvement Greedy policy improvement? (haha ,that is 𝝴-Greddy)\n\n\n### Exploration\n**recall greedy**   \n\n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$ \n\n**𝝴-Greddy Exploration**\n\n- Simplest idea for ensuring continual exploration          \n- All m actions are tried with non-zero probability             \n- With probability 1-𝝴  choose the greedy action           \n- With probability 𝝴 choose an action at random                \n$ \n\\pi(a|s) = \n\\begin{cases}\n    \\epsilon/m +1-\\epsilon , & \\text{if }  a^*= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} Q(s,a))    \\\\\n    \\epsilon/m, & \\text{otherwise}\n\\end{cases}\n$ \n\n**𝝴-Greedy Policy Improvement**          \n\nTheorem         \nFor any 𝝴-Greedy Policy 𝛑 , the 𝝴-Greedy policy 𝛑' with respect to q𝛑 is an improvement $ v_{\\pi'}(s) \\geqslant v_\\pi(s)  $\n$ \n\\begin{align*}\nq_\\pi(s, \\pi'(s)) &= \\sum_{a}\\pi'(a|s)q_\\pi(s,a) \\\\\n                  &= \\epsilon/m \\sum_{a} q_\\pi(s,a) + (1-\\epsilon) \\max_a q_\\pi(s,a) \\\\\n                  &\\geqslant  \\epsilon/m \\sum_{a} q_\\pi(s,a) + + (1-\\epsilon) \\sum_{a} \\frac{\\pi(a|s) - \\epsilon/m}{1-\\epsilon} q_\\pi(s,a) \\\\\n                  &= \\sum_a \\pi(a|s) q_\\pi(s,a) \\\\\n                  &= v_\\pi(s)\n\\end{align*}\n$ \n\nTherefore from policy improvement theorem,  $ v_{\\pi'}(s) \\geqslant v_\\pi(s)  $\n\n### GLIE \n**Definition**          \n_Greedy in the Limit with Infinite Exploration_(GLIE)   \n\n- All state-action pairs are explored infinitely many times,               \n$ \\lim\\limits_{k\\rightarrow \\infty} N_k(s,a)=\\infty $             \n\n- The policy converges on a greedy policy,                 \n$ \\lim\\limits_{k\\rightarrow \\infty} \\pi_k(a|s)=1(a=\\operatorname*{arg\\,max}\\limits_{a'} Q_k(s,a')) $          \n\nFor example, 𝝴-greedy is GLIE if 𝝴 reduces to zero at $ \\epsilon_k = \\frac{1}{k}$\n\n**GLIE Monte-Carlo Control**        \n\n- Sample kth episode using 𝛑: {S1, A1, R2, ..., ST } ∼ 𝛑          \n- For each state St and action At in the episode        \nrecall _Incremental Mean_          \n\n$\n\\begin{align*}\n    N(S_t, A_t) &\\leftarrow N(S_t, A_t) + 1 \\\\\n    Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))   \n\\end{align*}\n$      \n- Improve policy based on new action-value function             \n$\n\\begin{align*}\n    \\epsilon    &\\leftarrow 1/k \\\\\n    \\pi         &\\leftarrow \\epsilon-greedy(Q)\n\\end{align*}\n$  \n\n**Theroem**     \nGLIE Monte-Carlo control converges to the optimal action-value function, $ Q(s,a) \\rightarrow q_*(s,a) $\n\n## On-Policy TD Learning\n\n## Off-Policy Learning\n\n## Summary\n\n# Excises"},"fields":{"slug":"/series/rl/5_model_free_control/","category":"series"},"frontmatter":{"title":"5 Model Free Control","date":"2017-03-14T15:05:04+08:00","tags":["machine learning","rl"]}}],"tag":"machine learning","pagesSum":1,"page":0}}