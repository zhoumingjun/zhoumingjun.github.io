{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","description":"personal blog"}}},"pageContext":{"posts":[{"internal":{"content":"\n# Introduction\nThis is following the pytorch tutorial article\n> https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n\nAnd for more, read the papers that introduced these topics:\n> [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n>  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/abs/1406.1078)\n> [Sequence to Sequence Learning with Neural Networks](http://arxiv.org/abs/1409.3215)\n> [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n> [A Neural Conversational Model](http://arxiv.org/abs/1506.05869)\n\n\n# Theory\n\n# Attention\nhttp://phontron.com/class/nn4nlp2017/assets/slides/nn4nlp-09-attention.pdf\n\nattention in pytorch\nhttps://github.com/thomlake/pytorch-attention\n\nhttps://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n \n# seq2seq\nhttps://github.com/keon/seq2seq/blob/master/model.py\n\n# DataSet \n1. http://www.manythings.org/anki/ \n\n\n\n"},"fields":{"fpath":"/series/pytorch/nlp/4-seq2seq/","permalink":"/2018/09/13/seq2seq-network-and-attention","category":"series"},"frontmatter":{"title":"seq2seq network and attention","date":"2018-09-13","tags":["machine learning","pytorch","seq2seq","attention"],"desc":null,"slug":null}}],"tag":"seq2seq","pagesSum":1,"page":0}}