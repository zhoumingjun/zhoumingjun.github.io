{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","description":"personal blog"}}},"pageContext":{"posts":[{"internal":{"content":"\n# Introduction\nThis is following the pytorch tutorial article\n> https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n\nAnd for more, read the papers that introduced these topics:\n> [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n>  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/abs/1406.1078)\n> [Sequence to Sequence Learning with Neural Networks](http://arxiv.org/abs/1409.3215)\n> [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n> [A Neural Conversational Model](http://arxiv.org/abs/1506.05869)\n\n# DataSet \n1. http://www.manythings.org/anki/ "},"fields":{"fpath":"/series/pytorch/rnn/4-seq2seq/","permalink":"/2018/09/13/seq2seq-network-and-attention","category":"series"},"frontmatter":{"title":"seq2seq network and attention","date":"2018-09-13","tags":["machine learning","pytorch","seq2seq","attention"],"desc":null,"slug":null}}],"tag":"attention","pagesSum":1,"page":0}}