{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","sourceUrl":"https://github.com/zhoumingjun/zhoumingjun.github.io/blob/source/","siteUrl":"https://zhoumingjun.github.io","disqusShortname":"zhoumingjun"}},"markdownRemark":{"id":"c7850b68-efe6-57fd-9689-877ed8d893d9","html":"<h1>Key Points</h1>\n<h1>Lecture Notes</h1>\n<h2>Introcution</h2>\n<p>Model-free control can solve these problems   </p>\n<ul>\n<li>MDP model is unknown, but experience can be sampled           </li>\n<li>MDP model is known, but is too big to use, except by samples          </li>\n</ul>\n<p>On/Off policy learning              </p>\n<ul>\n<li>\n<p>On-policy learning              </p>\n<ul>\n<li>‚ÄúLearn on the job‚Äù                </li>\n<li>Learn about policy ùõë from experience sampled from ùõë     </li>\n</ul>\n</li>\n<li>\n<p>Off-policy learning         </p>\n<ul>\n<li>‚ÄúLook over someone‚Äôs shoulder‚Äù                </li>\n<li>Learn about policy ùõë from experience sampled from Œº                       </li>\n</ul>\n</li>\n</ul>\n<h2>On-Policy MC Control</h2>\n<h3>Generalised Policy Iteration with Action-Value Function</h3>\n<p>Greedy policy improvement over V(s) requires model of MDP<br>\n$\\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\mathcal{P}_{ss'}^aV(s')$</p>\n<p>Greedy policy improvement over Q(s,a) is model-free<br>\n$\\pi'(s) = \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} \\mathcal{Q}(s,a))$</p>\n<p>Policy evaluation Monte-Carlo policy evaluation, $Q = q_\\pi$<br>\nPolicy improvement Greedy policy improvement? (haha ,that is ùù¥-Greddy)</p>\n<h3>Exploration</h3>\n<p><strong>recall greedy</strong>   </p>\n<p>$\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}$ </p>\n<p><strong>ùù¥-Greddy Exploration</strong></p>\n<ul>\n<li>Simplest idea for ensuring continual exploration          </li>\n<li>All m actions are tried with non-zero probability             </li>\n<li>With probability 1-ùù¥  choose the greedy action           </li>\n<li>With probability ùù¥ choose an action at random<br>\n$\\pi(a|s) = \n\\begin{cases}\n\\epsilon/m +1-\\epsilon , & \\text{if }  a^*= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} Q(s,a))    \\\\\n\\epsilon/m, & \\text{otherwise}\n\\end{cases}$ </li>\n</ul>\n<p><strong>ùù¥-Greedy Policy Improvement</strong>          </p>\n<p>Theorem<br>\nFor any ùù¥-Greedy Policy ùõë , the ùù¥-Greedy policy ùõë‚Äô with respect to qùõë is an improvement $v_{\\pi'}(s) \\geqslant v_\\pi(s)$\n$\\begin{align*}\nq_\\pi(s, \\pi'(s)) &= \\sum_{a}\\pi'(a|s)q_\\pi(s,a) \\\\\n                  &= \\epsilon/m \\sum_{a} q_\\pi(s,a) + (1-\\epsilon) \\max_a q_\\pi(s,a) \\\\\n                  &\\geqslant  \\epsilon/m \\sum_{a} q_\\pi(s,a) + + (1-\\epsilon) \\sum_{a} \\frac{\\pi(a|s) - \\epsilon/m}{1-\\epsilon} q_\\pi(s,a) \\\\\n                  &= \\sum_a \\pi(a|s) q_\\pi(s,a) \\\\\n                  &= v_\\pi(s)\n\\end{align*}$ </p>\n<p>Therefore from policy improvement theorem,  $v_{\\pi'}(s) \\geqslant v_\\pi(s)$</p>\n<h3>GLIE</h3>\n<p><strong>Definition</strong><br>\n<em>Greedy in the Limit with Infinite Exploration</em>(GLIE)   </p>\n<ul>\n<li>\n<p>All state-action pairs are explored infinitely many times,<br>\n$\\lim\\limits_{k\\rightarrow \\infty} N_k(s,a)=\\infty$             </p>\n</li>\n<li>\n<p>The policy converges on a greedy policy,<br>\n$\\lim\\limits_{k\\rightarrow \\infty} \\pi_k(a|s)=1(a=\\operatorname*{arg\\,max}\\limits_{a'} Q_k(s,a'))$          </p>\n</li>\n</ul>\n<p>For example, ùù¥-greedy is GLIE if ùù¥ reduces to zero at $\\epsilon_k = \\frac{1}{k}$</p>\n<p><strong>GLIE Monte-Carlo Control</strong>        </p>\n<ul>\n<li>Sample kth episode using ùõë: {S1, A1, R2, ‚Ä¶, ST } ‚àº ùõë          </li>\n<li>For each state St and action At in the episode<br>\nrecall <em>Incremental Mean</em>          </li>\n</ul>\n<p>$\\begin{align*}\n    N(S_t, A_t) &\\leftarrow N(S_t, A_t) + 1 \\\\\n    Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))   \n\\end{align*}$      </p>\n<ul>\n<li>Improve policy based on new action-value function<br>\n$\\begin{align*}\n\\epsilon    &\\leftarrow 1/k \\\\\n\\pi         &\\leftarrow \\epsilon-greedy(Q)\n\\end{align*}$  </li>\n</ul>\n<p><strong>Theroem</strong><br>\nGLIE Monte-Carlo control converges to the optimal action-value function, $Q(s,a) \\rightarrow q_*(s,a)$</p>\n<h2>On-Policy TD Learning</h2>\n<h2>Off-Policy Learning</h2>\n<h2>Summary</h2>\n<h1>Excises</h1>","fileAbsolutePath":"/home/zhoumingjun/github.com/zhoumingjun/zhoumingjun.github.io/content/series/rl/5_model_free_control.md","fields":{"permalink":"/2017/03/14/5-Model-Free-Control","category":"series"},"frontmatter":{"title":"5 Model Free Control","date":"March 14, 2017"}}},"pageContext":{"permalink":"/2017/03/14/5-Model-Free-Control","toc":"{\"children\":{\"5_model_free_control\":{\"post\":{\"internal\":{\"content\":\"\\n# Key Points\\n# Lecture Notes\\n\\n## Introcution\\nModel-free control can solve these problems   \\n\\n- MDP model is unknown, but experience can be sampled           \\n- MDP model is known, but is too big to use, except by samples          \\n \\nOn/Off policy learning              \\n\\n- On-policy learning              \\n    - ‚ÄúLearn on the job‚Äù                \\n    - Learn about policy ùõë from experience sampled from ùõë     \\n- Off-policy learning         \\n    - ‚ÄúLook over someone‚Äôs shoulder‚Äù                \\n    - Learn about policy ùõë from experience sampled from Œº                       \\n\\n## On-Policy MC Control\\n\\n### Generalised Policy Iteration with Action-Value Function\\n\\nGreedy policy improvement over V(s) requires model of MDP           \\n$ \\\\pi'(s) = \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} \\\\mathcal{R}_s^a + \\\\mathcal{P}_{ss'}^aV(s')    $\\n\\nGreedy policy improvement over Q(s,a) is model-free                 \\n$ \\\\pi'(s) = \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} \\\\mathcal{Q}(s,a))    $\\n\\nPolicy evaluation Monte-Carlo policy evaluation, $ Q = q_\\\\pi$             \\nPolicy improvement Greedy policy improvement? (haha ,that is ùù¥-Greddy)\\n\\n\\n### Exploration\\n**recall greedy**   \\n\\n$ \\n\\\\pi_*(a|s) = \\n\\\\begin{cases}\\n    1, & \\\\text{if }  a= \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_*(s,a))    \\\\\\\\\\n    0, & \\\\text{otherwise}\\n\\\\end{cases}\\n$ \\n\\n**ùù¥-Greddy Exploration**\\n\\n- Simplest idea for ensuring continual exploration          \\n- All m actions are tried with non-zero probability             \\n- With probability 1-ùù¥  choose the greedy action           \\n- With probability ùù¥ choose an action at random                \\n$ \\n\\\\pi(a|s) = \\n\\\\begin{cases}\\n    \\\\epsilon/m +1-\\\\epsilon , & \\\\text{if }  a^*= \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} Q(s,a))    \\\\\\\\\\n    \\\\epsilon/m, & \\\\text{otherwise}\\n\\\\end{cases}\\n$ \\n\\n**ùù¥-Greedy Policy Improvement**          \\n\\nTheorem         \\nFor any ùù¥-Greedy Policy ùõë , the ùù¥-Greedy policy ùõë' with respect to qùõë is an improvement $ v_{\\\\pi'}(s) \\\\geqslant v_\\\\pi(s)  $\\n$ \\n\\\\begin{align*}\\nq_\\\\pi(s, \\\\pi'(s)) &= \\\\sum_{a}\\\\pi'(a|s)q_\\\\pi(s,a) \\\\\\\\\\n                  &= \\\\epsilon/m \\\\sum_{a} q_\\\\pi(s,a) + (1-\\\\epsilon) \\\\max_a q_\\\\pi(s,a) \\\\\\\\\\n                  &\\\\geqslant  \\\\epsilon/m \\\\sum_{a} q_\\\\pi(s,a) + + (1-\\\\epsilon) \\\\sum_{a} \\\\frac{\\\\pi(a|s) - \\\\epsilon/m}{1-\\\\epsilon} q_\\\\pi(s,a) \\\\\\\\\\n                  &= \\\\sum_a \\\\pi(a|s) q_\\\\pi(s,a) \\\\\\\\\\n                  &= v_\\\\pi(s)\\n\\\\end{align*}\\n$ \\n\\nTherefore from policy improvement theorem,  $ v_{\\\\pi'}(s) \\\\geqslant v_\\\\pi(s)  $\\n\\n### GLIE \\n**Definition**          \\n_Greedy in the Limit with Infinite Exploration_(GLIE)   \\n\\n- All state-action pairs are explored infinitely many times,               \\n$ \\\\lim\\\\limits_{k\\\\rightarrow \\\\infty} N_k(s,a)=\\\\infty $             \\n\\n- The policy converges on a greedy policy,                 \\n$ \\\\lim\\\\limits_{k\\\\rightarrow \\\\infty} \\\\pi_k(a|s)=1(a=\\\\operatorname*{arg\\\\,max}\\\\limits_{a'} Q_k(s,a')) $          \\n\\nFor example, ùù¥-greedy is GLIE if ùù¥ reduces to zero at $ \\\\epsilon_k = \\\\frac{1}{k}$\\n\\n**GLIE Monte-Carlo Control**        \\n\\n- Sample kth episode using ùõë: {S1, A1, R2, ..., ST } ‚àº ùõë          \\n- For each state St and action At in the episode        \\nrecall _Incremental Mean_          \\n\\n$\\n\\\\begin{align*}\\n    N(S_t, A_t) &\\\\leftarrow N(S_t, A_t) + 1 \\\\\\\\\\n    Q(S_t, A_t) &\\\\leftarrow Q(S_t, A_t) + \\\\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))   \\n\\\\end{align*}\\n$      \\n- Improve policy based on new action-value function             \\n$\\n\\\\begin{align*}\\n    \\\\epsilon    &\\\\leftarrow 1/k \\\\\\\\\\n    \\\\pi         &\\\\leftarrow \\\\epsilon-greedy(Q)\\n\\\\end{align*}\\n$  \\n\\n**Theroem**     \\nGLIE Monte-Carlo control converges to the optimal action-value function, $ Q(s,a) \\\\rightarrow q_*(s,a) $\\n\\n## On-Policy TD Learning\\n\\n## Off-Policy Learning\\n\\n## Summary\\n\\n# Excises\"},\"fields\":{\"fpath\":\"/series/rl/5_model_free_control/\",\"permalink\":\"/2017/03/14/5-Model-Free-Control\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"5 Model Free Control\",\"date\":\"2017-03-14\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}},\"rl_essence\":{\"post\":{\"internal\":{\"content\":\"\\n# Definition\\n- MDP               \\nA Markov decision process (MDP) is a Markov reward process with decisions.  \\nIt is an environment in which all states are Markov.    \\nA Markov Decision Process is a tuple  $ <\\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, \\\\mathcal{R}, \\\\gamma> $    \\n\\n    - $ \\\\mathcal{S} $ is a finite set of states   \\n    - $ \\\\mathcal{A} $ is a finite set of actions\\n    - $ \\\\mathcal{P} $ is a state transition probability matrix,       \\n    $ \\\\mathcal{P}_{ss'}^a = \\\\mathbb{P}[S_{t+1} = s' | S_t=s, A_t = a] $  \\n    - $ \\\\mathcal{R} $ is a reward function,       \\n    $ \\\\mathcal{R}_s^a =  \\\\mathbb{E}[R_{t+1} | S_t=s, A_t = a] $   \\n    - $ \\\\gamma $ is a discount factor,  $ \\\\gamma \\\\in [0,1]$  \\n\\n- state action relation         \\n$\\n\\\\begin{align*}\\n    v_\\\\pi(s) &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s) q_\\\\pi(s,a)               \\\\\\\\\\n    q_\\\\pi(s,a) &= \\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_\\\\pi(s')  \\n\\\\end{align*}\\n$    \\n\\n- state-value function          \\n$ v_\\\\pi(s)  = \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  (\\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_\\\\pi(s') ) $ \\n\\n- action-value function          \\n    $ q_\\\\pi(s,a)  = \\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a'|s') q_\\\\pi(s',a') $  \\n\\n- return        \\n    $ G_t = R_{t+1} +  R_{t+2} + ... = \\\\sum\\\\limits_{k=0} ^\\\\infty \\\\gamma^kR_{t+k+1} $              \\n$\\n\\\\begin{align*}\\nV_\\\\pi(s) &= \\\\mathbb{E}_\\\\pi \\\\left \\\\{ {G_t | s_t = s} \\\\right \\\\}                                                   &  \\\\color{red} {MC} \\\\\\\\\\n         &= \\\\mathbb{E}_\\\\pi\\\\left \\\\{  R_{t+1} + \\\\gamma V_\\\\pi(s_{t+1}) | s_t = s\\\\right \\\\}                          &  \\\\color{red} {TD(0)} \\\\\\\\\\n         &= \\\\sum_a\\\\pi(s,a)\\\\sum_{s'}\\\\mathbb{P}_{ss'}^a [R_{ss'}^a + \\\\gamma V_\\\\pi(s')]                            &  \\\\color{red} {DP}\\n\\\\end{align*}\\n$\\n\\n# Method\\n \\n## DP           \\n$ V(S_t) \\\\leftarrow \\\\mathbb{E}_\\\\pi[ R_{t+1} + \\\\gamma V(S_{t+1})] $    \\n\\nThe way to find a policy $ \\\\pi_*$ over $ \\\\pi $              \\n$ \\n\\\\pi_*(a|s) = \\n\\\\begin{cases}\\n    1, & \\\\text{if }  a= \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_*(s,a))    \\\\\\\\\\n    0, & \\\\text{otherwise}\\n\\\\end{cases}\\n$                          \\n$  v_*(s) = \\\\max\\\\limits_a q_* (s,a)  \\\\ \\\\ \\\\ \\\\ \\\\    q_*(s,a) = R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') $\\n\\n## MC       \\n $ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha(G_t-V(S_t))$       \\n\\n## TD           \\n$ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha(R_{t+1} + \\\\gamma V(S_{t+1})-V(S_t))$            \\n\"},\"fields\":{\"fpath\":\"/series/rl/rl_essence/\",\"permalink\":\"/2017/03/14/RL-Essence\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"RL Essence\",\"date\":\"2017-03-14\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}},\"4_model_free_predication\":{\"post\":{\"internal\":{\"content\":\"\\n# Key Points\\n**Return**      \\n$\\n\\\\begin{align*}\\nV_\\\\pi(s) &= \\\\mathbb{E}_\\\\pi\\\\left \\\\{ \\\\sum_{k=0}^\\\\infty \\\\gamma_k R_{t+k+1} | s_t = s\\\\right \\\\}                      &  \\\\color{red}  {Definition} \\\\\\\\\\n         &= \\\\mathbb{E}_\\\\pi\\\\left \\\\{  R_{t+1} + \\\\gamma \\\\sum_{k=0}^\\\\infty \\\\gamma_k R_{t+k+2} | s_t = s\\\\right \\\\}    &  \\\\color{red}  {Unfolding} \\\\\\\\\\n         &= \\\\mathbb{E}_\\\\pi\\\\left \\\\{  R_{t+1} + \\\\gamma G_{t+1} | s_t = s\\\\right \\\\}                                 &  \\\\color{red}  {Recursive \\\\ formula} \\n\\\\end{align*}\\n$\\n\\n**Value function**           \\n$\\n\\\\begin{align*}\\nV_\\\\pi(s) &= \\\\mathbb{E}_\\\\pi \\\\left \\\\{ {G_t | s_t = s} \\\\right \\\\}                                                   &  \\\\color{red} {MC} \\\\\\\\\\n         &= \\\\mathbb{E}_\\\\pi\\\\left \\\\{  R_{t+1} + \\\\gamma V_\\\\pi(s_{t+1}) | s_t = s\\\\right \\\\}                          &  \\\\color{red} {TD(0)} \\\\\\\\\\n         &= \\\\sum_a\\\\pi(s,a)\\\\sum_{s'}\\\\mathbb{P}_{ss'}^a [R_{ss'}^a + \\\\gamma V_\\\\pi(s')]                            &  \\\\color{red} {DP}\\n\\\\end{align*}\\n$\\n\\nMC Backup: $ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha(G_t-V(S_t))$       \\nTD Backup: $ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha(R_{t+1} + \\\\gamma V(S_{t+1})-V(S_t))$            \\nDP Backup: $ V(S_t) \\\\leftarrow \\\\mathbb{E}_\\\\pi[ R_{t+1} + \\\\gamma V(S_{t+1})] $\\n\\n# Lecture Notes\\n[some reading]https://www.tu-chemnitz.de/informatik/KI/scripts/ws0910/ml09_6.pdf\\n## introduction\\n**Model-free prediction**           \\nEstimate the value function of an _unknown_ MDP\\n\\n## Monte-Carlo Learning\\n[MC method](https://en.wikipedia.org/wiki/Monte_Carlo_method) \\n\\n- MC methods learn directly from episodes of experience\\n- MC is model-free: no knowledge of MDP transitions / rewards \\n- MC learns from complete episodes: no bootstrapping\\n- MC uses the simplest possible idea: value = mean return\\n- Caveat: can only apply MC to episodic MDPs \\n    - All episodes must terminate\\n\\n### Some definition\\n- Goal: learn $v_\\\\pi$ from episodes of experience under policy $\\\\pi$            \\n    $ S_1, A_1, R_2, \\\\dots, S_k \\\\sim \\\\pi $\\n- Recall that the return is the total discounted reward:            \\n    $ G_t = R_{t+1} +  \\\\gammaR_{t+2} + ... = \\\\sum_{k=0} ^\\\\infty \\\\gamma^kR_{t+k+1} $     \\n- Recall that the value function is the expected return:            \\n    $  v(s) = \\\\mathbb{E}[G_t | S_t =s] $    \\n- Monte-Carlo policy evaluation uses **empirical mean return** instead of expected return        \\n\\n\\n### MC Policy Evaluation (algorithm)\\nThare are two kinds of evaluation method:  _the first time-step_ and _every time-step_     \\n\\n- The first/Every time-step  t that state s is visited in an episode,       \\n- Increment counter ,     $ N(s) \\\\leftarrow N(s) +1 $       \\n- Increment total return, $ S(s) \\\\leftarrow S(s) +G_t $     \\n- Value is estimated by mean return $ V(s) = S(s)/N(s)$     \\n- By law of large numbers, $ V(s) \\\\rightarrow v_\\\\pi(s) \\\\text{  as  } N(s) \\\\rightarrow \\\\infty$       \\n\\n\\n### Incremental Mean and Incremental MC updates (practice)\\n**Incremental Mean**        \\nThe Mean $ \\\\mu_1, \\\\mu_2, \\\\dots $ of a sequence $x_1, x_2, \\\\dots$ can be computed incrementally,                \\n$\\n\\\\begin{align*}\\n\\\\mu_k &= \\\\frac{1}{k}\\\\sum_{j=1}^kx_j \\\\\\\\\\n      &= \\\\frac{1}{k}(x_k + \\\\sum_{j=1}^{k-1}x_j)  \\\\\\\\\\n      &= \\\\frac{1}{k}(x_k + (k-1)\\\\mu_{k-1})  \\\\\\\\\\n      &= \\\\frac{1}{k}x_k  +\\\\mu_{k-1} - \\\\frac{1}{k}\\\\mu_{k-1} \\\\\\\\\\n      &= \\\\mu_{k-1} + \\\\frac{1}{k}(x_k - \\\\mu_{k-1}) \\\\\\\\\\n\\\\end{align*}\\n$\\n\\n**Incremental MC updates**         \\n\\n- Update V(s) incrementally after episode $ S_1, A_1, R_2, \\\\dots, S_k \\\\sim \\\\pi $          \\n- For each state $S_t$ with return $G_t$            \\n$\\n\\\\begin{align*}    \\n    N(s) & \\\\leftarrow N(s) +1 \\\\\\\\\\n    V(S_t) & \\\\leftarrow V(S_t) + \\\\frac{1}{N(s)}(G_t - V(S_t))\\n\\\\end{align*}\\n$\\n- In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.                   \\n$ V(S_t)  \\\\leftarrow V(S_t) + \\\\alpha(G_t - V(S_t)) $     \\n<span style=\\\"color:red\\\">what is stationary/non-stationary problems ???</span>   \\n\\n## Temporal-Difference Learning\\n- TD methods learn directly from episodes of experience\\n- TD is **model-free**: no knowledge of MDP transitions / rewards \\n- TD learns from incomplete episodes, by bootstrapping\\n- TD updates a guess towards a guess\\n\\n### TD Policy Evaluation\\n- Simplest temporal-difference learning algorithm: TD(0) \\n    - Update value $V(S_t)$ toward estimated return $ R_{t+1} + \\\\gamma V(S_{t+1})$  \\n        $ V(S_t) \\\\leftarrow + \\\\alpha(R_{t+1} + \\\\gamma V(S_{t+1} - V(S_t))$    \\n    - $ R_{t+1} + \\\\gamma V(S_{t+1}$ is called the TD target       \\n    - $ \\\\delta _t = R_{t+1} + \\\\gamma V(S_{t+1} - V(S_t)$ is called the TD error     \\n\\n## MC vs TD\\n### Bias\\n- TD can learn before knowing the final outcome\\n    - TD can learn online after every step\\n    - MC must wait until end of episode before return is known\\n- TD can learn without the final outcome \\n    - TD can learn from incomplete sequences\\n    - MC can only learn from complete sequences\\n    - TD works in continuing (non-terminating) environments \\n    - MC only works for episodic (terminating) environments\\n\\n- The return value\\n    - Return $ G_t = R_{t+1} +  R_{t+2} + ... \\\\gamma^{T-1}R_T $  is unbiased estimate of $ v_\\\\pi(S_t)$\\n    - True TD target $ R_{t+1} + \\\\gamma v_\\\\pi(S_{t+1})$ is is unbiased estimate of $ v_\\\\pi(S_t)$\\n    - TD target $ R_{t+1} + \\\\gamma V(S_{t+1})$ is biased estimate of vœÄ(St)\\n    - TD target is much lower variance than the return:\\n        - Return depends on many random actions, transitions, rewards \\n        - TD target depends on one random action, transition, reward\\n\\n### Variance\\n MC | TD\\n--- | ---\\nMC has high variance, zero bias    |        TD has low variance, some bias                                                                      \\nGood convergence properties        |        Usually more efficient than MC                                                 \\n(even with function approximation) |        TD(0) converges to $v_\\\\pi(s)$                                                     \\nNot very sensitive to initial value|        (but not always with function approximation)                                                  \\nVery simple to understand and use  |        More sensitive to initial value            \\n\\n### markov property\\n- TD exploits Markov property\\n    - Usually more efficient in Markov environments\\n- MC does not exploit Markov property\\n    - Usually more effective in non-Markov environments       \\n\\nMC Backup: $ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha(G_t-V(S_t))$       \\nTD Backup: $ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha(R_{t+1} + \\\\gamma V(S_{t+1})-V(S_t))$            \\nDP Backup: $ V(S_t) \\\\leftarrow \\\\mathbb{E}_\\\\pi[ R_{t+1} + \\\\gamma V(S_{t+1})] $\\n\\n## Unified view of reinforcement learning\\n- Bootstrapping: update involves an estimate \\n    - MC does not bootstrap\\n    - DP bootstraps\\n    - TD bootstraps\\n- Sampling: update samples an expectation \\n    - MC samples\\n    - DP does not sample \\n    - TD samples\\n\\n![image](/img/content/note/rl/unified_view_of_rl.png)\\n\\n## TD(Œª)\\n### n-step return\\n- Consider the followingn-step returns for $ n= 1,2,3, \\\\dots, \\\\infty$         \\n$\\n\\\\begin{align*}\\nn=1          \\\\ \\\\ \\\\ \\\\ \\\\ \\\\      &  G_t^{(1)} =  R_{t+1} + \\\\gamma V(S_{t+1})      \\\\\\\\\\nn=2          \\\\ \\\\ \\\\ \\\\ \\\\ \\\\      &  G_t^{(2)} =  R_{t+1} + \\\\gamma R_{t+2} + \\\\gamma^2 V(S_{t+2})      \\\\\\\\\\n\\\\vdots       \\\\ \\\\ \\\\ \\\\ \\\\ \\\\      &  \\\\vdots  \\\\\\\\\\nn=\\\\infty     \\\\ \\\\ \\\\ \\\\ \\\\ \\\\      &  G_t^{(\\\\infty)} =   R_{t+1} + \\\\gamma R_{t+2} + \\\\dots +\\\\gamma^{T-1}R_T      \\n\\\\end{align*}\\n$\\n- Define the n-step return      \\n$ G_t^{(n)} =  R_{t+1} + \\\\gamma R_{t+2} + \\\\dots + \\\\gamma^n V(S_{t+n})   $         \\n- n-step TD learning        \\n$ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha ( G_t^{(n)} - V(S_t)  )$\\n\\n### Forward view of TD(Œª)  \\n\\n**averaging n-step returns**            \\n\\n- We can averagen-step returns over different n          \\n    e.g.  $ \\\\frac{1}{2}G^{(2)} + \\\\frac{1}{2}G^{(4)} $  \\n\\n**Œª-return**            \\n\\n- The Œª-return $G_t^\\\\lambda$ combines all n-step returns $G_t^{(n)}$        \\n- Using weight $(1-\\\\lambda)\\\\lambda^{n-1}$       \\n    $ G_t^\\\\lambda  = (1-\\\\lambda)\\\\sum_{n=1}^\\\\infty \\\\lambda^{n-1}G_t^{(n)} $       \\n- Forward-view TD(Œª)     \\n    $ V(S_t) \\\\leftarrow V(S_t) + \\\\alpha (G_t^\\\\lambda - V(S_t)) $\\n\\n\\n\\nUpdate value function towards the Œª-return          \\nForward-view looks into the future to compute $G_t^\\\\lambda$           \\nLike MC, can only be computed from complete episodes            \\n\\n### Backward View TD(Œª)  \\n- Forward view provides theory\\n- Backward view provides mechanism\\n- Update online, every step, from incomplete sequences\\n\\n# Excises\\n\"},\"fields\":{\"fpath\":\"/series/rl/4_model_free_predication/\",\"permalink\":\"/2017/03/13/4-Model-Free-Predication\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"4 Model Free Predication\",\"date\":\"2017-03-13\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}},\"3_planning_by_dynamic_programming\":{\"post\":{\"internal\":{\"content\":\"\\n# Key Points\\nDynamic programming assumes full knowledge of the MDP\\n\\n**value evaluation**        \\ngiven MDP and policy $\\\\pi$, compute the state value $v_\\\\pi$  \\njust follows the bellmen equation , and compute value of each state iteratively   \\n$\\n\\\\begin{align*}\\nv_{k+1}(s)  &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  (\\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_k(s') ) \\\\\\\\\\nv_{k+1} &= \\\\mathcal{R}^\\\\pi + \\\\gamma \\\\mathcal{P}^\\\\pi v_k\\n\\\\end{align*}\\n$   \\n\\n**policy optimality**       \\ngiven MDP, compute the optimal policy $\\\\pi_*$           \\n\\n- Policy iteration       \\nPolicy iteration try to find the optimal policy $\\\\pi_*$ by improving the current policy $\\\\pi$ step-by-step      \\nThe key is greedy algorithm     \\n$ \\\\pi'(s) = \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_\\\\pi(s,a))$      \\n\\n- Value iteration             \\nValue iteration try to find the optimal policy $\\\\pi_*$ by the known solutions to the sub problems                     \\nIt is different to the policy iteration, and there is no explicit policy     \\nThe key is compute state value from all the successor states(the known solution of the subproblem)          \\n$ \\n\\\\begin{align*} v_{k+1}(s) &= \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} (\\\\mathcal{R}_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') ) \\\\\\\\\\n                  v_{k+1} &= \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} (\\\\mathcal{R}^a + \\\\gamma\\\\mathcal{P}^av_k) \\n\\\\end{align*} \\n$\\n\\n# Lecture \\n## Introduction\\nDynamic Programming is a very general solution method forproblems which have two properties:\\n\\n- Optimal substructure\\n    - Principle of optimality applies\\n    - Optimal solution can be decomposed into subproblems\\n- Overlapping subproblems\\n    - Subproblems recur many times\\n    - Solutions can be cached and reused\\n- Markov decision processes satisfy both properties\\n    - Bellman equation gives recursive decomposition\\n    - Value function stores and reuses solutions\\n\\n## Policy Evaluation\\n\\n### definition\\n\\n- Problem:  evaluate a given policy       \\n- Solution:  iterative application of Bellman expectation backup        \\n$ v_1 -> v_2 -> v_3 $             \\n- Using synchronous backups,          \\n    - At each iterationk+ 1\\n    - For all statess $ s \\\\in \\\\mathcal{S} $\\n    - Update $ v_{k+1}(s) $ from   $ v_{k}(s')$ wheres $s'$ is a sucdessor state of s\\n\\n### equation\\n```dot\\ndigraph g { \\n   node[shape=\\\"circle\\\" , label=\\\"\\\", width=0.2, height=0.2]\\n   l1[xlabel=\\\"Vk+1\\\\(s\\\\)\\\"]\\n   l21[xlabel=\\\"a\\\", width=0.1, height=0.1 , style=filled]\\n   l22[width=0.1, height=0.1, style=filled]\\n   l31[xlabel=\\\"Vk\\\\(s'\\\\)\\\"]\\n\\n   l1 -> l21\\n   l1 -> l22\\n   l21 -> l31 [xlabel=\\\"r\\\"]\\n   l21 -> l32\\n   l22 -> l33\\n   l22 -> l34\\n}\\n```\\n$\\n\\\\begin{align*}\\nv_{k+1}(s)  &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  (\\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_k(s') ) \\\\\\\\\\nv_{k+1} &= \\\\mathcal{R}^\\\\pi + \\\\gamma \\\\mathcal{P}^\\\\pi v_k\\n\\\\end{align*}\\n$         \\n\\n## Policy Iteration\\n### definition\\n- Given a policy $ \\\\pi $\\n    - Evaluate the policy $ \\\\pi $ \\n    $ v_\\\\pi(s) = \\\\mathcal{E}[R_{t+1} + \\\\gamma$_{t+2} + ... | S_t=s ]$     \\n    - Improve the policy by acting greedily with respect to $ v_\\\\pi $     \\n    $ \\\\pi' = greedy(v_\\\\pi) $\\n\\n![ policy iteration ](/img/content/note/rl/policy_iteration.png) \\n\\n### policy improvement\\n\\n- Consider a deterministic policy, $ a=\\\\pi(s)$      \\n- We can improve the poilcy by acting greedily        \\n$ \\\\pi'(s) = \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_\\\\pi(s,a))$      \\n- This improves the value from any state s over one step      \\n$ q_\\\\pi(s, \\\\pi'(s)) = \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} q_\\\\pi (s,a)   \\\\geqslant q_\\\\pi(s, \\\\pi(s)) = v_\\\\pi(s) $       \\n- It therefore improves the value function, $ v_\\\\pi'(s) \\\\geqslant v_\\\\pi(s) $      \\n$\\n\\\\begin{align*}\\nv_\\\\pi(s)& \\\\leqslant q_\\\\pi(s,\\\\pi'(s))   \\\\\\\\\\n        & \\\\leqslant \\\\mathbb{E}_{\\\\pi'}[R_{t+1} + \\\\gamma v_\\\\pi(S_{t+1}) | S_t = s]    \\\\\\\\\\n        & \\\\leqslant \\\\mathbb{E}_{\\\\pi'}[R_{t+1} + \\\\gamma q_\\\\pi(S_{t+1}, \\\\pi'(S_{t+1})) | S_t = s]    \\\\\\\\\\n        & \\\\leqslant \\\\mathbb{E}_{\\\\pi'}[R_{t+1} + \\\\gamma R_{t+2} + \\\\gamma^2 q_\\\\pi(S_{t+2}, \\\\pi'(S_{t+2})) | S_t = s]    \\\\\\\\\\n        & \\\\leqslant \\\\mathbb{E}_{\\\\pi'}[R_{t+1} + \\\\gamma R_{t+2} + \\\\dots| S_t = s]    \\\\\\\\\\n        &= v_{\\\\pi'}(s)\\n\\\\end{align*}\\n$\\n- if improvements stop,     \\n$ q_\\\\pi(s, \\\\pi'(s)) = \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} q_\\\\pi (s,a) = q_\\\\pi(s, \\\\pi(s)) = v_\\\\pi(s) $         \\n- Then the Bellman optimality equation has been satisfied           \\n$ v_\\\\pi(s) \\\\max\\\\limits_{a \\\\in \\\\mathbb{A}} q_\\\\pi(s,a) $  \\n- Therefore $ v_\\\\pi(s) v_*(s)  \\\\text{   for all} s \\\\in \\\\mathbb(S) $       \\n- so $\\\\pi$ is an optimal policy\\n\\n### extensions to policy iteration\\n\\n![ generalised policy iteration ](/img/content/note/rl/generalised_policy_iteration.png) \\n\\n\\n## Value Iteration\\n\\n### definition\\n- Problem:  find optimal policy $\\\\pi$     \\n- Solution:  iterative application of Bellman optimality backup        \\n$ v_1 -> v_2 -> \\\\dots -> v_* $             \\n- Using synchronous backups,          \\n    - At each iteration k+1\\n    - For all statess $ s \\\\in \\\\mathcal{S} $\\n    - Update $ v_{k+1}(s) $ from   $ v_{k}(s')$ \\n- Convergence to $v_*$  \\n- **Unlike policy iteration, there is no explicit policy**\\n- Intermediate value functions may not correspond to any policy\\n\\n### Principle of Optimality\\nAny optimal policy can be subdivided into two components\\n\\n- An optimal first action $A_*$\\n- Followed by an optimal policy from successor state $\\\\mathbb{S}'$        \\n\\n**Theorem**     \\nA policy $ \\\\pi(a|s)$ achieves the optimal value from state s, $v_\\\\pi(s) = v_*(s)$, if and only if            \\n\\n- For any state $s'$ reachable from s     \\n- $\\\\pi$ achieves the optimal value from state $s'$, $ v_\\\\pi(s') = v_*(s') $       \\n\\nso if we know the subproblem's solution $v_*(s')$,$v_*(s)$ can be found by one-step lookahead       \\n$ v_*(s) <- \\\\max\\\\limits_{a \\\\in \\\\mathbb{A}} R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') $\\n\\nHere is the final algorithm:        \\n$\\n\\\\begin{align*}\\nv_{k+1}(s) &= \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} (\\\\mathcal{R}_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s')  ) \\\\\\\\ \\nv_{k+1}    &= \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} (\\\\mathcal{R}^a + \\\\gamma\\\\mathcal{P}^av_k)\\n\\\\end{align*}\\n$\\n\\n\\n## Extensions to Dynamic Programming\\n## Contraction Mapping\\n\\n# Excises\\n## Policy Evaluation Solution\\n[algorithm](https://github.com/zhoumingjun/reinforcement-learning/blob/master/DP/Policy%20Evaluation%20Solution%202.ipynb)          \\nThis is a modified version, and I only make the bellman_equation as a function to make it easier to understand.                 \\nThe key is the bellman equation .           \\n\\n$\\n\\\\begin{align*}\\nv_{k+1}(s)  &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  (\\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_k(s') ) \\\\\\\\\\nv_{k+1} &= \\\\mathcal{R}^\\\\pi + \\\\gamma \\\\mathcal{P}^\\\\pi v_k\\n\\\\end{align*}\\n$  \\n\\n```python\\ndef bellman_equation(policy, env, V, s, gamma ):\\n    \\\"\\\"\\\"\\n    compute the state value according to the ballman equation\\n\\n    Args:\\n        policy: [S, A] shaped matrix representing the policy.\\n        env: OpenAI env. env.P represents the transition probabilities of the environment.\\n            env.P[s][a] is a (prob, next_state, reward, done) tuple.\\n        V: the values of the states\\n        s: the state,\\n        gamma: gamma discount factor.\\n\\n    Returns:\\n        the value of the state s\\n    \\\"\\\"\\\"    \\n    v = 0\\n    for action, action_prob in enumerate(policy[s]):\\n        for  state_transition_prob, next_state, reward, done in env.P[s][action]:\\n            # Calculate the expected value\\n            v += action_prob * state_transition_prob * (reward + gamma * V[next_state])\\n    return v\\n```\\n\\nsome explanation:   \\n\\n- action_prob \\n    action_prob is action probability, which is  $\\\\pi(a|s)$  \\n- state_transition_prob    \\n    state_transition_prob is state transition probability, which is $P_(ss')^\\\\pi$\\n\\n## policy iteration\\n[algorithm](https://github.com/zhoumingjun/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution%202.ipynb)          \\nThe key is the greedy algorithm, that is    \\n$ \\\\pi'(s) = \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_\\\\pi(s,a))$\\n\\n```python\\ndef greedy(env, policy, V, discount_factor=1.0):\\n    policy_stable = True\\n    for s in range(env.nS):\\n        # The best action we would take under the currect policy\\n        chosen_a = np.argmax(policy[s])\\n\\n        # Find the best action by one-step lookahead\\n        # Ties are resolved arbitarily\\n        action_values = np.zeros(env.nA)\\n        for a in range(env.nA):\\n            for prob, next_state, reward, done in env.P[s][a]:\\n                action_values[a] += prob * (reward + discount_factor * V[next_state])\\n        best_a = np.argmax(action_values)\\n\\n        # Greedily update the policy\\n        if chosen_a != best_a:\\n            policy_stable = False\\n        policy[s] = np.eye(env.nA)[best_a]\\n        \\n    return   policy , policy_stable\\n```        \\n\\n## value iteration\\nThe key is one_step_lookahead, that is compute the max value of the state from all the successor states\\n\\n$ \\n\\\\begin{align*} v_{k+1}(s) &= \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} (\\\\mathcal{R}_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') ) \\\\\\\\\\n                  v_{k+1} &= \\\\max\\\\limits_{a \\\\in \\\\mathcal{A}} (\\\\mathcal{R}^a + \\\\gamma\\\\mathcal{P}^av_k) \\n\\\\end{align*} \\n$\\n```python\\n    def one_step_lookahead(state, V):\\n        \\\"\\\"\\\"\\n        Helper function to calculate the value for all action in a given state.\\n        \\n        Args:\\n            state: The state to consider (int)\\n            V: The value to use as an estimator, Vector of length env.nS\\n        \\n        Returns:\\n            A vector of length env.nA containing the expected value of each action.\\n        \\\"\\\"\\\"\\n        A = np.zeros(env.nA)\\n        for a in range(env.nA):\\n            for prob, next_state, reward, done in env.P[state][a]:\\n                A[a] += prob * (reward + discount_factor * V[next_state])\\n        return A\\n\\n    A = one_step_lookahead(s, V)        \\n    best_action_value = np.max(A)        \\n```        \"},\"fields\":{\"fpath\":\"/series/rl/3_planning_by_dynamic_programming/\",\"permalink\":\"/2017/03/10/3-Planning-by-Dynamic-Programming\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"3 Planning by Dynamic Programming\",\"date\":\"2017-03-10\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}},\"2_markov_decision_processes\":{\"post\":{\"internal\":{\"content\":\"\\n# Key points\\n\\n- state action relation \\nThis describe the relation between state and action. \\n    - $ v_\\\\pi(s) = \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s) q_\\\\pi(s,a)  $\\n    - $ q_\\\\pi(s,a) = \\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_\\\\pi(s') $\\n  \\n```dot\\ndigraph g { \\n   node[shape=\\\"circle\\\" , label=\\\"\\\", width=0.2, height=0.2]\\n   l1[xlabel=\\\"v\\\\(s\\\\)\\\"]\\n   l21[xlabel=\\\"a\\\", width=0.1, height=0.1 , style=filled]\\n   l22[width=0.1, height=0.1, style=filled]\\n   l31[xlabel=\\\"v\\\\(s'\\\\)\\\"]\\n\\n   l1 -> l21\\n   l1 -> l22\\n   l21 -> l31 [xlabel=\\\"r\\\"]\\n   l21 -> l32\\n   l22 -> l33\\n   l22 -> l34\\n}\\n```\\n\\n- Bellman equation\\nBellman equation is the key to the MDP, and it describe the relation of the elements in MDP  \\n    $ <\\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, \\\\mathcal{R}, \\\\gamma> $       \\n    $ v_\\\\pi = \\\\mathcal{R}_\\\\pi + \\\\gamma \\\\mathcal{P}_\\\\pi v  $       \\n\\nstate-value function and action-value function can be described recursivly according to the state-action relation equation      \\n    - state-value function $ v_\\\\pi(s)  = \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  (\\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_\\\\pi(s') ) $        \\n    - action-value function $ q_\\\\pi(s,a)  = \\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a'|s') q_\\\\pi(s',a') $     \\n\\n- Optimal Policy    \\nThe way to find a policy $ \\\\pi_*$ over $ \\\\pi $  \\n$ \\n\\\\pi_*(a|s) = \\n\\\\begin{cases}\\n    1, & \\\\text{if }  a= \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_*(s,a))    \\\\\\\\\\n    0, & \\\\text{otherwise}\\n\\\\end{cases}\\n$ \\n$  v_*(s) = \\\\max\\\\limits_a q_* (s,a)  \\\\ \\\\ \\\\ \\\\ \\\\    q_*(s,a) = R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') $        \\nThen we get the following optimal policy            \\n    - state-value function: $ v_*(s) = \\\\max\\\\limits_a R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') $\\n    - action-value function: $ q_*(s,a) = R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a \\\\max\\\\limits_a' q_*(s',a') $  \\n\\n# Lecture Notes   \\n## Markov Processes\\nA state $S_t$ is Markov if and only if $ \\\\mathbb{P}[S_{t+1} | s_t] = \\\\mathbb{P}[S_{t+1}|S_1, ... , S_t] $   \\nA Markov Process (or Markov Chain) is a tuple $ <\\\\mathcal{S}, \\\\mathcal{P}> $\\n\\n- $\\\\mathcal{S}$ is a (finite) set of states\\n- $\\\\mathcal{P}$ is a state transition probability matrix  \\n  $ \\\\mathcal{P} = \\\\mathbb{P}[S_{t+1} = s' | S_t = s] $\\n \\n## Markov Reward Processes\\n### definition\\nA Markov reward process is a Markov chain with values.  \\nA Markov Reward Process is a tuple  $ <\\\\mathcal{S}, \\\\mathcal{P}, \\\\mathcal{R}, \\\\gamma> $   \\n\\n- $ \\\\mathcal{S} $ is a finite set of states   \\n- $ \\\\mathcal{P} $ is a state transition probability matrix,       \\n  $ \\\\mathcal{P}_{ss'} = \\\\mathbb{P}[S_{t+1} = s' | S_t = s] $  \\n- $ \\\\mathcal{R} $ is a reward function,       \\n  $ \\\\mathcal{R}_s =  \\\\mathbb{E}[R_{t+1} | S_t=s]$   \\n- $ \\\\gamma $ is a discount factor,  $ \\\\gamma \\\\in [0,1]$     \\n\\n### Return  \\nThe return Gt is the total discounted reward from time-step t.  \\n$ G_t = R_{t+1} +  R_{t+2} + ... = \\\\sum_{k=0} ^\\\\infty \\\\gamma^kR_{t+k+1} $     \\n\\n### Value Function\\nThe state value function v(s) of an MRP is the expected return starting from state s    \\n$  v(s) = \\\\mathbb{E}[G_t | S_t =s] $    \\n\\n### Bellman Equation for MRPs\\n$ \\n\\\\begin{align*}\\n    v(s) &= \\\\mathbb{E}[G_t | S_t =s]  \\\\\\\\ \\n         &= \\\\mathbb{E}[R_{t+1} + \\\\gamma v(S_{t+1}) | S_t =s]     \\\\\\\\     \\n         &= \\\\mathcal{R}_s + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'} v(s')\\n\\\\end{align*}    \\n$  \\n\\n### Bellman Equation in Matrix Form  \\n$ \\n\\\\begin{align*}\\nv &= \\\\mathcal{R} + \\\\gamma \\\\mathcal{P}v \\\\\\\\\\nv &= (1-\\\\gamma\\\\mathcal{P})^{-1}\\\\mathcal{R}\\n\\\\end{align*}\\n$    \\n$ \\n\\\\begin{bmatrix}\\n    v_{1} \\\\\\\\\\n    \\\\vdots \\\\\\\\\\n    v_{m}  \\n\\\\end{bmatrix} = \\n\\\\begin{bmatrix}\\n    \\\\mathcal{R}_{1} \\\\\\\\\\n    \\\\vdots \\\\\\\\\\n    \\\\mathcal{R}_{m}  \\n\\\\end{bmatrix} + r\\n\\\\begin{bmatrix}\\n    \\\\mathcal{P}_{11} \\\\dots \\\\mathcal{P}_{1n} \\\\\\\\\\n    \\\\vdots \\\\\\\\\\n    \\\\mathcal{P}_{n1} \\\\dots \\\\mathcal{P}_{nn}  \\n\\\\end{bmatrix} \\n\\\\begin{bmatrix}\\n    v_{1} \\\\\\\\\\n    \\\\vdots \\\\\\\\\\n    v_{m}  \\n\\\\end{bmatrix} \\n$\\n\\n## Markov Decision Processes\\n### definition\\nA Markov decision process (MDP) is a Markov reward process with decisions.  \\nIt is an environment in which all states are Markov.    \\nA Markov Decision Process is a tuple  $ <\\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, \\\\mathcal{R}, \\\\gamma> $    \\n\\n- $ \\\\mathcal{S} $ is a finite set of states   \\n- $ \\\\mathcal{A} $ is a finite set of actions\\n- $ \\\\mathcal{P} $ is a state transition probability matrix,       \\n  $ \\\\mathcal{P}_{ss'}^a = \\\\mathbb{P}[S_{t+1} = s' | S_t=s, A_t = a] $  \\n- $ \\\\mathcal{R} $ is a reward function,       \\n  $ \\\\mathcal{R}_s^a =  \\\\mathbb{E}[R_{t+1} | S_t=s, A_t = a] $   \\n- $ \\\\gamma $ is a discount factor,  $ \\\\gamma \\\\in [0,1]$  \\n\\n### policy\\nA policy $ \\\\pi $ is a distribution over actions given states,     \\n$ \\\\pi(a|s) =\\\\mathbb{P}[A_t=a|S_t=s] $     \\nPolicies are stationary (time-independent),     \\n$ A_t \\\\sim \\\\pi(\\\\cdot|S_t) $\\n\\n- Given an MDP $ \\\\mathcal{M} = <\\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, \\\\mathcal{R}, \\\\gamma> $\\n- The state sequence $ S_1, S_2, S_3, \\\\dots $ is a Markov process $ <\\\\mathcal{S}, \\\\mathcal{P^\\\\pi}> $\\n- The state and reward sequence $ S_1, S_2, S_3, \\\\dots $ is a Markov reward process  $ <\\\\mathcal{S}, \\\\mathcal{P}^\\\\pi, \\\\mathcal{R}^\\\\pi, \\\\gamma> $   \\n- where\\n$\\n\\\\begin{align*}\\n    \\\\mathcal{P}_{ss'}^\\\\pi &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  \\\\mathcal{P}_{ss'}^a \\\\\\\\\\n    \\\\mathcal{R}_{s}^\\\\pi &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  \\\\mathcal{R}_{s}^a\\n\\\\end{align*}\\n$\\n\\n### value function\\nThe state-value function $ v_\\\\pi(s) $  of an MDP is the expected return starting from state s, and then following policy $ \\\\pi $    \\n$  v_\\\\pi(s) = \\\\mathbb{E}_\\\\pi[G_t | S_t =s] $    \\n\\nThe action-value function $ q_\\\\pi(s,a) $ is the expected return starting from state s, taking action a, and then following policy $ \\\\pi $   \\n$  q_\\\\pi(s,a) = \\\\mathbb{E}_\\\\pi[G_t | S_t =s,A_t=a] $    \\n\\n### Bellman Expectation Equation\\n**state-value function**       \\n$ \\n\\\\begin{align*}\\nv_\\\\pi(s) &=\\\\mathbb{E}_\\\\pi[R_{t+1} + \\\\gamma v_\\\\pi(S_{t+1}) | S_t =s] \\\\\\\\\\nv_\\\\pi(s) &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s) q_\\\\pi(s,a) \\\\\\\\\\n         &= \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a|s)  (\\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_\\\\pi(s') ) \\n\\\\end{align*}\\n$  \\n\\n**action-value function**   \\n$\\n\\\\begin{align*}\\nq_\\\\pi(s,a) &= \\\\mathbb{E}_\\\\pi[R_{t+1} + \\\\gamma q_\\\\pi(S_{t+1}, A_{t+1})| S_t =s,A_t=a] \\\\\\\\\\nq_\\\\pi(s,a) &= \\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_\\\\pi(s') \\\\\\\\\\n           &= \\\\mathcal{R}_s^a + \\\\gamma \\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a \\\\sum_{a \\\\in \\\\mathcal{A}} \\\\pi(a'|s') q_\\\\pi(s',a')\\n\\\\end{align*}\\n$ \\n\\n### Bellman Equation in Matrix Form  \\n$ \\n\\\\begin{align*}\\nv_\\\\pi &= \\\\mathcal{R}_\\\\pi + \\\\gamma \\\\mathcal{P}_\\\\pi v \\\\\\\\\\nv_\\\\pi &= (1-\\\\gamma\\\\mathcal{P}_\\\\pi)^{-1}\\\\mathcal{R}_\\\\pi\\n\\\\end{align*}\\n$    \\n\\n### Optimal Value Function\\n#### definition\\nThe optimal state-value function $ v_*(s)$ is the maximum value function over all policies    \\n$ v_*(s)=\\\\max\\\\limits_{\\\\pi}v_{\\\\pi}(s) $    \\nThe optimal action-value function q‚á§(s,a) is the maximum action-value function over all policies    \\n$ q_*(s,a)=\\\\max\\\\limits_{\\\\pi}v_{\\\\pi}(s,a) $    \\n\\n- The optimal value function specifies the best possible performance in the MDP.\\n- An MDP is ‚Äúsolved‚Äù when we know the optimal value fn. \\n\\n#### Optimal policy\\nDefine a partial ordering over policies\\n$ \\\\pi \\\\geqslant \\\\pi' \\\\ if\\\\ v_\\\\pi(s) \\\\geqslant v_\\\\pi'(s), \\\\forall s $  \\n**Theorem** \\nFor any Markov Decision Process \\n\\n- There exists an optimal policy $ \\\\pi_*$ that is better than or equal to all other policies,     \\n$ \\\\pi_* \\\\geqslant \\\\pi, \\\\forall \\\\pi $\\n- All optimal policies achieve the optimal value function   \\n$ v_{\\\\pi_*}(s) = v_*(s) $\\n- All optimal policies achieve the optimal action-value function    \\n$ q_{\\\\pi_*}(s,a) = q_*(s,a) $\\n\\n#### Finding an Optimal Policy\\n\\nAn optimal policy can be found by maximising over $ q_*(s,a) $,   \\n$ \\n\\\\pi_*(a|s) = \\n\\\\begin{cases}\\n    1, & \\\\text{if }  a= \\\\operatorname*{arg\\\\,max}\\\\limits_{a \\\\in \\\\mathcal{A}} q_*(s,a))    \\\\\\\\\\n    0, & \\\\text{otherwise}\\n\\\\end{cases}\\n$ \\n\\n- There is always a deterministic optimal policy for any MDP \\n- If we know $ q_*(s,a) $, we immediately have the optimal policy\\n\\n$  \\nv_*(s) = \\\\max\\\\limits_a q_* (s,a)  \\\\ \\\\ \\\\ \\\\ \\\\    q_*(s,a) = R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') \\\\\\\\ \\nv_*(s) = \\\\max\\\\limits_a R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a v_*(s') \\\\\\\\ \\nq_*(s,a) = R_s^a + \\\\gamma\\\\sum_{s' \\\\in \\\\mathcal{S}} \\\\mathcal{P}_{ss'}^a \\\\max\\\\limits_a' q_*(s',a')\\n$  \\n\\n### Solving the Bellman Optimality Equation\\n- Bellman Optimality Equation is non-linear \\n- No closed form solution (in general) \\n- Many iterative solution methods\\n    - Value Iteration \\n    - Policy Iteration \\n    - Q-learning \\n    - Sarsa\\n\\n## Extensions to MDPs\\n...\\n\"},\"fields\":{\"fpath\":\"/series/rl/2_markov_decision_processes/\",\"permalink\":\"/2017/03/09/2-Markov-Decision-Processes\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"2 Markov Decision Processes\",\"date\":\"2017-03-09\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}},\"1_introduction_to_reinforcement_learning\":{\"post\":{\"internal\":{\"content\":\"\\n## About Reinforcement Learning  \\n\\nWhat makes reinforcement learning different from other machine learning paradigms?   \\n\\n- There is no supervisor, only a reward signal\\n- Feedback is delayed, not instantaneous\\n- Time really matters (sequential, non i.i.d data)\\n- Agent‚Äôs actions affect the subsequent data it receives\\n\\n## The Reinforcement Learning Problem\\n### Rewards\\n\\n**Definition (Reward Hypothesis)**   \\nAll goals can be described by the maximisation of expected cumulative reward\\n\\n### Sequential Decision Making\\n- Goal: select actions to maximise total future reward\\n- Actions may have long term consequences\\n- Reward may be delayed\\n- It may be better to sacrifice immediate reward to gain more long-term reward\\n\\n\\n## Inside An RL Agent\\n\\nAn RL agent may include one or more of these components:   \\n- Policy: agent‚Äôs behaviour function  \\n- Value function: how good is each state and/or action \\n- Model: agent‚Äôs representation of the environment \\n\\n\\n### policy\\nA policy is the agent‚Äôs behaviour   \\nIt is a map from state to action, e.g. Deterministic policy: a = ùúã(s)   \\nStochastic policy:   $ \\\\pi(a|s) = \\\\mathbb{P}[A_t=t |S_t=s] $\\n \\n### value function\\nValue function is a prediction of future reward    \\nUsed to evaluate the goodness/badness of states    \\nAnd therefore to select between actions, e.g.\\n$$ \\nv_{\\\\pi}(s) = \\\\mathbb{E}[R_{t+1} + \\\\gamma R_{t+2} +\\\\gamma^{t+2} R_{t+3} + .. | S_{t} = s] \\n$$\\n\\n### model \\nA model predicts what the environment will do next P predicts the next state  \\nR predicts the next (immediate) reward, e.g.  \\n$ P_{ss{}'}^{\\\\alpha }=\\\\mathbb{P}[S_{t+1}=s{}'|S_{t} =s,A_{t} =a] $\\n$ R_{s}^{\\\\alpha }=\\\\mathbb{E}[R_{t+1} |S_{t} =s,A_{t} =a] $\\n\\n## Problems within Reinforcement Learning\\n\"},\"fields\":{\"fpath\":\"/series/rl/1_introduction_to_reinforcement_learning/\",\"permalink\":\"/2017/03/08/1-Introduction-to-Reinforcement-Learning\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"1 Introduction to Reinforcement Learning\",\"date\":\"2017-03-08\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}},\"0_preface\":{\"post\":{\"internal\":{\"content\":\"\\n# Introduction\\nThis is a series of reinforcement learning materials\\n\\n- book: [Reinforcement Learning: An Introduction (2nd Edition)](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)\\n- lectures: [David Silver's Reinforcement Learning Course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) \\n- exercises: [Reinforcement learning](https://github.com/dennybritz/reinforcement-learning)\\n\"},\"fields\":{\"fpath\":\"/series/rl/0_preface/\",\"permalink\":\"/2017/03/07/series-rl-preface\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"0 Preface\",\"date\":\"2017-03-07\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":\"series-rl-preface\"}}}},\"post\":{\"internal\":{\"content\":\"\\nhave not been revised yet.\"},\"fields\":{\"fpath\":\"/series/rl/\",\"permalink\":\"/2017/03/14/Reinforcement-learning\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Reinforcement learning\",\"date\":\"2017-03-14\",\"tags\":[\"machine learning\",\"reinforcement learning\"],\"desc\":null,\"slug\":null}}}","next":{"internal":{"content":"\n# Definition\n- MDP               \nA Markov decision process (MDP) is a Markov reward process with decisions.  \nIt is an environment in which all states are Markov.    \nA Markov Decision Process is a tuple  $ <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma> $    \n\n    - $ \\mathcal{S} $ is a finite set of states   \n    - $ \\mathcal{A} $ is a finite set of actions\n    - $ \\mathcal{P} $ is a state transition probability matrix,       \n    $ \\mathcal{P}_{ss'}^a = \\mathbb{P}[S_{t+1} = s' | S_t=s, A_t = a] $  \n    - $ \\mathcal{R} $ is a reward function,       \n    $ \\mathcal{R}_s^a =  \\mathbb{E}[R_{t+1} | S_t=s, A_t = a] $   \n    - $ \\gamma $ is a discount factor,  $ \\gamma \\in [0,1]$  \n\n- state action relation         \n$\n\\begin{align*}\n    v_\\pi(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a|s) q_\\pi(s,a)               \\\\\n    q_\\pi(s,a) &= \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s')  \n\\end{align*}\n$    \n\n- state-value function          \n$ v_\\pi(s)  = \\sum_{a \\in \\mathcal{A}} \\pi(a|s)  (\\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s') ) $ \n\n- action-value function          \n    $ q_\\pi(s,a)  = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\sum_{a \\in \\mathcal{A}} \\pi(a'|s') q_\\pi(s',a') $  \n\n- return        \n    $ G_t = R_{t+1} +  R_{t+2} + ... = \\sum\\limits_{k=0} ^\\infty \\gamma^kR_{t+k+1} $              \n$\n\\begin{align*}\nV_\\pi(s) &= \\mathbb{E}_\\pi \\left \\{ {G_t | s_t = s} \\right \\}                                                   &  \\color{red} {MC} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma V_\\pi(s_{t+1}) | s_t = s\\right \\}                          &  \\color{red} {TD(0)} \\\\\n         &= \\sum_a\\pi(s,a)\\sum_{s'}\\mathbb{P}_{ss'}^a [R_{ss'}^a + \\gamma V_\\pi(s')]                            &  \\color{red} {DP}\n\\end{align*}\n$\n\n# Method\n \n## DP           \n$ V(S_t) \\leftarrow \\mathbb{E}_\\pi[ R_{t+1} + \\gamma V(S_{t+1})] $    \n\nThe way to find a policy $ \\pi_*$ over $ \\pi $              \n$ \n\\pi_*(a|s) = \n\\begin{cases}\n    1, & \\text{if }  a= \\operatorname*{arg\\,max}\\limits_{a \\in \\mathcal{A}} q_*(s,a))    \\\\\n    0, & \\text{otherwise}\n\\end{cases}\n$                          \n$  v_*(s) = \\max\\limits_a q_* (s,a)  \\ \\ \\ \\ \\    q_*(s,a) = R_s^a + \\gamma\\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s') $\n\n## MC       \n $ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t))$       \n\n## TD           \n$ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1})-V(S_t))$            \n"},"fields":{"fpath":"/series/rl/rl_essence/","permalink":"/2017/03/14/RL-Essence","category":"series"},"frontmatter":{"title":"RL Essence","date":"2017-03-14","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}},"prev":{"internal":{"content":"\n# Key Points\n**Return**      \n$\n\\begin{align*}\nV_\\pi(s) &= \\mathbb{E}_\\pi\\left \\{ \\sum_{k=0}^\\infty \\gamma_k R_{t+k+1} | s_t = s\\right \\}                      &  \\color{red}  {Definition} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma_k R_{t+k+2} | s_t = s\\right \\}    &  \\color{red}  {Unfolding} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma G_{t+1} | s_t = s\\right \\}                                 &  \\color{red}  {Recursive \\ formula} \n\\end{align*}\n$\n\n**Value function**           \n$\n\\begin{align*}\nV_\\pi(s) &= \\mathbb{E}_\\pi \\left \\{ {G_t | s_t = s} \\right \\}                                                   &  \\color{red} {MC} \\\\\n         &= \\mathbb{E}_\\pi\\left \\{  R_{t+1} + \\gamma V_\\pi(s_{t+1}) | s_t = s\\right \\}                          &  \\color{red} {TD(0)} \\\\\n         &= \\sum_a\\pi(s,a)\\sum_{s'}\\mathbb{P}_{ss'}^a [R_{ss'}^a + \\gamma V_\\pi(s')]                            &  \\color{red} {DP}\n\\end{align*}\n$\n\nMC Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t))$       \nTD Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1})-V(S_t))$            \nDP Backup: $ V(S_t) \\leftarrow \\mathbb{E}_\\pi[ R_{t+1} + \\gamma V(S_{t+1})] $\n\n# Lecture Notes\n[some reading]https://www.tu-chemnitz.de/informatik/KI/scripts/ws0910/ml09_6.pdf\n## introduction\n**Model-free prediction**           \nEstimate the value function of an _unknown_ MDP\n\n## Monte-Carlo Learning\n[MC method](https://en.wikipedia.org/wiki/Monte_Carlo_method) \n\n- MC methods learn directly from episodes of experience\n- MC is model-free: no knowledge of MDP transitions / rewards \n- MC learns from complete episodes: no bootstrapping\n- MC uses the simplest possible idea: value = mean return\n- Caveat: can only apply MC to episodic MDPs \n    - All episodes must terminate\n\n### Some definition\n- Goal: learn $v_\\pi$ from episodes of experience under policy $\\pi$            \n    $ S_1, A_1, R_2, \\dots, S_k \\sim \\pi $\n- Recall that the return is the total discounted reward:            \n    $ G_t = R_{t+1} +  \\gammaR_{t+2} + ... = \\sum_{k=0} ^\\infty \\gamma^kR_{t+k+1} $     \n- Recall that the value function is the expected return:            \n    $  v(s) = \\mathbb{E}[G_t | S_t =s] $    \n- Monte-Carlo policy evaluation uses **empirical mean return** instead of expected return        \n\n\n### MC Policy Evaluation (algorithm)\nThare are two kinds of evaluation method:  _the first time-step_ and _every time-step_     \n\n- The first/Every time-step  t that state s is visited in an episode,       \n- Increment counter ,     $ N(s) \\leftarrow N(s) +1 $       \n- Increment total return, $ S(s) \\leftarrow S(s) +G_t $     \n- Value is estimated by mean return $ V(s) = S(s)/N(s)$     \n- By law of large numbers, $ V(s) \\rightarrow v_\\pi(s) \\text{  as  } N(s) \\rightarrow \\infty$       \n\n\n### Incremental Mean and Incremental MC updates (practice)\n**Incremental Mean**        \nThe Mean $ \\mu_1, \\mu_2, \\dots $ of a sequence $x_1, x_2, \\dots$ can be computed incrementally,                \n$\n\\begin{align*}\n\\mu_k &= \\frac{1}{k}\\sum_{j=1}^kx_j \\\\\n      &= \\frac{1}{k}(x_k + \\sum_{j=1}^{k-1}x_j)  \\\\\n      &= \\frac{1}{k}(x_k + (k-1)\\mu_{k-1})  \\\\\n      &= \\frac{1}{k}x_k  +\\mu_{k-1} - \\frac{1}{k}\\mu_{k-1} \\\\\n      &= \\mu_{k-1} + \\frac{1}{k}(x_k - \\mu_{k-1}) \\\\\n\\end{align*}\n$\n\n**Incremental MC updates**         \n\n- Update V(s) incrementally after episode $ S_1, A_1, R_2, \\dots, S_k \\sim \\pi $          \n- For each state $S_t$ with return $G_t$            \n$\n\\begin{align*}    \n    N(s) & \\leftarrow N(s) +1 \\\\\n    V(S_t) & \\leftarrow V(S_t) + \\frac{1}{N(s)}(G_t - V(S_t))\n\\end{align*}\n$\n- In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.                   \n$ V(S_t)  \\leftarrow V(S_t) + \\alpha(G_t - V(S_t)) $     \n<span style=\"color:red\">what is stationary/non-stationary problems ???</span>   \n\n## Temporal-Difference Learning\n- TD methods learn directly from episodes of experience\n- TD is **model-free**: no knowledge of MDP transitions / rewards \n- TD learns from incomplete episodes, by bootstrapping\n- TD updates a guess towards a guess\n\n### TD Policy Evaluation\n- Simplest temporal-difference learning algorithm: TD(0) \n    - Update value $V(S_t)$ toward estimated return $ R_{t+1} + \\gamma V(S_{t+1})$  \n        $ V(S_t) \\leftarrow + \\alpha(R_{t+1} + \\gamma V(S_{t+1} - V(S_t))$    \n    - $ R_{t+1} + \\gamma V(S_{t+1}$ is called the TD target       \n    - $ \\delta _t = R_{t+1} + \\gamma V(S_{t+1} - V(S_t)$ is called the TD error     \n\n## MC vs TD\n### Bias\n- TD can learn before knowing the final outcome\n    - TD can learn online after every step\n    - MC must wait until end of episode before return is known\n- TD can learn without the final outcome \n    - TD can learn from incomplete sequences\n    - MC can only learn from complete sequences\n    - TD works in continuing (non-terminating) environments \n    - MC only works for episodic (terminating) environments\n\n- The return value\n    - Return $ G_t = R_{t+1} +  R_{t+2} + ... \\gamma^{T-1}R_T $  is unbiased estimate of $ v_\\pi(S_t)$\n    - True TD target $ R_{t+1} + \\gamma v_\\pi(S_{t+1})$ is is unbiased estimate of $ v_\\pi(S_t)$\n    - TD target $ R_{t+1} + \\gamma V(S_{t+1})$ is biased estimate of vœÄ(St)\n    - TD target is much lower variance than the return:\n        - Return depends on many random actions, transitions, rewards \n        - TD target depends on one random action, transition, reward\n\n### Variance\n MC | TD\n--- | ---\nMC has high variance, zero bias    |        TD has low variance, some bias                                                                      \nGood convergence properties        |        Usually more efficient than MC                                                 \n(even with function approximation) |        TD(0) converges to $v_\\pi(s)$                                                     \nNot very sensitive to initial value|        (but not always with function approximation)                                                  \nVery simple to understand and use  |        More sensitive to initial value            \n\n### markov property\n- TD exploits Markov property\n    - Usually more efficient in Markov environments\n- MC does not exploit Markov property\n    - Usually more effective in non-Markov environments       \n\nMC Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(G_t-V(S_t))$       \nTD Backup: $ V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1})-V(S_t))$            \nDP Backup: $ V(S_t) \\leftarrow \\mathbb{E}_\\pi[ R_{t+1} + \\gamma V(S_{t+1})] $\n\n## Unified view of reinforcement learning\n- Bootstrapping: update involves an estimate \n    - MC does not bootstrap\n    - DP bootstraps\n    - TD bootstraps\n- Sampling: update samples an expectation \n    - MC samples\n    - DP does not sample \n    - TD samples\n\n![image](/img/content/note/rl/unified_view_of_rl.png)\n\n## TD(Œª)\n### n-step return\n- Consider the followingn-step returns for $ n= 1,2,3, \\dots, \\infty$         \n$\n\\begin{align*}\nn=1          \\ \\ \\ \\ \\ \\      &  G_t^{(1)} =  R_{t+1} + \\gamma V(S_{t+1})      \\\\\nn=2          \\ \\ \\ \\ \\ \\      &  G_t^{(2)} =  R_{t+1} + \\gamma R_{t+2} + \\gamma^2 V(S_{t+2})      \\\\\n\\vdots       \\ \\ \\ \\ \\ \\      &  \\vdots  \\\\\nn=\\infty     \\ \\ \\ \\ \\ \\      &  G_t^{(\\infty)} =   R_{t+1} + \\gamma R_{t+2} + \\dots +\\gamma^{T-1}R_T      \n\\end{align*}\n$\n- Define the n-step return      \n$ G_t^{(n)} =  R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^n V(S_{t+n})   $         \n- n-step TD learning        \n$ V(S_t) \\leftarrow V(S_t) + \\alpha ( G_t^{(n)} - V(S_t)  )$\n\n### Forward view of TD(Œª)  \n\n**averaging n-step returns**            \n\n- We can averagen-step returns over different n          \n    e.g.  $ \\frac{1}{2}G^{(2)} + \\frac{1}{2}G^{(4)} $  \n\n**Œª-return**            \n\n- The Œª-return $G_t^\\lambda$ combines all n-step returns $G_t^{(n)}$        \n- Using weight $(1-\\lambda)\\lambda^{n-1}$       \n    $ G_t^\\lambda  = (1-\\lambda)\\sum_{n=1}^\\infty \\lambda^{n-1}G_t^{(n)} $       \n- Forward-view TD(Œª)     \n    $ V(S_t) \\leftarrow V(S_t) + \\alpha (G_t^\\lambda - V(S_t)) $\n\n\n\nUpdate value function towards the Œª-return          \nForward-view looks into the future to compute $G_t^\\lambda$           \nLike MC, can only be computed from complete episodes            \n\n### Backward View TD(Œª)  \n- Forward view provides theory\n- Backward view provides mechanism\n- Update online, every step, from incomplete sequences\n\n# Excises\n"},"fields":{"fpath":"/series/rl/4_model_free_predication/","permalink":"/2017/03/13/4-Model-Free-Predication","category":"series"},"frontmatter":{"title":"4 Model Free Predication","date":"2017-03-13","tags":["machine learning","reinforcement learning"],"desc":null,"slug":null}}}}