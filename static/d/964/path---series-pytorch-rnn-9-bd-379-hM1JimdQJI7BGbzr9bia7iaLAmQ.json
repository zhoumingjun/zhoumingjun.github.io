{"pageContext":{"slug":"/series/pytorch/rnn/","toc":"{\"post\":{\"internal\":{\"content\":\"\\nPytorch\"},\"fields\":{\"slug\":\"/series/pytorch/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"pytorch\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\"],\"desc\":null}},\"children\":{\"rnn\":{\"post\":{\"internal\":{\"content\":\"\\nRNN\\n\\ntodo:\\nseq2seq:\\nhttps://www.microsoft.com/en-us/cognitive-toolkit/blog/2016/11/sequence-to-sequence-deep-recurrent-neural-networks-in-cntk-part-1/\"},\"fields\":{\"slug\":\"/series/pytorch/rnn/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"RNN\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}},\"children\":{\"1-Fundamental\":{\"post\":{\"internal\":{\"content\":\"\\n\\n# python3\\n\\n## star operator\\n\\n## map zip lambda\\n \\n## NDArray Indexing\\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\\n\\n\\n# pytorch\\n\\n\\n## RNN\\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\\n\\nAndrej Karpathy’s diagram shows the different pattern in RNN        \\n![RNN](rnn.jpg)\\n\\n### Sequence \\nThe following code show the concept about sequence. \\ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.ones(4, 1, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: step by step\\ninput_0 = input[0,:,:].view(1,1,10)\\ninput_1 = input[1,:,:].view(1,1,10)\\ninput_2 = input[2,:,:].view(1,1,10)\\ninput_3 = input[3,:,:].view(1,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1, hidden_0)\\noutput_2, hidden_2 = model(input_2, hidden_1)\\noutput_3, hidden_3 = model(input_3, hidden_2)\\n\\n\\nprint(hidden)\\nprint(output)\\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\\nprint(output_0, output_1, output_2,output_3)\\n\\n\\n# compare option1 & option2\\nprint ((output[0]==output_0).sum().item() == hidden_size)\\nprint ((output[1]==output_1).sum().item() == hidden_size)\\nprint ((output[2]==output_2).sum().item() == hidden_size)\\nprint ((output[3]==output_3).sum().item() == hidden_size)\\n\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n# relation between hidden & output\\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n```\\n\\nAs the result of the above code shown   \\n1. the output contains all outputs of each iteration.\\n2. the output is the collection of hidden state of each iteration\\n3. from the last layer of the LSTM (in much layer network, see the official document)\\n\\n\\n\\n\\n### Batch Processing\\n\\n### Batch processing with variable length sequences\\n \\n\\n\\n\\nhttps://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\\n\\n# refs\\n\\nhttps://www.pythonlikeyoumeanit.com/intro.html\\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558\"},\"fields\":{\"slug\":\"/series/pytorch/rnn/1-Fundamental/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Fundamental\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}}}}}}","next":{"internal":{"content":"\n\n# python3\n\n## star operator\n\n## map zip lambda\n \n## NDArray Indexing\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\n\n\n# pytorch\n\n\n## RNN\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\n\nAndrej Karpathy’s diagram shows the different pattern in RNN        \n![RNN](rnn.jpg)\n\n### Sequence \nThe following code show the concept about sequence. \ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \n\n```python\nimport torch\n\ninput_size = 10\nhidden_size = 20\nnum_layers = 1\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\n\n# data\ninput = torch.ones(4, 1, 10)\n\n# option1: sequence\noutput, hidden = model(input)\n\n# option2: step by step\ninput_0 = input[0,:,:].view(1,1,10)\ninput_1 = input[1,:,:].view(1,1,10)\ninput_2 = input[2,:,:].view(1,1,10)\ninput_3 = input[3,:,:].view(1,1,10)\n\noutput_0, hidden_0 = model(input_0)\noutput_1, hidden_1 = model(input_1, hidden_0)\noutput_2, hidden_2 = model(input_2, hidden_1)\noutput_3, hidden_3 = model(input_3, hidden_2)\n\n\nprint(hidden)\nprint(output)\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\nprint(output_0, output_1, output_2,output_3)\n\n\n# compare option1 & option2\nprint ((output[0]==output_0).sum().item() == hidden_size)\nprint ((output[1]==output_1).sum().item() == hidden_size)\nprint ((output[2]==output_2).sum().item() == hidden_size)\nprint ((output[3]==output_3).sum().item() == hidden_size)\n\n\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"\n# relation between hidden & output\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\n\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"\n```\n\nAs the result of the above code shown   \n1. the output contains all outputs of each iteration.\n2. the output is the collection of hidden state of each iteration\n3. from the last layer of the LSTM (in much layer network, see the official document)\n\n\n\n\n### Batch Processing\n\n### Batch processing with variable length sequences\n \n\n\n\nhttps://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\n\n# refs\n\nhttps://www.pythonlikeyoumeanit.com/intro.html\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558"},"fields":{"slug":"/series/pytorch/rnn/1-Fundamental/","category":"series"},"frontmatter":{"title":"Fundamental","date":"2018-09-07T10:16:47Z","tags":["machine learning","pytorch","rnn"],"desc":null}},"prev":{"internal":{"content":"\nPytorch"},"fields":{"slug":"/series/pytorch/","category":"series"},"frontmatter":{"title":"pytorch","date":"2018-09-07T10:16:47Z","tags":["machine learning","pytorch"],"desc":null}}}}