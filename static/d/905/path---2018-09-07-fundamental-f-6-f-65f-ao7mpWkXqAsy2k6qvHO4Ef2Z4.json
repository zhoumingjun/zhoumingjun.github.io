{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","sourceUrl":"https://github.com/zhoumingjun/zhoumingjun.github.io/blob/source/","siteUrl":"https://zhoumingjun.github.io","disqusShortname":"zhoumingjun"}},"markdownRemark":{"id":"80d99cfc-a5a6-5f69-83b3-f1a506366457","html":"<h1>python3</h1>\n<ul>\n<li>star operator</li>\n<li>map zip lambda</li>\n<li>NDArray Indexing\n<a href=\"https://docs.scipy.org/doc/numpy/user/basics.indexing.html\">https://docs.scipy.org/doc/numpy/user/basics.indexing.html</a></li>\n</ul>\n<h1>pytorch</h1>\n<h2>RNN</h2>\n<p>Refer to <a href=\"https://pytorch.org/docs/stable/nn.html#lstm\">https://pytorch.org/docs/stable/nn.html#lstm</a></p>\n<p>Andrej Karpathy’s diagram shows the different pattern in RNN<br>\n<a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/rnn-215fcabac4361840573d5f3123d532d0-cf2b7.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n  \n  <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 31.34535367545076%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB24FB/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAAAEQ/9oACAEBAAY/AhT/xAAZEAABBQAAAAAAAAAAAAAAAAAAARExQXH/2gAIAQEAAT8hYkJen//aAAwDAQACAAMAAAAQ9B//xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPxCI/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHBABAAIBBQAAAAAAAAAAAAAAAQARITFBcaHB/9oACAEBAAE/ENQmw+wFC6xfcOeSf//Z'); background-size: cover; display: block;\"\n    >\n      <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\"\n        alt=\"RNN\"\n        title=\"\"\n        src=\"/static/rnn-215fcabac4361840573d5f3123d532d0-f8fb9.jpg\"\n        srcset=\"/static/rnn-215fcabac4361840573d5f3123d532d0-e8976.jpg 148w,\n/static/rnn-215fcabac4361840573d5f3123d532d0-63df2.jpg 295w,\n/static/rnn-215fcabac4361840573d5f3123d532d0-f8fb9.jpg 590w,\n/static/rnn-215fcabac4361840573d5f3123d532d0-cf2b7.jpg 721w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n    </span>\n  </span>\n  \n  </a>\n    </p>\n<h3>Sequence</h3>\n<p>The following code show the concept about sequence.\ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n\ninput_size <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nhidden_size <span class=\"token operator\">=</span> <span class=\"token number\">20</span>\nnum_layers <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n\n<span class=\"token comment\"># model</span>\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># data</span>\n<span class=\"token builtin\">input</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># option1: sequence</span>\noutput<span class=\"token punctuation\">,</span> hidden <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># option2: step by step</span>\ninput_0 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ninput_1 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ninput_2 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ninput_3 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n\noutput_0<span class=\"token punctuation\">,</span> hidden_0 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_0<span class=\"token punctuation\">)</span>\noutput_1<span class=\"token punctuation\">,</span> hidden_1 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_1<span class=\"token punctuation\">,</span> hidden_0<span class=\"token punctuation\">)</span>\noutput_2<span class=\"token punctuation\">,</span> hidden_2 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_2<span class=\"token punctuation\">,</span> hidden_1<span class=\"token punctuation\">)</span>\noutput_3<span class=\"token punctuation\">,</span> hidden_3 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_3<span class=\"token punctuation\">,</span> hidden_2<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>hidden<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>hidden_0<span class=\"token punctuation\">,</span> hidden_1<span class=\"token punctuation\">,</span> hidden_2<span class=\"token punctuation\">,</span>hidden_3<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>output_0<span class=\"token punctuation\">,</span> output_1<span class=\"token punctuation\">,</span> output_2<span class=\"token punctuation\">,</span>output_3<span class=\"token punctuation\">)</span>\n\n\n<span class=\"token comment\"># compare option1 &amp; option2</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>output_0<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>output_1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>output_2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>output_3<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"</span>\n<span class=\"token comment\"># relation between hidden &amp; output</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>hidden_0<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>hidden_1<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>hidden_2<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token operator\">==</span>hidden_3<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> hidden_size<span class=\"token punctuation\">)</span>\n<span class=\"token triple-quoted-string string\">\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"</span></code></pre></div>\n<p>As the result of the above code shown   </p>\n<ol>\n<li>the output contains all outputs of each iteration.</li>\n<li>the output is the collection of hidden state of each iteration</li>\n<li>from the last layer of the LSTM (in much layer network, see the official document)</li>\n</ol>\n<h3>Batch Processing</h3>\n<p>In fact, pytorch handle data in batch.<br>\nIt’s more quickly, and save time.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n\ninput_size <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nhidden_size <span class=\"token operator\">=</span> <span class=\"token number\">20</span>\nnum_layers <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n\n<span class=\"token comment\"># model</span>\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># data</span>\n<span class=\"token builtin\">input</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># option1: sequence</span>\noutput<span class=\"token punctuation\">,</span> hidden <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># option2: one by one</span>\ninput_0 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ninput_1 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ninput_2 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\ninput_3 <span class=\"token operator\">=</span> <span class=\"token builtin\">input</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n\noutput_0<span class=\"token punctuation\">,</span> hidden_0 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_0<span class=\"token punctuation\">)</span>\noutput_1<span class=\"token punctuation\">,</span> hidden_1 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_1<span class=\"token punctuation\">)</span>\noutput_2<span class=\"token punctuation\">,</span> hidden_2 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_2<span class=\"token punctuation\">)</span>\noutput_3<span class=\"token punctuation\">,</span> hidden_3 <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>input_3<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">#compare</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token operator\">-</span> output_0<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token operator\">-</span> output_1<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token operator\">-</span> output_2<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token operator\">-</span> output_3<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token triple-quoted-string string\">\"\"\"\ntensor(8.1956e-08, grad_fn=&lt;SumBackward0>)\ntensor(3.2596e-09, grad_fn=&lt;SumBackward0>)\ntensor(9.1270e-08, grad_fn=&lt;SumBackward0>)\ntensor(5.1223e-08, grad_fn=&lt;SumBackward0>)\n\"\"\"</span></code></pre></div>\n<p>The above code process inputs in batch.\nOutput of shape is (seq<em>len, batch, num</em>directions * hidden_size)<br>\nWe can get the output according to the seq and batch<br>\noutput[i,j,:]: get the i iteration output of the sample j<br>\noutput[-1,j,:] get the last output of the sample j  </p>\n<h3>Batch processing with variable length sequences</h3>\n<p>Many real cases need to handle variable length sequences.\nE.g. in nlp, the sentence length is variable, the character count of a word is variable.</p>\n<p>Pytorch introduce several helper functions to handle this.</p>\n<p>helper functions</p>\n<ul>\n<li>torch.nn.utils.rnn.pack_sequence</li>\n<li>torch.nn.utils.rnn.pad_sequence</li>\n<li>torch.nn.utils.rnn.pad<em>packed</em>sequence</li>\n<li>torch.nn.utils.rnn.pack<em>padded</em>sequence</li>\n</ul>\n<p>helper structure</p>\n<ul>\n<li>\n<p>PackedSequence</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>rnn <span class=\"token keyword\">import</span> pad_sequence<span class=\"token punctuation\">,</span> pack_sequence<span class=\"token punctuation\">,</span> pack_padded_sequence<span class=\"token punctuation\">,</span> pad_packed_sequence\n\ninput_size <span class=\"token operator\">=</span> <span class=\"token number\">2</span>\nhidden_size<span class=\"token operator\">=</span> <span class=\"token number\">5</span>\nnum_layers <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\nnClasses <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\nnSamples <span class=\"token operator\">=</span> <span class=\"token number\">10</span>\n\na <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">)</span>\nb <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">)</span>\nc <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pad</span>\npad <span class=\"token operator\">=</span> pad_sequence<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>c<span class=\"token punctuation\">,</span>b<span class=\"token punctuation\">,</span>a<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"pad result\"</span><span class=\"token punctuation\">,</span> pad<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pack</span>\npack <span class=\"token operator\">=</span> pack_sequence<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>c<span class=\"token punctuation\">,</span>b<span class=\"token punctuation\">,</span>a<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"pack result:\"</span><span class=\"token punctuation\">,</span> pack<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> pack<span class=\"token punctuation\">.</span>batch_sizes<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pack_padded</span>\npack_padded <span class=\"token operator\">=</span> pack_padded_sequence<span class=\"token punctuation\">(</span>pad<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"pack_padded result:\"</span><span class=\"token punctuation\">,</span> pack_padded<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> pack_padded<span class=\"token punctuation\">.</span>batch_sizes<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pad_packed</span>\npad_packed_data<span class=\"token punctuation\">,</span> pad_packed_lengths <span class=\"token operator\">=</span> pad_packed_sequence<span class=\"token punctuation\">(</span>pack<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"pad_packed result:\"</span><span class=\"token punctuation\">,</span> pad_packed_data<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">,</span>pad_packed_lengths<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># pattern</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"\nprepare data/model/indices\n\"\"\"</span>\n\n<span class=\"token comment\"># data</span>\ninputs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\ntargets <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> idx <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>nSamples<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># set random len of input , and set the len as target</span>\n    <span class=\"token comment\"># input: ones(len, input_size)</span>\n    <span class=\"token comment\"># target: len</span>\n    <span class=\"token builtin\">len</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>randint<span class=\"token punctuation\">(</span>nSamples<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1</span>\n    sample <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">)</span>\n    inputs<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>sample<span class=\"token punctuation\">)</span>\n    targets<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># model</span>\nmodel <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\ndemo <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>  input_size<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"sample sequence result\"</span><span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">(</span>demo<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># indices</span>\nsample_length <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">]</span>\n_<span class=\"token punctuation\">,</span> indices_sorted <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span>sample_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> descending<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n_<span class=\"token punctuation\">,</span> indices_restore <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>indices_sorted<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"sample length:\"</span><span class=\"token punctuation\">,</span> sample_length<span class=\"token punctuation\">)</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"\noption1:\npre-process inputs\nsort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)\n\ntargets &lt;-> outputs  \n\"\"\"</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"option1\"</span><span class=\"token punctuation\">)</span>\n\n \n\n<span class=\"token comment\"># sort inputs</span>\ninputs_sorted <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>inputs<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> indices_sorted<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># pack inputs</span>\npack <span class=\"token operator\">=</span> pack_sequence<span class=\"token punctuation\">(</span>inputs_sorted<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># rnn ...</span>\noutputs<span class=\"token punctuation\">,</span> hidden <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>pack<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># unpack</span>\noutput_unpacked<span class=\"token punctuation\">,</span> unpack_outputs_length <span class=\"token operator\">=</span> pad_packed_sequence<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">)</span>\nlast_state <span class=\"token operator\">=</span> output_unpacked<span class=\"token punctuation\">[</span>unpack_outputs_length<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>x <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># unsort</span>\nunsorted_last_state <span class=\"token operator\">=</span> last_state<span class=\"token punctuation\">[</span>indices_restore<span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>tup<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tup<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> tup<span class=\"token punctuation\">[</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> tup <span class=\"token keyword\">in</span>   <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>inputs<span class=\"token punctuation\">,</span> targets<span class=\"token punctuation\">,</span> unsorted_last_state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"\noption2 \npre-process (inputs, targets)\nsort (inputs, targets)-> pack(inputs) -> rnn -> unpack\n\ntargets(sorted) &lt;--> outputs  \n\"\"\"</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"option2\"</span><span class=\"token punctuation\">)</span>\nbatch <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>inputs<span class=\"token punctuation\">,</span> targets<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># sort inputs</span>\nbatch_sorted <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>batch<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> indices_sorted<span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># pack inputs</span>\npack <span class=\"token operator\">=</span> pack_sequence<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>tup<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> tup <span class=\"token keyword\">in</span> batch_sorted<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># rnn ...</span>\noutputs<span class=\"token punctuation\">,</span> hidden <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>pack<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># unpack</span>\noutput_unpacked<span class=\"token punctuation\">,</span> unpack_outputs_length <span class=\"token operator\">=</span> pad_packed_sequence<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">)</span>\nlast_state <span class=\"token operator\">=</span> output_unpacked<span class=\"token punctuation\">[</span>unpack_outputs_length<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span>x <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span> <span class=\"token punctuation\">,</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>tup<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> tup<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> tup<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> tup <span class=\"token keyword\">in</span> <span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>batch_sorted<span class=\"token punctuation\">,</span> last_state<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token triple-quoted-string string\">\"\"\"\npad result torch.Size([7, 3, 2])\npack result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\npack_padded result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\npad_packed result: torch.Size([7, 3, 2]) tensor([7, 5, 3])\nsample sequence result tensor(\n\t\t[[[ 0.0121,  0.0403, -0.0511, -0.0392,  0.2119]],\n        [[ 0.0203,  0.0604, -0.0728, -0.0546,  0.2866]],\n        [[ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169]],\n        [[ 0.0271,  0.0784, -0.0860, -0.0653,  0.3302]],\n        [[ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362]],\n        [[ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390]],\n        [[ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403]],\n        [[ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409]],\n        [[ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412]],\n        [[ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413]]],\n       grad_fn=&lt;CatBackward>)\nsample length: [9, 7, 10, 8, 6, 8, 5, 3, 5, 5]\noption1\n[(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=&lt;SelectBackward>)), \n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=&lt;SelectBackward>)), \n(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=&lt;SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=&lt;SelectBackward>)), 0\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=&lt;SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=&lt;SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=&lt;SelectBackward>)), \n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=&lt;SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=&lt;SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=&lt;SelectBackward>))]\n\noption2\n[(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=&lt;SelectBackward>)), \n(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=&lt;SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=&lt;SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=&lt;SelectBackward>)), \n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=&lt;SelectBackward>)), \n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=&lt;SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=&lt;SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=&lt;SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=&lt;SelectBackward>)), \n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=&lt;SelectBackward>))]\n\"\"\"</span></code></pre></div>\n<p>The sample sequence result show the result of sample[1,1,1,1,1,1,1,1,1,1], so we can get the output of each iteration .<br>\nIn option 1: the outputs’ order are restored, and it’s the same as the orgin data<br>\nIn option 2: the outputs’ order are sorted(not restored), and it’s the same as the sorted data      </p>\n<h1>The source code</h1>\n<p>The soure code can be found on github</p>\n<p><a href=\"https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/1.basic.py\">basic</a> </p>\n<p><a href=\"https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/2.batch.py\">batch</a> </p>\n<p><a href=\"https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/3.variable%20length.py\">variable length</a>   </p>\n<h1>refs</h1>\n<p><a href=\"https://www.pythonlikeyoumeanit.com/intro.html\">https://www.pythonlikeyoumeanit.com/intro.html</a>\n<a href=\"https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\">https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/</a>\n<a href=\"https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558\">https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558</a></p>","fileAbsolutePath":"/home/zhoumingjun/github.com/zhoumingjun/zhoumingjun.github.io/content/series/pytorch/rnn/1-Fundamental/index.md","fields":{"permalink":"/2018/09/07/fundamental","category":"series"},"frontmatter":{"title":"Fundamental","date":"September 07, 2018"}}},"pageContext":{"permalink":"/2018/09/07/fundamental","toc":"{\"children\":{\"rnn\":{\"children\":{\"3-charnn-name-generation\":{\"post\":{\"internal\":{\"content\":\"\\n# Introduction\\nThis is following the idea of https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html#generating-names-with-a-character-level-rnn\\n\\n# Task definition\\nIn training phase, It's a kind of many-to-many task.\\n![task]https://i.imgur.com/JH58tXY.png\\nThe (input, label)  is (name X, X[1:]+\\\"<EOS>\\\"), here we use '-' as <EOS>\\ne.g.\\n(Cotton, otton-)\\n(Alex, lex-)\\n(Doolin, oolin-)\\n\\nIn prediction phase, we use the result of the last iteration as the input of the current iteration.\\n\\n# Details\\n\\n## dataset & dataloader\\n```python\\n\\n# dataset\\nclass NamesDataset(Dataset):\\n    def __init__(self, data_path, transforms=[]):\\n        inputs = []\\n        filepath = path.join(data_path)\\n\\n        with open(filepath) as f:\\n            lines = f.readlines()\\n            inputs += [line.strip() for line in lines]\\n\\n        self.inputs = inputs\\n        self.transforms = transforms\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n        input = self.inputs[idx]\\n        for transform in self.transforms:\\n            input = transform(input)\\n        return input\\n\\n\\nclass UnicodeToAscii(object):\\n    def __init__(self, letters):\\n        self.letters = letters\\n\\n    def __call__(self, s):\\n        return ''.join(\\n            c for c in unicodedata.normalize('NFD', s)\\n            if unicodedata.category(c) != 'Mn'\\n            and c in self.letters\\n        )\\n\\n\\nnamesDataset = NamesDataset('./data/names/English.txt', transforms=[UnicodeToAscii(all_letters)])\\n\\n# dataloader\\nclass NamesClassifier(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super(NamesClassifier, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_size = output_size\\n\\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\\n        self.linear = torch.nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, x, hidden):\\n        x, hidden = self.lstm(x, hx=hidden)\\n        x = x.view(-1, hidden_size)\\n        x = self.linear(x)\\n        return F.log_softmax(x, dim=1), hidden\\n\\n    def initHidden(self):\\n        return torch.zeros(1, self.hidden_size)\\n\\n\\ndataloader = DataLoader(namesDataset, batch_size=1, shuffle=True, num_workers=4, drop_last=True)\\n\\n```\\n\\n## sample  generation\\njust use the original code with minor changes\\n\\n``` python\\nmax_length = 20\\ndef sample(start_letter='A'):\\n    with torch.no_grad():  # no need to track history in sampling\\n        input = name2tensor(start_letter).view(-1, 1, input_size)\\n        output_name = start_letter\\n        hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1,1,hidden_size))\\n\\n        for i in range(max_length):\\n            output, hidden = model(input,hidden)\\n            topv, topi = output.topk(1)\\n            topi = topi[0][0]\\n            if topi == n_letters - 1:\\n                break\\n            else:\\n                letter = all_letters[topi]\\n                output_name += letter\\n\\n            input = name2tensor(letter).view(-1, 1, input_size)\\n\\n        return output_name\\n\\n\\n# Get multiple samples from one category and multiple starting letters\\ndef samples(start_letters='ABC'):\\n    for start_letter in start_letters:\\n        print(sample(start_letter))\\n\\n```\\n## model\\nThe model is different from the last post.\\nHere we need to return the hidden for the next iteration.\\n\\n```python\\nclass NamesClassifier(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super(NamesClassifier, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_size = output_size\\n\\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\\n        self.linear = torch.nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, x, hidden):\\n        x, hidden = self.lstm(x, hx=hidden)\\n        x = x.view(-1, hidden_size)\\n        x = self.linear(x)\\n        return F.log_softmax(x, dim=1), hidden\\n\\n    def initHidden(self):\\n        return torch.zeros(1, self.hidden_size)\\n\\n```\\n## training\\nThe training code is straightforward\\n```python\\n\\ninput_size = n_letters\\nhidden_size = 50\\noutput_size = input_size\\n\\n# define model/optimizer/criterion\\nmodel = NamesClassifier(input_size, hidden_size, output_size)\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\\ncriterion = torch.nn.NLLLoss()\\n\\nfor epoch in range(20):\\n\\n    loss_sum = 0;\\n    nRound = 0\\n    for i_batch, batch in enumerate(dataloader):\\n\\n        inputs = batch\\n\\n        for idx, input in enumerate(inputs):\\n            label = input[1:] + all_letters[-1]\\n\\n            optimizer.zero_grad()\\n\\n            input = name2tensor(input).view(-1, 1, input_size)\\n            label = [all_letters.index(x) for x in label]\\n\\n            output, _ = model(input,  None)\\n            loss = criterion(output, torch.LongTensor(label))\\n\\n            loss.backward()\\n            optimizer.step()\\n\\n            loss_sum += loss\\n            nRound += 1\\n\\n    print(\\\"epoch {} i_batch {} loss {}\\\".format(epoch, i_batch, loss_sum / nRound))\\n    samples()\\n\\n\\\"\\\"\\\"\\nepoch 0 i_batch 3667 loss 2.161202907562256\\nAlling\\nBoule\\nColler\\nepoch 1 i_batch 3667 loss 1.931467890739441\\nAlling\\nBrand\\nCoode\\nepoch 2 i_batch 3667 loss 1.8524298667907715\\nAlling\\nBarring\\nCower\\nepoch 3 i_batch 3667 loss 1.7972185611724854\\nAllins\\nBart\\nCorrin\\nepoch 4 i_batch 3667 loss 1.7545592784881592\\nAllam\\nBarrer\\nCorright\\nepoch 5 i_batch 3667 loss 1.7233166694641113\\nAllam\\nBreatt\\nCrowe\\nepoch 6 i_batch 3667 loss 1.7019587755203247\\nAllam\\nBarris\\nColley\\nepoch 7 i_batch 3667 loss 1.6757022142410278\\nAllam\\nBarrett\\nCorner\\nepoch 8 i_batch 3667 loss 1.6573164463043213\\nAllan\\nBarkin\\nColler\\nepoch 9 i_batch 3667 loss 1.6444448232650757\\nAllam\\nBarr\\nCorrigan\\nepoch 10 i_batch 3667 loss 1.629043459892273\\nAllam\\nBray\\nColley\\nepoch 11 i_batch 3667 loss 1.618332028388977\\nAllam\\nBarr\\nColling\\nepoch 12 i_batch 3667 loss 1.6057300567626953\\nAllam\\nBarty\\nColley\\nepoch 13 i_batch 3667 loss 1.600183129310608\\nAllington\\nBrand\\nColling\\nepoch 14 i_batch 3667 loss 1.5862054824829102\\nAllam\\nBurne\\nCrowther\\nepoch 15 i_batch 3667 loss 1.5789239406585693\\nAllan\\nBurner\\nColl\\nepoch 16 i_batch 3667 loss 1.5749832391738892\\nAllam\\nBarron\\nColley\\nepoch 17 i_batch 3667 loss 1.5670925378799438\\nAllam\\nBarr\\nCower\\nepoch 18 i_batch 3667 loss 1.56125009059906\\nAllam\\nBarr\\nCorre\\nepoch 19 i_batch 3667 loss 1.5633057355880737\\nAllam\\nBarr\\nCollam\\n\\n\\\"\\\"\\\"\\n```\\n\\n# The source code \\n\\nThe soure code can be found on [github](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/5.char-rnn-generation.py)\"},\"fields\":{\"fpath\":\"/series/pytorch/rnn/3-charnn-name-generation/\",\"permalink\":\"/2018/09/11/generating-names-with-a-character-level-rnn\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Generating Names with a Character-Level RNN\",\"date\":\"2018-09-11\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null,\"slug\":null}}},\"2-charnn\":{\"post\":{\"internal\":{\"content\":\"\\n# Introduction\\nThe pytorch's official tutorial introduces the character-level RNN\\nhttps://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\\n\\nHere I will implement it by using lstm\\n\\n# Points\\n1. Classifying Names is a kind of many-to-one RNN task\\n2. The Inputs' length are variable  \\n3. It's better to handle the data in batch to boost the training\\n4. Use dataset and dataloader to handle the raw data\\n   \\n\\n# Task definition\\nTraining data is collection of (name, language)\\nThe task is to preidct the language of the given name\\n\\n# Details\\n\\n## Raw data\\nDownload data from https://download.pytorch.org/tutorial/data.zip\\n\\n## Unicode & ascii\\nplease read my previous post about [unicode](https://zhoumingjun.github.io/note/unicode)    \\nwe will transform the names into NFD (Normalization Form Decomposition) and filter all non-ascii characters\\n\\n## Dataset and dataloader\\nThe dataset is a helper class to load data.\\n\\n```python\\nclass NamesDataset(Dataset):\\n    def __init__(self, data_dir, transforms=[]):\\n        self.data_dir = data_dir\\n\\n        all_langs = []\\n        inputs = []\\n        labels = []\\n\\n        for filepath in glob.glob(path.join(data_dir, \\\"*\\\")):\\n            lang = os.path.splitext(os.path.basename(filepath))[0]\\n            if not lang in all_langs:\\n                all_langs.append(lang)\\n\\n            label = all_langs.index(lang)\\n\\n            with open(filepath) as f:\\n                lines = f.readlines()\\n                inputs += [line.strip() for line in lines]\\n                labels += [label] * len(lines)\\n\\n        self.all_langs = all_langs\\n        self.inputs = inputs\\n        self.labels = labels\\n        self.transforms = transforms\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n\\n        item = self.inputs[idx]\\n        for transform in self.transforms:\\n            item = transform(item)\\n\\n        return item, self.labels[idx]\\n\\n    def getLangs(self):\\n        return self.all_langs\\n```\\n\\n## Model\\nThe model is straitforward.\\n1. use lstm to compute the output\\n2. use linear to map the output's feature to language classes\\n3. use crossentrophy(log_softmax + NLLLoss) to do classification\\n   \\n*attention, here we use packed sequece as the input* \\n\\n```python\\nclass NamesClassifier(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super(NamesClassifier, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_size = output_size\\n\\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\\n        self.linear = torch.nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, x):\\n        x, hidden = self.lstm(x)\\n        output_unpacked, unpack_outputs_length = pad_packed_sequence(x)\\n\\n        seqs = unpack_outputs_length - 1\\n        batch = [x for x in range(len(unpack_outputs_length))]\\n        last_state = output_unpacked[seqs, batch, :].view(-1, self.hidden_size)\\n\\n        x = self.linear(last_state)\\n        return F.log_softmax(x, dim=1)\\n```\\n\\n## Training\\nHere we follow the pattern sort-> pack -> rnn to train the model in batch \\nrefer to [fundamental](https://zhoumingjun.github.io/series/pytorch/rnn/1-Fundamental/)\\n\\n```python\\nfor epoch in range(10):\\n    \\n    # train\\n    loss_sum = 0;\\n    nRound = 0\\n    for i_batch, batch in enumerate(dataloader):\\n\\n        # zero\\n        optimizer.zero_grad()\\n\\n        inputs, labels = batch\\n        # pre-process\\n        inputs = [name2tensor(name) for name in inputs]\\n\\n        inputs_length = [x.size(0) for x in inputs]\\n        _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\\n        _, indices_restore = torch.sort(indices_sorted)\\n\\n        # sort\\n        inputs_sorted = [inputs[x] for x in indices_sorted]\\n        labels_sorted = labels[indices_sorted]\\n\\n        # pack inputs\\n        pack = pack_sequence(inputs_sorted)\\n\\n        # rnn\\n        outputs = model(pack)\\n\\n        # loss/bp/step\\n        loss = criterion(outputs, labels_sorted)\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        loss_sum += loss\\n        nRound += 1\\n        if i_batch % 50 == 0:\\n            print(\\\"epoch {} i_batch {} loss {}\\\".format(epoch, i_batch, loss_sum / nRound))\\n            \\n    # validate\\n    with torch.no_grad():\\n        acc = 0\\n        for i_batch, batch in enumerate(dataloader):\\n            inputs, labels = batch\\n            # pre-process\\n            inputs = [name2tensor(name) for name in inputs]\\n\\n            inputs_length = [x.size(0) for x in inputs]\\n            _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\\n            _, indices_restore = torch.sort(indices_sorted)\\n\\n            # sort\\n            inputs_sorted = [inputs[x] for x in indices_sorted]\\n            labels_sorted = labels[indices_sorted]\\n\\n            # pack inputs\\n            pack = pack_sequence(inputs_sorted)\\n\\n            # rnn\\n            outputs = model(pack)\\n\\n            top_v, topi = torch.topk(outputs, 1)\\n            acc += (topi.view(1, -1) == labels_sorted).sum().item()\\n```\\n\\nHere is the training output\\n\\n```\\nepoch 0 i_batch 0 loss 2.942514419555664\\nepoch 0 i_batch 50 loss 1.760701298713684\\nepoch 0 i_batch 100 loss 1.5069572925567627\\nepoch 0 i_batch 150 loss 1.3581050634384155\\nepoch 0 acc:14758/20074 \\nepoch 1 i_batch 0 loss 0.8439935445785522\\nepoch 1 i_batch 50 loss 0.8764864802360535\\nepoch 1 i_batch 100 loss 0.8679843544960022\\nepoch 1 i_batch 150 loss 0.836237907409668\\nepoch 1 acc:15769/20074 \\nepoch 2 i_batch 0 loss 0.6725496649742126\\nepoch 2 i_batch 50 loss 0.6784597635269165\\nepoch 2 i_batch 100 loss 0.6801468729972839\\nepoch 2 i_batch 150 loss 0.6713106632232666\\nepoch 2 acc:16419/20074 \\nepoch 3 i_batch 0 loss 0.7285385131835938\\nepoch 3 i_batch 50 loss 0.5921918749809265\\nepoch 3 i_batch 100 loss 0.5902780294418335\\nepoch 3 i_batch 150 loss 0.5864360928535461\\nepoch 3 acc:16829/20074 \\nepoch 4 i_batch 0 loss 0.5711963176727295\\nepoch 4 i_batch 50 loss 0.5279501676559448\\nepoch 4 i_batch 100 loss 0.521149218082428\\nepoch 4 i_batch 150 loss 0.5309893488883972\\nepoch 4 acc:17058/20074 \\nepoch 5 i_batch 0 loss 0.5366474390029907\\nepoch 5 i_batch 50 loss 0.46456900238990784\\nepoch 5 i_batch 100 loss 0.47316044569015503\\nepoch 5 i_batch 150 loss 0.4669898450374603\\nepoch 5 acc:17373/20074 \\nepoch 6 i_batch 0 loss 0.4217735528945923\\nepoch 6 i_batch 50 loss 0.4200931489467621\\nepoch 6 i_batch 100 loss 0.43043628334999084\\nepoch 6 i_batch 150 loss 0.4396715462207794\\nepoch 6 acc:17447/20074 \\nepoch 7 i_batch 0 loss 0.31758126616477966\\nepoch 7 i_batch 50 loss 0.38675254583358765\\nepoch 7 i_batch 100 loss 0.399675190448761\\nepoch 7 i_batch 150 loss 0.4018913507461548\\nepoch 7 acc:17867/20074 \\nepoch 8 i_batch 0 loss 0.3606557548046112\\nepoch 8 i_batch 50 loss 0.33634153008461\\nepoch 8 i_batch 100 loss 0.34979167580604553\\nepoch 8 i_batch 150 loss 0.36078399419784546\\nepoch 8 acc:18013/20074 \\nepoch 9 i_batch 0 loss 0.31942570209503174\\nepoch 9 i_batch 50 loss 0.30287066102027893\\nepoch 9 i_batch 100 loss 0.31636151671409607\\nepoch 9 i_batch 150 loss 0.33173689246177673\\nepoch 9 acc:18151/20074 \\n```\\n\\n## Predict\\n\\nAfter training, we can predict names' language using the model.\\n``` python\\n# do some preidct\\nfor i_batch, batch in enumerate(dataloader):\\n    input, label = batch\\n    for idx, input in enumerate(input):\\n        lang, lang_id = predict(input)\\n        print(\\\"input {}, label {}, predict {}, result: {}\\\".format(\\n            input, all_langs[label[idx].item()],\\n            lang,\\n            lang_id == label[idx].item()))\\n\\n    break\\n```\\n\\nHere is the result\\n```\\ninput Purse, label English, predict English, result: True\\ninput Teague, label Irish, predict English, result: False\\ninput Daher, label Arabic, predict Arabic, result: True\\ninput Soma, label Japanese, predict Japanese, result: True\\ninput Mikhail, label Arabic, predict Arabic, result: True\\ninput Pawluk, label Russian, predict Russian, result: True\\ninput Bakaleinikov, label Russian, predict Russian, result: True\\ninput Zhaivoronok, label Russian, predict Russian, result: True\\ninput Fujiwara, label Japanese, predict Japanese, result: True\\ninput Jachikov, label Russian, predict Russian, result: True\\ninput Sardelis, label Greek, predict Greek, result: True\\ninput Yatzenko, label Russian, predict Russian, result: True\\ninput Kabalevsky, label Russian, predict Russian, result: True\\ninput Godo, label Japanese, predict Japanese, result: True\\ninput Imran, label English, predict English, result: True\\ninput Kokkali, label Greek, predict Greek, result: True\\ninput Adam, label Russian, predict English, result: False\\ninput Todorov, label Russian, predict Russian, result: True\\ninput Bastian, label Russian, predict Russian, result: True\\ninput Samson, label French, predict French, result: True\\ninput Bereznitsky, label Russian, predict Russian, result: True\\ninput Bakhmetiev, label Russian, predict Russian, result: True\\ninput Hadad, label Arabic, predict Arabic, result: True\\ninput Sung, label Korean, predict Chinese, result: False\\ninput Oishi, label Japanese, predict Japanese, result: True\\ninput Baidin, label Russian, predict Russian, result: True\\ninput Pavlyuchkov, label Russian, predict Russian, result: True\\ninput Sarkis, label Arabic, predict Arabic, result: True\\ninput Kirwin, label English, predict English, result: True\\ninput Pokhis, label Russian, predict Russian, result: True\\ninput Granitov, label Russian, predict Russian, result: True\\ninput Wirner, label German, predict German, result: True\\ninput Jamburg, label Russian, predict Russian, result: True\\ninput Lowe, label German, predict English, result: False\\ninput Nespola, label Italian, predict Italian, result: True\\ninput Maclean, label Scottish, predict English, result: False\\ninput St martin, label French, predict French, result: True\\ninput Langer, label German, predict English, result: False\\ninput Shaikin, label Russian, predict Russian, result: True\\ninput Podsevalov, label Russian, predict Russian, result: True\\ninput Romano, label Italian, predict Italian, result: True\\ninput Hadad, label Arabic, predict Arabic, result: True\\ninput Hoshino, label Japanese, predict Japanese, result: True\\ninput Morra, label Italian, predict Italian, result: True\\ninput Ashwell, label English, predict English, result: True\\ninput Awad, label Arabic, predict Arabic, result: True\\ninput Okubo, label Japanese, predict Japanese, result: True\\ninput Ametistov, label Russian, predict Russian, result: True\\ninput Jachmenev, label Russian, predict Russian, result: True\\ninput Lodygin, label Russian, predict Russian, result: True\\ninput Vikulov, label Russian, predict Russian, result: True\\ninput Prokoshin, label Russian, predict Russian, result: True\\ninput Deulin, label Russian, predict Russian, result: True\\ninput Zhdankov, label Russian, predict Russian, result: True\\ninput Antyufeev, label Russian, predict Russian, result: True\\ninput Babayan, label Russian, predict Russian, result: True\\ninput Khouri, label Arabic, predict Arabic, result: True\\ninput Dizhbak, label Russian, predict Russian, result: True\\ninput Ogterop, label Dutch, predict Dutch, result: True\\ninput Minitsky, label Russian, predict Russian, result: True\\ninput Dobrynsky, label Russian, predict Russian, result: True\\ninput Mikheev, label Russian, predict Russian, result: True\\ninput Kelly, label Irish, predict English, result: False\\ninput Martoyas, label Russian, predict Greek, result: False\\ninput Koning, label Dutch, predict German, result: False\\ninput Manfredi, label Italian, predict Italian, result: True\\ninput Bakh, label Russian, predict Russian, result: True\\ninput Foran, label English, predict English, result: True\\ninput Nelson, label English, predict English, result: True\\ninput Hapitsky, label Russian, predict Russian, result: True\\ninput To The First Page, label Russian, predict Russian, result: True\\ninput Basara, label Arabic, predict Arabic, result: True\\ninput Coffey, label English, predict English, result: True\\ninput Kenyon, label English, predict English, result: True\\ninput Ichisada, label Japanese, predict Japanese, result: True\\ninput Alves, label Portuguese, predict Spanish, result: False\\ninput Awdiysky, label Russian, predict Russian, result: True\\ninput Lecce, label Italian, predict Italian, result: True\\ninput Gosselin, label French, predict French, result: True\\ninput Rettig, label German, predict German, result: True\\ninput Hudoshin, label Russian, predict Russian, result: True\\ninput Yushkevich, label Russian, predict Russian, result: True\\ninput Anderson, label Scottish, predict English, result: False\\ninput Bassin, label Russian, predict Russian, result: True\\ninput Bavilin, label Russian, predict Russian, result: True\\ninput Altshuler, label Russian, predict German, result: False\\ninput Hafizov, label Russian, predict Russian, result: True\\ninput Holland, label English, predict English, result: True\\ninput Lodyjensky, label Russian, predict Russian, result: True\\ninput Matsura, label Japanese, predict Japanese, result: True\\ninput Kalb, label Arabic, predict Arabic, result: True\\ninput Daniau, label French, predict French, result: True\\ninput Napoletani, label Italian, predict Italian, result: True\\ninput Renov, label Russian, predict Russian, result: True\\ninput Traversini, label Italian, predict Italian, result: True\\ninput Ba, label Arabic, predict Arabic, result: True\\ninput Nakamura, label Japanese, predict Japanese, result: True\\ninput Gimondi, label Italian, predict Italian, result: True\\ninput Lohanov, label Russian, predict Russian, result: True\\ninput Lezhenko, label Russian, predict Russian, result: True\\n\\n```\\n\\n# The source code \\n\\nThe soure code can be found on [github](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/4.char-rnn.py)\"},\"fields\":{\"fpath\":\"/series/pytorch/rnn/2-charnn/\",\"permalink\":\"/2018/09/10/character-level-rnn\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Character-Level RNN\",\"date\":\"2018-09-10\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null,\"slug\":null}}},\"1-Fundamental\":{\"post\":{\"internal\":{\"content\":\"\\n\\n# python3\\n* star operator\\n* map zip lambda\\n* NDArray Indexing\\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\\n\\n# pytorch\\n\\n\\n## RNN\\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\\n\\nAndrej Karpathy’s diagram shows the different pattern in RNN        \\n![RNN](rnn.jpg)\\n\\n### Sequence \\nThe following code show the concept about sequence. \\ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.ones(4, 1, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: step by step\\ninput_0 = input[0,:,:].view(1,1,10)\\ninput_1 = input[1,:,:].view(1,1,10)\\ninput_2 = input[2,:,:].view(1,1,10)\\ninput_3 = input[3,:,:].view(1,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1, hidden_0)\\noutput_2, hidden_2 = model(input_2, hidden_1)\\noutput_3, hidden_3 = model(input_3, hidden_2)\\n\\n\\nprint(hidden)\\nprint(output)\\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\\nprint(output_0, output_1, output_2,output_3)\\n\\n\\n# compare option1 & option2\\nprint ((output[0]==output_0).sum().item() == hidden_size)\\nprint ((output[1]==output_1).sum().item() == hidden_size)\\nprint ((output[2]==output_2).sum().item() == hidden_size)\\nprint ((output[3]==output_3).sum().item() == hidden_size)\\n\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n# relation between hidden & output\\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n```\\n\\nAs the result of the above code shown   \\n1. the output contains all outputs of each iteration.\\n2. the output is the collection of hidden state of each iteration\\n3. from the last layer of the LSTM (in much layer network, see the official document)\\n\\n\\n### Batch Processing\\nIn fact, pytorch handle data in batch.  \\nIt's more quickly, and save time.\\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.randn(4, 4, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: one by one\\ninput_0 = input[:, 0, :].view(4,1,10)\\ninput_1 = input[:, 1, :].view(4,1,10)\\ninput_2 = input[:, 2, :].view(4,1,10)\\ninput_3 = input[:, 3, :].view(4,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1)\\noutput_2, hidden_2 = model(input_2)\\noutput_3, hidden_3 = model(input_3)\\n\\n#compare\\nprint((output[-1][0]- output_0[-1][0]).sum())\\nprint((output[-1][1]- output_1[-1][0]).sum())\\nprint((output[-1][2]- output_2[-1][0]).sum())\\nprint((output[-1][3]- output_3[-1][0]).sum())\\n\\\"\\\"\\\"\\ntensor(8.1956e-08, grad_fn=<SumBackward0>)\\ntensor(3.2596e-09, grad_fn=<SumBackward0>)\\ntensor(9.1270e-08, grad_fn=<SumBackward0>)\\ntensor(5.1223e-08, grad_fn=<SumBackward0>)\\n\\\"\\\"\\\"\\n\\n```\\nThe above code process inputs in batch. \\nOutput of shape is (seq_len, batch, num_directions * hidden_size)   \\nWe can get the output according to the seq and batch    \\noutput[i,j,:]: get the i iteration output of the sample j   \\noutput[-1,j,:] get the last output of the sample j  \\n\\n### Batch processing with variable length sequences\\nMany real cases need to handle variable length sequences.\\nE.g. in nlp, the sentence length is variable, the character count of a word is variable.\\n\\nPytorch introduce several helper functions to handle this.\\n\\nhelper functions\\n* torch.nn.utils.rnn.pack_sequence\\n* torch.nn.utils.rnn.pad_sequence\\n* torch.nn.utils.rnn.pad_packed_sequence\\n* torch.nn.utils.rnn.pack_padded_sequence\\n\\nhelper structure\\n* PackedSequence\\n  \\n```python\\nimport torch\\nimport numpy as np\\n\\nfrom torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\\n\\ninput_size = 2\\nhidden_size= 5\\nnum_layers = 1\\nnClasses = 10\\nnSamples = 10\\n\\na = torch.ones(3, input_size)\\nb = torch.ones(5, input_size)\\nc = torch.ones(7, input_size)\\n\\n# pad\\npad = pad_sequence([c,b,a])\\nprint(\\\"pad result\\\", pad.size())\\n\\n# pack\\npack = pack_sequence([c,b,a])\\nprint(\\\"pack result:\\\", pack.data.size(), pack.batch_sizes)\\n\\n# pack_padded\\npack_padded = pack_padded_sequence(pad, [7,5,3])\\nprint(\\\"pack_padded result:\\\", pack_padded.data.size(), pack_padded.batch_sizes)\\n\\n# pad_packed\\npad_packed_data, pad_packed_lengths = pad_packed_sequence(pack)\\nprint(\\\"pad_packed result:\\\", pad_packed_data.size() ,pad_packed_lengths)\\n\\n# pattern\\n\\n\\\"\\\"\\\"\\nprepare data/model/indices\\n\\\"\\\"\\\"\\n\\n# data\\ninputs = []\\ntargets = []\\nfor idx in range(nSamples):\\n    # set random len of input , and set the len as target\\n    # input: ones(len, input_size)\\n    # target: len\\n    len = np.random.randint(nSamples)+1\\n    sample = torch.ones(len, input_size)\\n    inputs.append(sample)\\n    targets.append(len)\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\ndemo = torch.ones(10,1,  input_size)\\nprint(\\\"sample sequence result\\\", model(demo)[0])\\n\\n# indices\\nsample_length = [x.size(0) for x in inputs]\\n_, indices_sorted = torch.sort(torch.LongTensor(sample_length), descending=True)\\n_, indices_restore = torch.sort(indices_sorted)\\n\\nprint(\\\"sample length:\\\", sample_length)\\n\\n\\\"\\\"\\\"\\noption1:\\npre-process inputs\\nsort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)\\n\\ntargets <-> outputs  \\n\\\"\\\"\\\"\\nprint(\\\"option1\\\")\\n\\n \\n\\n# sort inputs\\ninputs_sorted = [inputs[x] for x in indices_sorted]\\n\\n# pack inputs\\npack = pack_sequence(inputs_sorted)\\n\\n# rnn ...\\noutputs, hidden = model(pack)\\n\\n# unpack\\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\\n\\n# unsort\\nunsorted_last_state = last_state[indices_restore,:]\\nprint([(tup[0].size(0), tup[1], tup[2]) for tup in   zip(inputs, targets, unsorted_last_state)])\\n\\n\\\"\\\"\\\"\\noption2 \\npre-process (inputs, targets)\\nsort (inputs, targets)-> pack(inputs) -> rnn -> unpack\\n\\ntargets(sorted) <--> outputs  \\n\\\"\\\"\\\"\\n\\nprint(\\\"option2\\\")\\nbatch = list(zip(inputs, targets))\\n\\n# sort inputs\\nbatch_sorted = [batch[x] for x in indices_sorted]\\n\\n# pack inputs\\npack = pack_sequence([tup[0] for tup in batch_sorted])\\n\\n# rnn ...\\noutputs, hidden = model(pack)\\n\\n# unpack\\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\\n\\nprint([(tup[0][0].size(0), tup[0][1], tup[1]) for tup in zip(batch_sorted, last_state)])\\n\\n\\\"\\\"\\\"\\npad result torch.Size([7, 3, 2])\\npack result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\\npack_padded result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\\npad_packed result: torch.Size([7, 3, 2]) tensor([7, 5, 3])\\nsample sequence result tensor(\\n\\t\\t[[[ 0.0121,  0.0403, -0.0511, -0.0392,  0.2119]],\\n        [[ 0.0203,  0.0604, -0.0728, -0.0546,  0.2866]],\\n        [[ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169]],\\n        [[ 0.0271,  0.0784, -0.0860, -0.0653,  0.3302]],\\n        [[ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362]],\\n        [[ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390]],\\n        [[ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403]],\\n        [[ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409]],\\n        [[ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412]],\\n        [[ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413]]],\\n       grad_fn=<CatBackward>)\\nsample length: [9, 7, 10, 8, 6, 8, 5, 3, 5, 5]\\noption1\\n[(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \\n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \\n(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), 0\\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>))]\\n\\noption2\\n[(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \\n(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>))]\\n\\\"\\\"\\\"\\n```\\nThe sample sequence result show the result of sample[1,1,1,1,1,1,1,1,1,1], so we can get the output of each iteration .     \\nIn option 1: the outputs' order are restored, and it's the same as the orgin data           \\nIn option 2: the outputs' order are sorted(not restored), and it's the same as the sorted data      \\n\\n# The source code\\nThe soure code can be found on github\\n\\n[basic](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/1.basic.py) \\n\\n[batch](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/2.batch.py) \\n\\n[variable length](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/3.variable%20length.py)   \\n\\n# refs\\n\\nhttps://www.pythonlikeyoumeanit.com/intro.html\\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558\"},\"fields\":{\"fpath\":\"/series/pytorch/rnn/1-Fundamental/\",\"permalink\":\"/2018/09/07/fundamental\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Fundamental\",\"date\":\"2018-09-07\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null,\"slug\":null}}}},\"post\":{\"internal\":{\"content\":\"\\n# RNN\\n\\n \"},\"fields\":{\"fpath\":\"/series/pytorch/rnn/\",\"permalink\":\"/2018/09/07/rnn\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"RNN\",\"date\":\"2018-09-07\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null,\"slug\":null}}}},\"post\":{\"internal\":{\"content\":\"\\n# Pytorch\"},\"fields\":{\"fpath\":\"/series/pytorch/\",\"permalink\":\"/2018/09/07/pytorch\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Pytorch\",\"date\":\"2018-09-07\",\"tags\":[\"machine learning\",\"pytorch\"],\"desc\":null,\"slug\":null}}}","next":{"internal":{"content":"\n# Introduction\nThe pytorch's official tutorial introduces the character-level RNN\nhttps://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n\nHere I will implement it by using lstm\n\n# Points\n1. Classifying Names is a kind of many-to-one RNN task\n2. The Inputs' length are variable  \n3. It's better to handle the data in batch to boost the training\n4. Use dataset and dataloader to handle the raw data\n   \n\n# Task definition\nTraining data is collection of (name, language)\nThe task is to preidct the language of the given name\n\n# Details\n\n## Raw data\nDownload data from https://download.pytorch.org/tutorial/data.zip\n\n## Unicode & ascii\nplease read my previous post about [unicode](https://zhoumingjun.github.io/note/unicode)    \nwe will transform the names into NFD (Normalization Form Decomposition) and filter all non-ascii characters\n\n## Dataset and dataloader\nThe dataset is a helper class to load data.\n\n```python\nclass NamesDataset(Dataset):\n    def __init__(self, data_dir, transforms=[]):\n        self.data_dir = data_dir\n\n        all_langs = []\n        inputs = []\n        labels = []\n\n        for filepath in glob.glob(path.join(data_dir, \"*\")):\n            lang = os.path.splitext(os.path.basename(filepath))[0]\n            if not lang in all_langs:\n                all_langs.append(lang)\n\n            label = all_langs.index(lang)\n\n            with open(filepath) as f:\n                lines = f.readlines()\n                inputs += [line.strip() for line in lines]\n                labels += [label] * len(lines)\n\n        self.all_langs = all_langs\n        self.inputs = inputs\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n\n        item = self.inputs[idx]\n        for transform in self.transforms:\n            item = transform(item)\n\n        return item, self.labels[idx]\n\n    def getLangs(self):\n        return self.all_langs\n```\n\n## Model\nThe model is straitforward.\n1. use lstm to compute the output\n2. use linear to map the output's feature to language classes\n3. use crossentrophy(log_softmax + NLLLoss) to do classification\n   \n*attention, here we use packed sequece as the input* \n\n```python\nclass NamesClassifier(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(NamesClassifier, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\n        self.linear = torch.nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x, hidden = self.lstm(x)\n        output_unpacked, unpack_outputs_length = pad_packed_sequence(x)\n\n        seqs = unpack_outputs_length - 1\n        batch = [x for x in range(len(unpack_outputs_length))]\n        last_state = output_unpacked[seqs, batch, :].view(-1, self.hidden_size)\n\n        x = self.linear(last_state)\n        return F.log_softmax(x, dim=1)\n```\n\n## Training\nHere we follow the pattern sort-> pack -> rnn to train the model in batch \nrefer to [fundamental](https://zhoumingjun.github.io/series/pytorch/rnn/1-Fundamental/)\n\n```python\nfor epoch in range(10):\n    \n    # train\n    loss_sum = 0;\n    nRound = 0\n    for i_batch, batch in enumerate(dataloader):\n\n        # zero\n        optimizer.zero_grad()\n\n        inputs, labels = batch\n        # pre-process\n        inputs = [name2tensor(name) for name in inputs]\n\n        inputs_length = [x.size(0) for x in inputs]\n        _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\n        _, indices_restore = torch.sort(indices_sorted)\n\n        # sort\n        inputs_sorted = [inputs[x] for x in indices_sorted]\n        labels_sorted = labels[indices_sorted]\n\n        # pack inputs\n        pack = pack_sequence(inputs_sorted)\n\n        # rnn\n        outputs = model(pack)\n\n        # loss/bp/step\n        loss = criterion(outputs, labels_sorted)\n\n        loss.backward()\n        optimizer.step()\n\n        loss_sum += loss\n        nRound += 1\n        if i_batch % 50 == 0:\n            print(\"epoch {} i_batch {} loss {}\".format(epoch, i_batch, loss_sum / nRound))\n            \n    # validate\n    with torch.no_grad():\n        acc = 0\n        for i_batch, batch in enumerate(dataloader):\n            inputs, labels = batch\n            # pre-process\n            inputs = [name2tensor(name) for name in inputs]\n\n            inputs_length = [x.size(0) for x in inputs]\n            _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\n            _, indices_restore = torch.sort(indices_sorted)\n\n            # sort\n            inputs_sorted = [inputs[x] for x in indices_sorted]\n            labels_sorted = labels[indices_sorted]\n\n            # pack inputs\n            pack = pack_sequence(inputs_sorted)\n\n            # rnn\n            outputs = model(pack)\n\n            top_v, topi = torch.topk(outputs, 1)\n            acc += (topi.view(1, -1) == labels_sorted).sum().item()\n```\n\nHere is the training output\n\n```\nepoch 0 i_batch 0 loss 2.942514419555664\nepoch 0 i_batch 50 loss 1.760701298713684\nepoch 0 i_batch 100 loss 1.5069572925567627\nepoch 0 i_batch 150 loss 1.3581050634384155\nepoch 0 acc:14758/20074 \nepoch 1 i_batch 0 loss 0.8439935445785522\nepoch 1 i_batch 50 loss 0.8764864802360535\nepoch 1 i_batch 100 loss 0.8679843544960022\nepoch 1 i_batch 150 loss 0.836237907409668\nepoch 1 acc:15769/20074 \nepoch 2 i_batch 0 loss 0.6725496649742126\nepoch 2 i_batch 50 loss 0.6784597635269165\nepoch 2 i_batch 100 loss 0.6801468729972839\nepoch 2 i_batch 150 loss 0.6713106632232666\nepoch 2 acc:16419/20074 \nepoch 3 i_batch 0 loss 0.7285385131835938\nepoch 3 i_batch 50 loss 0.5921918749809265\nepoch 3 i_batch 100 loss 0.5902780294418335\nepoch 3 i_batch 150 loss 0.5864360928535461\nepoch 3 acc:16829/20074 \nepoch 4 i_batch 0 loss 0.5711963176727295\nepoch 4 i_batch 50 loss 0.5279501676559448\nepoch 4 i_batch 100 loss 0.521149218082428\nepoch 4 i_batch 150 loss 0.5309893488883972\nepoch 4 acc:17058/20074 \nepoch 5 i_batch 0 loss 0.5366474390029907\nepoch 5 i_batch 50 loss 0.46456900238990784\nepoch 5 i_batch 100 loss 0.47316044569015503\nepoch 5 i_batch 150 loss 0.4669898450374603\nepoch 5 acc:17373/20074 \nepoch 6 i_batch 0 loss 0.4217735528945923\nepoch 6 i_batch 50 loss 0.4200931489467621\nepoch 6 i_batch 100 loss 0.43043628334999084\nepoch 6 i_batch 150 loss 0.4396715462207794\nepoch 6 acc:17447/20074 \nepoch 7 i_batch 0 loss 0.31758126616477966\nepoch 7 i_batch 50 loss 0.38675254583358765\nepoch 7 i_batch 100 loss 0.399675190448761\nepoch 7 i_batch 150 loss 0.4018913507461548\nepoch 7 acc:17867/20074 \nepoch 8 i_batch 0 loss 0.3606557548046112\nepoch 8 i_batch 50 loss 0.33634153008461\nepoch 8 i_batch 100 loss 0.34979167580604553\nepoch 8 i_batch 150 loss 0.36078399419784546\nepoch 8 acc:18013/20074 \nepoch 9 i_batch 0 loss 0.31942570209503174\nepoch 9 i_batch 50 loss 0.30287066102027893\nepoch 9 i_batch 100 loss 0.31636151671409607\nepoch 9 i_batch 150 loss 0.33173689246177673\nepoch 9 acc:18151/20074 \n```\n\n## Predict\n\nAfter training, we can predict names' language using the model.\n``` python\n# do some preidct\nfor i_batch, batch in enumerate(dataloader):\n    input, label = batch\n    for idx, input in enumerate(input):\n        lang, lang_id = predict(input)\n        print(\"input {}, label {}, predict {}, result: {}\".format(\n            input, all_langs[label[idx].item()],\n            lang,\n            lang_id == label[idx].item()))\n\n    break\n```\n\nHere is the result\n```\ninput Purse, label English, predict English, result: True\ninput Teague, label Irish, predict English, result: False\ninput Daher, label Arabic, predict Arabic, result: True\ninput Soma, label Japanese, predict Japanese, result: True\ninput Mikhail, label Arabic, predict Arabic, result: True\ninput Pawluk, label Russian, predict Russian, result: True\ninput Bakaleinikov, label Russian, predict Russian, result: True\ninput Zhaivoronok, label Russian, predict Russian, result: True\ninput Fujiwara, label Japanese, predict Japanese, result: True\ninput Jachikov, label Russian, predict Russian, result: True\ninput Sardelis, label Greek, predict Greek, result: True\ninput Yatzenko, label Russian, predict Russian, result: True\ninput Kabalevsky, label Russian, predict Russian, result: True\ninput Godo, label Japanese, predict Japanese, result: True\ninput Imran, label English, predict English, result: True\ninput Kokkali, label Greek, predict Greek, result: True\ninput Adam, label Russian, predict English, result: False\ninput Todorov, label Russian, predict Russian, result: True\ninput Bastian, label Russian, predict Russian, result: True\ninput Samson, label French, predict French, result: True\ninput Bereznitsky, label Russian, predict Russian, result: True\ninput Bakhmetiev, label Russian, predict Russian, result: True\ninput Hadad, label Arabic, predict Arabic, result: True\ninput Sung, label Korean, predict Chinese, result: False\ninput Oishi, label Japanese, predict Japanese, result: True\ninput Baidin, label Russian, predict Russian, result: True\ninput Pavlyuchkov, label Russian, predict Russian, result: True\ninput Sarkis, label Arabic, predict Arabic, result: True\ninput Kirwin, label English, predict English, result: True\ninput Pokhis, label Russian, predict Russian, result: True\ninput Granitov, label Russian, predict Russian, result: True\ninput Wirner, label German, predict German, result: True\ninput Jamburg, label Russian, predict Russian, result: True\ninput Lowe, label German, predict English, result: False\ninput Nespola, label Italian, predict Italian, result: True\ninput Maclean, label Scottish, predict English, result: False\ninput St martin, label French, predict French, result: True\ninput Langer, label German, predict English, result: False\ninput Shaikin, label Russian, predict Russian, result: True\ninput Podsevalov, label Russian, predict Russian, result: True\ninput Romano, label Italian, predict Italian, result: True\ninput Hadad, label Arabic, predict Arabic, result: True\ninput Hoshino, label Japanese, predict Japanese, result: True\ninput Morra, label Italian, predict Italian, result: True\ninput Ashwell, label English, predict English, result: True\ninput Awad, label Arabic, predict Arabic, result: True\ninput Okubo, label Japanese, predict Japanese, result: True\ninput Ametistov, label Russian, predict Russian, result: True\ninput Jachmenev, label Russian, predict Russian, result: True\ninput Lodygin, label Russian, predict Russian, result: True\ninput Vikulov, label Russian, predict Russian, result: True\ninput Prokoshin, label Russian, predict Russian, result: True\ninput Deulin, label Russian, predict Russian, result: True\ninput Zhdankov, label Russian, predict Russian, result: True\ninput Antyufeev, label Russian, predict Russian, result: True\ninput Babayan, label Russian, predict Russian, result: True\ninput Khouri, label Arabic, predict Arabic, result: True\ninput Dizhbak, label Russian, predict Russian, result: True\ninput Ogterop, label Dutch, predict Dutch, result: True\ninput Minitsky, label Russian, predict Russian, result: True\ninput Dobrynsky, label Russian, predict Russian, result: True\ninput Mikheev, label Russian, predict Russian, result: True\ninput Kelly, label Irish, predict English, result: False\ninput Martoyas, label Russian, predict Greek, result: False\ninput Koning, label Dutch, predict German, result: False\ninput Manfredi, label Italian, predict Italian, result: True\ninput Bakh, label Russian, predict Russian, result: True\ninput Foran, label English, predict English, result: True\ninput Nelson, label English, predict English, result: True\ninput Hapitsky, label Russian, predict Russian, result: True\ninput To The First Page, label Russian, predict Russian, result: True\ninput Basara, label Arabic, predict Arabic, result: True\ninput Coffey, label English, predict English, result: True\ninput Kenyon, label English, predict English, result: True\ninput Ichisada, label Japanese, predict Japanese, result: True\ninput Alves, label Portuguese, predict Spanish, result: False\ninput Awdiysky, label Russian, predict Russian, result: True\ninput Lecce, label Italian, predict Italian, result: True\ninput Gosselin, label French, predict French, result: True\ninput Rettig, label German, predict German, result: True\ninput Hudoshin, label Russian, predict Russian, result: True\ninput Yushkevich, label Russian, predict Russian, result: True\ninput Anderson, label Scottish, predict English, result: False\ninput Bassin, label Russian, predict Russian, result: True\ninput Bavilin, label Russian, predict Russian, result: True\ninput Altshuler, label Russian, predict German, result: False\ninput Hafizov, label Russian, predict Russian, result: True\ninput Holland, label English, predict English, result: True\ninput Lodyjensky, label Russian, predict Russian, result: True\ninput Matsura, label Japanese, predict Japanese, result: True\ninput Kalb, label Arabic, predict Arabic, result: True\ninput Daniau, label French, predict French, result: True\ninput Napoletani, label Italian, predict Italian, result: True\ninput Renov, label Russian, predict Russian, result: True\ninput Traversini, label Italian, predict Italian, result: True\ninput Ba, label Arabic, predict Arabic, result: True\ninput Nakamura, label Japanese, predict Japanese, result: True\ninput Gimondi, label Italian, predict Italian, result: True\ninput Lohanov, label Russian, predict Russian, result: True\ninput Lezhenko, label Russian, predict Russian, result: True\n\n```\n\n# The source code \n\nThe soure code can be found on [github](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/4.char-rnn.py)"},"fields":{"fpath":"/series/pytorch/rnn/2-charnn/","permalink":"/2018/09/10/character-level-rnn","category":"series"},"frontmatter":{"title":"Character-Level RNN","date":"2018-09-10","tags":["machine learning","pytorch","rnn"],"desc":null,"slug":null}},"prev":{"internal":{"content":"\n# RNN\n\n "},"fields":{"fpath":"/series/pytorch/rnn/","permalink":"/2018/09/07/rnn","category":"series"},"frontmatter":{"title":"RNN","date":"2018-09-07","tags":["machine learning","pytorch","rnn"],"desc":null,"slug":null}}}}