{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","sourceUrl":"https://github.com/zhoumingjun/zhoumingjun.github.io/blob/source/","siteUrl":"https://zhoumingjun.github.io","disqusShortname":"zhoumingjun"}},"markdownRemark":{"id":"9167e02e-b6f0-54ab-9292-0b7b7676f2c5","html":"<h1>Introduction</h1>\n<p>The pytorch’s official tutorial introduces the character-level RNN\n<a href=\"https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\">https://pytorch.org/tutorials/intermediate/char<em>rnn</em>classification_tutorial.html</a></p>\n<p>Here I will implement it by using lstm</p>\n<h1>Basic Points</h1>\n<ol>\n<li>\n<p>Classifying Names is a kind of many-to-one RNN task</p>\n</li>\n<li>\n<p>The Inputs’ length are variable  </p>\n</li>\n<li>\n<p>It’s better to handle the data in batch to boost the training</p>\n</li>\n<li>\n<p>Use dataset and dataloader to handle the raw data</p>\n</li>\n</ol>\n<h1>Details</h1>\n<h2>Raw data</h2>\n<p>Download data from <a href=\"https://download.pytorch.org/tutorial/data.zip\">https://download.pytorch.org/tutorial/data.zip</a></p>\n<h2>Unicode &#x26; ascii</h2>\n<p>please read my previous post about <a href=\"https://zhoumingjun.github.io/note/unicode\">unicode</a><br>\nwe will transform the names into NFD (Normalization Form Decomposition) and filter all non-ascii characters</p>\n<h2>Dataset and dataloader</h2>\n<p>The dataset is a helper class to load data.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">NamesDataset</span><span class=\"token punctuation\">(</span>Dataset<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> data_dir<span class=\"token punctuation\">,</span> transforms<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        self<span class=\"token punctuation\">.</span>data_dir <span class=\"token operator\">=</span> data_dir\n\n        all_langs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        inputs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        labels <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n\n        <span class=\"token keyword\">for</span> filepath <span class=\"token keyword\">in</span> glob<span class=\"token punctuation\">.</span>glob<span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>data_dir<span class=\"token punctuation\">,</span> <span class=\"token string\">\"*\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            lang <span class=\"token operator\">=</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>splitext<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>basename<span class=\"token punctuation\">(</span>filepath<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n            <span class=\"token keyword\">if</span> <span class=\"token operator\">not</span> lang <span class=\"token keyword\">in</span> all_langs<span class=\"token punctuation\">:</span>\n                all_langs<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>lang<span class=\"token punctuation\">)</span>\n\n            label <span class=\"token operator\">=</span> all_langs<span class=\"token punctuation\">.</span>index<span class=\"token punctuation\">(</span>lang<span class=\"token punctuation\">)</span>\n\n            <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>filepath<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n                lines <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>readlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n                inputs <span class=\"token operator\">+=</span> <span class=\"token punctuation\">[</span>line<span class=\"token punctuation\">.</span>strip<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> lines<span class=\"token punctuation\">]</span>\n                labels <span class=\"token operator\">+=</span> <span class=\"token punctuation\">[</span>label<span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>lines<span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>all_langs <span class=\"token operator\">=</span> all_langs\n        self<span class=\"token punctuation\">.</span>inputs <span class=\"token operator\">=</span> inputs\n        self<span class=\"token punctuation\">.</span>labels <span class=\"token operator\">=</span> labels\n        self<span class=\"token punctuation\">.</span>transforms <span class=\"token operator\">=</span> transforms\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__len__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>inputs<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__getitem__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n        item <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>inputs<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> transform <span class=\"token keyword\">in</span> self<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">:</span>\n            item <span class=\"token operator\">=</span> transform<span class=\"token punctuation\">(</span>item<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> item<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>labels<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">]</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">getLangs</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>all_langs</code></pre></div>\n<h2>Model</h2>\n<p>The model is straitforward.</p>\n<ol>\n<li>\n<p>use lstm to compute the output</p>\n</li>\n<li>\n<p>use linear to map the output’s feature to language classes</p>\n</li>\n<li>\n<p>use crossentrophy(log_softmax + NLLLoss) to do classification</p>\n</li>\n</ol>\n<p><em>attention, here we use packed sequece as the input</em> </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">NamesClassifier</span><span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>NamesClassifier<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>input_size <span class=\"token operator\">=</span> input_size\n        self<span class=\"token punctuation\">.</span>hidden_size <span class=\"token operator\">=</span> hidden_size\n        self<span class=\"token punctuation\">.</span>output_size <span class=\"token operator\">=</span> output_size\n\n        self<span class=\"token punctuation\">.</span>lstm <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>LSTM<span class=\"token punctuation\">(</span>input_size<span class=\"token punctuation\">,</span> hidden_size<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>linear <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_size<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x<span class=\"token punctuation\">,</span> hidden <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>lstm<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        output_unpacked<span class=\"token punctuation\">,</span> unpack_outputs_length <span class=\"token operator\">=</span> pad_packed_sequence<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n\n        seqs <span class=\"token operator\">=</span> unpack_outputs_length <span class=\"token operator\">-</span> <span class=\"token number\">1</span>\n        batch <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>unpack_outputs_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n        last_state <span class=\"token operator\">=</span> output_unpacked<span class=\"token punctuation\">[</span>seqs<span class=\"token punctuation\">,</span> batch<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">.</span>hidden_size<span class=\"token punctuation\">)</span>\n\n        x <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>linear<span class=\"token punctuation\">(</span>last_state<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> F<span class=\"token punctuation\">.</span>log_softmax<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Training</h2>\n<p>Here we follow the pattern sort-> pack -> rnn to train the model in batch\nrefer to <a href=\"https://zhoumingjun.github.io/series/pytorch/rnn/1-Fundamental/\">fundamental</a></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    \n    <span class=\"token comment\"># train</span>\n    loss_sum <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n    nRound <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">for</span> i_batch<span class=\"token punctuation\">,</span> batch <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n        <span class=\"token comment\"># zero</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        inputs<span class=\"token punctuation\">,</span> labels <span class=\"token operator\">=</span> batch\n        <span class=\"token comment\"># pre-process</span>\n        inputs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>name2tensor<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> name <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">]</span>\n\n        inputs_length <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">]</span>\n        _<span class=\"token punctuation\">,</span> indices_sorted <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span>inputs_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> descending<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n        _<span class=\"token punctuation\">,</span> indices_restore <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>indices_sorted<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># sort</span>\n        inputs_sorted <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>inputs<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> indices_sorted<span class=\"token punctuation\">]</span>\n        labels_sorted <span class=\"token operator\">=</span> labels<span class=\"token punctuation\">[</span>indices_sorted<span class=\"token punctuation\">]</span>\n\n        <span class=\"token comment\"># pack inputs</span>\n        pack <span class=\"token operator\">=</span> pack_sequence<span class=\"token punctuation\">(</span>inputs_sorted<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># rnn</span>\n        outputs <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>pack<span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># loss/bp/step</span>\n        loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> labels_sorted<span class=\"token punctuation\">)</span>\n\n        loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        loss_sum <span class=\"token operator\">+=</span> loss\n        nRound <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">if</span> i_batch <span class=\"token operator\">%</span> <span class=\"token number\">50</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"epoch {} i_batch {} loss {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>epoch<span class=\"token punctuation\">,</span> i_batch<span class=\"token punctuation\">,</span> loss_sum <span class=\"token operator\">/</span> nRound<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            \n    <span class=\"token comment\"># validate</span>\n    <span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        acc <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        <span class=\"token keyword\">for</span> i_batch<span class=\"token punctuation\">,</span> batch <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            inputs<span class=\"token punctuation\">,</span> labels <span class=\"token operator\">=</span> batch\n            <span class=\"token comment\"># pre-process</span>\n            inputs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>name2tensor<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> name <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">]</span>\n\n            inputs_length <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> inputs<span class=\"token punctuation\">]</span>\n            _<span class=\"token punctuation\">,</span> indices_sorted <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span>inputs_length<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> descending<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n            _<span class=\"token punctuation\">,</span> indices_restore <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sort<span class=\"token punctuation\">(</span>indices_sorted<span class=\"token punctuation\">)</span>\n\n            <span class=\"token comment\"># sort</span>\n            inputs_sorted <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>inputs<span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> indices_sorted<span class=\"token punctuation\">]</span>\n            labels_sorted <span class=\"token operator\">=</span> labels<span class=\"token punctuation\">[</span>indices_sorted<span class=\"token punctuation\">]</span>\n\n            <span class=\"token comment\"># pack inputs</span>\n            pack <span class=\"token operator\">=</span> pack_sequence<span class=\"token punctuation\">(</span>inputs_sorted<span class=\"token punctuation\">)</span>\n\n            <span class=\"token comment\"># rnn</span>\n            outputs <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>pack<span class=\"token punctuation\">)</span>\n\n            top_v<span class=\"token punctuation\">,</span> topi <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>topk<span class=\"token punctuation\">(</span>outputs<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n            acc <span class=\"token operator\">+=</span> <span class=\"token punctuation\">(</span>topi<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> labels_sorted<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">sum</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Here is the training output</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">epoch 0 i_batch 0 loss 2.9128077030181885\nepoch 0 i_batch 50 loss 1.6704312562942505\nepoch 0 i_batch 100 loss 1.440101981163025\nepoch 0 i_batch 150 loss 1.3050810098648071\nepoch 0 acc:12182/16406 \nepoch 1 i_batch 0 loss 0.8428565859794617\nepoch 1 i_batch 50 loss 0.8336539268493652\nepoch 1 i_batch 100 loss 0.7949255108833313\nepoch 1 i_batch 150 loss 0.7666547894477844\nepoch 1 acc:13227/16406 \nepoch 2 i_batch 0 loss 0.6228170990943909\nepoch 2 i_batch 50 loss 0.6274971961975098\nepoch 2 i_batch 100 loss 0.6237800121307373\nepoch 2 i_batch 150 loss 0.6122254729270935\nepoch 2 acc:13737/16406 \nepoch 3 i_batch 0 loss 0.5636349320411682\nepoch 3 i_batch 50 loss 0.5250101685523987\nepoch 3 i_batch 100 loss 0.5150720477104187\nepoch 3 i_batch 150 loss 0.5117108225822449\nepoch 3 acc:14008/16406 \nepoch 4 i_batch 0 loss 0.48472052812576294\nepoch 4 i_batch 50 loss 0.44357186555862427\nepoch 4 i_batch 100 loss 0.4550507664680481\nepoch 4 i_batch 150 loss 0.4460631012916565\nepoch 4 acc:14387/16406 \nepoch 5 i_batch 0 loss 0.225796639919281\nepoch 5 i_batch 50 loss 0.39754441380500793\nepoch 5 i_batch 100 loss 0.4006364941596985\nepoch 5 i_batch 150 loss 0.4059876799583435\nepoch 5 acc:14484/16406 \nepoch 6 i_batch 0 loss 0.36545485258102417\nepoch 6 i_batch 50 loss 0.3650299608707428\nepoch 6 i_batch 100 loss 0.3646186292171478\nepoch 6 i_batch 150 loss 0.36661288142204285\nepoch 6 acc:14699/16406 \nepoch 7 i_batch 0 loss 0.42295753955841064\nepoch 7 i_batch 50 loss 0.3185514807701111\nepoch 7 i_batch 100 loss 0.3292921483516693\nepoch 7 i_batch 150 loss 0.3317612409591675\nepoch 7 acc:14859/16406 \nepoch 8 i_batch 0 loss 0.40569090843200684\nepoch 8 i_batch 50 loss 0.28691983222961426\nepoch 8 i_batch 100 loss 0.2941666543483734\nepoch 8 i_batch 150 loss 0.30278101563453674\nepoch 8 acc:14966/16406 \nepoch 9 i_batch 0 loss 0.24612517654895782\nepoch 9 i_batch 50 loss 0.2607012093067169\nepoch 9 i_batch 100 loss 0.2631416916847229\nepoch 9 i_batch 150 loss 0.27081555128097534\nepoch 9 acc:15141/16406 </code></pre></div>\n<h2>Predict</h2>\n<p>After training, we can predict names’ language using the model.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># do some preidct</span>\n<span class=\"token keyword\">for</span> i_batch<span class=\"token punctuation\">,</span> batch <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>dataloader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token builtin\">input</span><span class=\"token punctuation\">,</span> label <span class=\"token operator\">=</span> batch\n    <span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> <span class=\"token builtin\">input</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        lang<span class=\"token punctuation\">,</span> lang_id <span class=\"token operator\">=</span> predict<span class=\"token punctuation\">(</span><span class=\"token builtin\">input</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"input {}, label {}, predict {}, result: {}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>\n            <span class=\"token builtin\">input</span><span class=\"token punctuation\">,</span> all_langs<span class=\"token punctuation\">[</span>label<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            lang<span class=\"token punctuation\">,</span>\n            lang_id <span class=\"token operator\">==</span> label<span class=\"token punctuation\">[</span>idx<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">break</span></code></pre></div>\n<p>Here is the result</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>","fileAbsolutePath":"/home/zhoumingjun/github.com/zhoumingjun/zhoumingjun.github.io/content/series/pytorch/rnn/2-charnn/index.md","fields":{"slug":"/series/pytorch/rnn/2-charnn/","category":"series"},"frontmatter":{"title":"Character-Level RNN","date":"September 10, 2018"}}},"pageContext":{"slug":"/series/pytorch/rnn/2-charnn/","toc":"{\"children\":{\"rnn\":{\"children\":{\"2-charnn\":{\"post\":{\"internal\":{\"content\":\"\\n# Introduction\\nThe pytorch's official tutorial introduces the character-level RNN\\nhttps://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\\n\\nHere I will implement it by using lstm\\n\\n# Basic Points\\n1. Classifying Names is a kind of many-to-one RNN task\\n2. The Inputs' length are variable  \\n3. It's better to handle the data in batch to boost the training\\n4. Use dataset and dataloader to handle the raw data\\n   \\n\\n# Details\\n\\n## Raw data\\nDownload data from https://download.pytorch.org/tutorial/data.zip\\n\\n## Unicode & ascii\\nplease read my previous post about [unicode](https://zhoumingjun.github.io/note/unicode)    \\nwe will transform the names into NFD (Normalization Form Decomposition) and filter all non-ascii characters\\n\\n## Dataset and dataloader\\nThe dataset is a helper class to load data.\\n\\n```python\\nclass NamesDataset(Dataset):\\n    def __init__(self, data_dir, transforms=[]):\\n        self.data_dir = data_dir\\n\\n        all_langs = []\\n        inputs = []\\n        labels = []\\n\\n        for filepath in glob.glob(path.join(data_dir, \\\"*\\\")):\\n            lang = os.path.splitext(os.path.basename(filepath))[0]\\n            if not lang in all_langs:\\n                all_langs.append(lang)\\n\\n            label = all_langs.index(lang)\\n\\n            with open(filepath) as f:\\n                lines = f.readlines()\\n                inputs += [line.strip() for line in lines]\\n                labels += [label] * len(lines)\\n\\n        self.all_langs = all_langs\\n        self.inputs = inputs\\n        self.labels = labels\\n        self.transforms = transforms\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n\\n        item = self.inputs[idx]\\n        for transform in self.transforms:\\n            item = transform(item)\\n\\n        return item, self.labels[idx]\\n\\n    def getLangs(self):\\n        return self.all_langs\\n```\\n\\n## Model\\nThe model is straitforward.\\n1. use lstm to compute the output\\n2. use linear to map the output's feature to language classes\\n3. use crossentrophy(log_softmax + NLLLoss) to do classification\\n   \\n*attention, here we use packed sequece as the input* \\n\\n```python\\nclass NamesClassifier(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super(NamesClassifier, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_size = output_size\\n\\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\\n        self.linear = torch.nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, x):\\n        x, hidden = self.lstm(x)\\n        output_unpacked, unpack_outputs_length = pad_packed_sequence(x)\\n\\n        seqs = unpack_outputs_length - 1\\n        batch = [x for x in range(len(unpack_outputs_length))]\\n        last_state = output_unpacked[seqs, batch, :].view(-1, self.hidden_size)\\n\\n        x = self.linear(last_state)\\n        return F.log_softmax(x, dim=1)\\n```\\n\\n## Training\\nHere we follow the pattern sort-> pack -> rnn to train the model in batch \\nrefer to [fundamental](https://zhoumingjun.github.io/series/pytorch/rnn/1-Fundamental/)\\n\\n```python\\nfor epoch in range(10):\\n    \\n    # train\\n    loss_sum = 0;\\n    nRound = 0\\n    for i_batch, batch in enumerate(dataloader):\\n\\n        # zero\\n        optimizer.zero_grad()\\n\\n        inputs, labels = batch\\n        # pre-process\\n        inputs = [name2tensor(name) for name in inputs]\\n\\n        inputs_length = [x.size(0) for x in inputs]\\n        _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\\n        _, indices_restore = torch.sort(indices_sorted)\\n\\n        # sort\\n        inputs_sorted = [inputs[x] for x in indices_sorted]\\n        labels_sorted = labels[indices_sorted]\\n\\n        # pack inputs\\n        pack = pack_sequence(inputs_sorted)\\n\\n        # rnn\\n        outputs = model(pack)\\n\\n        # loss/bp/step\\n        loss = criterion(outputs, labels_sorted)\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        loss_sum += loss\\n        nRound += 1\\n        if i_batch % 50 == 0:\\n            print(\\\"epoch {} i_batch {} loss {}\\\".format(epoch, i_batch, loss_sum / nRound))\\n            \\n    # validate\\n    with torch.no_grad():\\n        acc = 0\\n        for i_batch, batch in enumerate(dataloader):\\n            inputs, labels = batch\\n            # pre-process\\n            inputs = [name2tensor(name) for name in inputs]\\n\\n            inputs_length = [x.size(0) for x in inputs]\\n            _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\\n            _, indices_restore = torch.sort(indices_sorted)\\n\\n            # sort\\n            inputs_sorted = [inputs[x] for x in indices_sorted]\\n            labels_sorted = labels[indices_sorted]\\n\\n            # pack inputs\\n            pack = pack_sequence(inputs_sorted)\\n\\n            # rnn\\n            outputs = model(pack)\\n\\n            top_v, topi = torch.topk(outputs, 1)\\n            acc += (topi.view(1, -1) == labels_sorted).sum().item()\\n```\\n\\nHere is the training output\\n```\\nepoch 0 i_batch 0 loss 2.9128077030181885\\nepoch 0 i_batch 50 loss 1.6704312562942505\\nepoch 0 i_batch 100 loss 1.440101981163025\\nepoch 0 i_batch 150 loss 1.3050810098648071\\nepoch 0 acc:12182/16406 \\nepoch 1 i_batch 0 loss 0.8428565859794617\\nepoch 1 i_batch 50 loss 0.8336539268493652\\nepoch 1 i_batch 100 loss 0.7949255108833313\\nepoch 1 i_batch 150 loss 0.7666547894477844\\nepoch 1 acc:13227/16406 \\nepoch 2 i_batch 0 loss 0.6228170990943909\\nepoch 2 i_batch 50 loss 0.6274971961975098\\nepoch 2 i_batch 100 loss 0.6237800121307373\\nepoch 2 i_batch 150 loss 0.6122254729270935\\nepoch 2 acc:13737/16406 \\nepoch 3 i_batch 0 loss 0.5636349320411682\\nepoch 3 i_batch 50 loss 0.5250101685523987\\nepoch 3 i_batch 100 loss 0.5150720477104187\\nepoch 3 i_batch 150 loss 0.5117108225822449\\nepoch 3 acc:14008/16406 \\nepoch 4 i_batch 0 loss 0.48472052812576294\\nepoch 4 i_batch 50 loss 0.44357186555862427\\nepoch 4 i_batch 100 loss 0.4550507664680481\\nepoch 4 i_batch 150 loss 0.4460631012916565\\nepoch 4 acc:14387/16406 \\nepoch 5 i_batch 0 loss 0.225796639919281\\nepoch 5 i_batch 50 loss 0.39754441380500793\\nepoch 5 i_batch 100 loss 0.4006364941596985\\nepoch 5 i_batch 150 loss 0.4059876799583435\\nepoch 5 acc:14484/16406 \\nepoch 6 i_batch 0 loss 0.36545485258102417\\nepoch 6 i_batch 50 loss 0.3650299608707428\\nepoch 6 i_batch 100 loss 0.3646186292171478\\nepoch 6 i_batch 150 loss 0.36661288142204285\\nepoch 6 acc:14699/16406 \\nepoch 7 i_batch 0 loss 0.42295753955841064\\nepoch 7 i_batch 50 loss 0.3185514807701111\\nepoch 7 i_batch 100 loss 0.3292921483516693\\nepoch 7 i_batch 150 loss 0.3317612409591675\\nepoch 7 acc:14859/16406 \\nepoch 8 i_batch 0 loss 0.40569090843200684\\nepoch 8 i_batch 50 loss 0.28691983222961426\\nepoch 8 i_batch 100 loss 0.2941666543483734\\nepoch 8 i_batch 150 loss 0.30278101563453674\\nepoch 8 acc:14966/16406 \\nepoch 9 i_batch 0 loss 0.24612517654895782\\nepoch 9 i_batch 50 loss 0.2607012093067169\\nepoch 9 i_batch 100 loss 0.2631416916847229\\nepoch 9 i_batch 150 loss 0.27081555128097534\\nepoch 9 acc:15141/16406 \\n```\\n\\n## Predict\\n\"},\"fields\":{\"slug\":\"/series/pytorch/rnn/2-charnn/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Character-Level RNN\",\"date\":\"2018-09-10T08:47:59Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}},\"1-Fundamental\":{\"post\":{\"internal\":{\"content\":\"\\n\\n# python3\\n* star operator\\n* map zip lambda\\n* NDArray Indexing\\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\\n\\n# pytorch\\n\\n\\n## RNN\\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\\n\\nAndrej Karpathy’s diagram shows the different pattern in RNN        \\n![RNN](rnn.jpg)\\n\\n### Sequence \\nThe following code show the concept about sequence. \\ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.ones(4, 1, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: step by step\\ninput_0 = input[0,:,:].view(1,1,10)\\ninput_1 = input[1,:,:].view(1,1,10)\\ninput_2 = input[2,:,:].view(1,1,10)\\ninput_3 = input[3,:,:].view(1,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1, hidden_0)\\noutput_2, hidden_2 = model(input_2, hidden_1)\\noutput_3, hidden_3 = model(input_3, hidden_2)\\n\\n\\nprint(hidden)\\nprint(output)\\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\\nprint(output_0, output_1, output_2,output_3)\\n\\n\\n# compare option1 & option2\\nprint ((output[0]==output_0).sum().item() == hidden_size)\\nprint ((output[1]==output_1).sum().item() == hidden_size)\\nprint ((output[2]==output_2).sum().item() == hidden_size)\\nprint ((output[3]==output_3).sum().item() == hidden_size)\\n\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n# relation between hidden & output\\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n```\\n\\nAs the result of the above code shown   \\n1. the output contains all outputs of each iteration.\\n2. the output is the collection of hidden state of each iteration\\n3. from the last layer of the LSTM (in much layer network, see the official document)\\n\\n\\n### Batch Processing\\nIn fact, pytorch handle data in batch.  \\nIt's more quickly, and save time.\\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.randn(4, 4, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: one by one\\ninput_0 = input[:, 0, :].view(4,1,10)\\ninput_1 = input[:, 1, :].view(4,1,10)\\ninput_2 = input[:, 2, :].view(4,1,10)\\ninput_3 = input[:, 3, :].view(4,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1)\\noutput_2, hidden_2 = model(input_2)\\noutput_3, hidden_3 = model(input_3)\\n\\n#compare\\nprint((output[-1][0]- output_0[-1][0]).sum())\\nprint((output[-1][1]- output_1[-1][0]).sum())\\nprint((output[-1][2]- output_2[-1][0]).sum())\\nprint((output[-1][3]- output_3[-1][0]).sum())\\n\\\"\\\"\\\"\\ntensor(8.1956e-08, grad_fn=<SumBackward0>)\\ntensor(3.2596e-09, grad_fn=<SumBackward0>)\\ntensor(9.1270e-08, grad_fn=<SumBackward0>)\\ntensor(5.1223e-08, grad_fn=<SumBackward0>)\\n\\\"\\\"\\\"\\n\\n```\\nThe above code process inputs in batch. \\nOutput of shape is (seq_len, batch, num_directions * hidden_size)   \\nWe can get the output according to the seq and batch    \\noutput[i,j,:]: get the i iteration output of the sample j   \\noutput[-1,j,:] get the last output of the sample j  \\n\\n### Batch processing with variable length sequences\\nMany real cases need to handle variable length sequences.\\nE.g. in nlp, the sentence length is variable, the character count of a word is variable.\\n\\nPytorch introduce several helper functions to handle this.\\n\\nhelper functions\\n* torch.nn.utils.rnn.pack_sequence\\n* torch.nn.utils.rnn.pad_sequence\\n* torch.nn.utils.rnn.pad_packed_sequence\\n* torch.nn.utils.rnn.pack_padded_sequence\\n\\nhelper structure\\n* PackedSequence\\n  \\n```python\\nimport torch\\nimport numpy as np\\n\\nfrom torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\\n\\ninput_size = 2\\nhidden_size= 5\\nnum_layers = 1\\nnClasses = 10\\nnSamples = 10\\n\\na = torch.ones(3, input_size)\\nb = torch.ones(5, input_size)\\nc = torch.ones(7, input_size)\\n\\n# pad\\npad = pad_sequence([c,b,a])\\nprint(\\\"pad result\\\", pad.size())\\n\\n# pack\\npack = pack_sequence([c,b,a])\\nprint(\\\"pack result:\\\", pack.data.size(), pack.batch_sizes)\\n\\n# pack_padded\\npack_padded = pack_padded_sequence(pad, [7,5,3])\\nprint(\\\"pack_padded result:\\\", pack_padded.data.size(), pack_padded.batch_sizes)\\n\\n# pad_packed\\npad_packed_data, pad_packed_lengths = pad_packed_sequence(pack)\\nprint(\\\"pad_packed result:\\\", pad_packed_data.size() ,pad_packed_lengths)\\n\\n# pattern\\n\\n\\\"\\\"\\\"\\nprepare data/model/indices\\n\\\"\\\"\\\"\\n\\n# data\\ninputs = []\\ntargets = []\\nfor idx in range(nSamples):\\n    # set random len of input , and set the len as target\\n    # input: ones(len, input_size)\\n    # target: len\\n    len = np.random.randint(nSamples)+1\\n    sample = torch.ones(len, input_size)\\n    inputs.append(sample)\\n    targets.append(len)\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\ndemo = torch.ones(10,1,  input_size)\\nprint(\\\"sample sequence result\\\", model(demo)[0])\\n\\n# indices\\nsample_length = [x.size(0) for x in inputs]\\n_, indices_sorted = torch.sort(torch.LongTensor(sample_length), descending=True)\\n_, indices_restore = torch.sort(indices_sorted)\\n\\nprint(\\\"sample length:\\\", sample_length)\\n\\n\\\"\\\"\\\"\\noption1:\\npre-process inputs\\nsort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)\\n\\ntargets <-> outputs  \\n\\\"\\\"\\\"\\nprint(\\\"option1\\\")\\n\\n \\n\\n# sort inputs\\ninputs_sorted = [inputs[x] for x in indices_sorted]\\n\\n# pack inputs\\npack = pack_sequence(inputs_sorted)\\n\\n# rnn ...\\noutputs, hidden = model(pack)\\n\\n# unpack\\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\\n\\n# unsort\\nunsorted_last_state = last_state[indices_restore,:]\\nprint([(tup[0].size(0), tup[1], tup[2]) for tup in   zip(inputs, targets, unsorted_last_state)])\\n\\n\\\"\\\"\\\"\\noption2 \\npre-process (inputs, targets)\\nsort (inputs, targets)-> pack(inputs) -> rnn -> unpack\\n\\ntargets(sorted) <--> outputs  \\n\\\"\\\"\\\"\\n\\nprint(\\\"option2\\\")\\nbatch = list(zip(inputs, targets))\\n\\n# sort inputs\\nbatch_sorted = [batch[x] for x in indices_sorted]\\n\\n# pack inputs\\npack = pack_sequence([tup[0] for tup in batch_sorted])\\n\\n# rnn ...\\noutputs, hidden = model(pack)\\n\\n# unpack\\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\\n\\nprint([(tup[0][0].size(0), tup[0][1], tup[1]) for tup in zip(batch_sorted, last_state)])\\n\\n\\\"\\\"\\\"\\npad result torch.Size([7, 3, 2])\\npack result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\\npack_padded result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\\npad_packed result: torch.Size([7, 3, 2]) tensor([7, 5, 3])\\nsample sequence result tensor(\\n\\t\\t[[[ 0.0121,  0.0403, -0.0511, -0.0392,  0.2119]],\\n        [[ 0.0203,  0.0604, -0.0728, -0.0546,  0.2866]],\\n        [[ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169]],\\n        [[ 0.0271,  0.0784, -0.0860, -0.0653,  0.3302]],\\n        [[ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362]],\\n        [[ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390]],\\n        [[ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403]],\\n        [[ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409]],\\n        [[ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412]],\\n        [[ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413]]],\\n       grad_fn=<CatBackward>)\\nsample length: [9, 7, 10, 8, 6, 8, 5, 3, 5, 5]\\noption1\\n[(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \\n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \\n(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), 0\\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>))]\\n\\noption2\\n[(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \\n(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>))]\\n\\\"\\\"\\\"\\n```\\nThe sample sequence result show the result of sample[1,1,1,1,1,1,1,1,1,1], so we can get the output of each iteration .     \\nIn option 1: the outputs' order are restored, and it's the same as the orgin data           \\nIn option 2: the outputs' order are sorted(not restored), and it's the same as the sorted data      \\n\\n# refs\\n\\nhttps://www.pythonlikeyoumeanit.com/intro.html\\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558\"},\"fields\":{\"slug\":\"/series/pytorch/rnn/1-Fundamental/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Fundamental\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}}},\"post\":{\"internal\":{\"content\":\"\\n# RNN\\n\\n \"},\"fields\":{\"slug\":\"/series/pytorch/rnn/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"RNN\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}}},\"post\":{\"internal\":{\"content\":\"\\n# Pytorch\"},\"fields\":{\"slug\":\"/series/pytorch/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Pytorch\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\"],\"desc\":null}}}","next":null,"prev":{"internal":{"content":"\n\n# python3\n* star operator\n* map zip lambda\n* NDArray Indexing\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\n\n# pytorch\n\n\n## RNN\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\n\nAndrej Karpathy’s diagram shows the different pattern in RNN        \n![RNN](rnn.jpg)\n\n### Sequence \nThe following code show the concept about sequence. \ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \n\n```python\nimport torch\n\ninput_size = 10\nhidden_size = 20\nnum_layers = 1\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\n\n# data\ninput = torch.ones(4, 1, 10)\n\n# option1: sequence\noutput, hidden = model(input)\n\n# option2: step by step\ninput_0 = input[0,:,:].view(1,1,10)\ninput_1 = input[1,:,:].view(1,1,10)\ninput_2 = input[2,:,:].view(1,1,10)\ninput_3 = input[3,:,:].view(1,1,10)\n\noutput_0, hidden_0 = model(input_0)\noutput_1, hidden_1 = model(input_1, hidden_0)\noutput_2, hidden_2 = model(input_2, hidden_1)\noutput_3, hidden_3 = model(input_3, hidden_2)\n\n\nprint(hidden)\nprint(output)\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\nprint(output_0, output_1, output_2,output_3)\n\n\n# compare option1 & option2\nprint ((output[0]==output_0).sum().item() == hidden_size)\nprint ((output[1]==output_1).sum().item() == hidden_size)\nprint ((output[2]==output_2).sum().item() == hidden_size)\nprint ((output[3]==output_3).sum().item() == hidden_size)\n\n\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"\n# relation between hidden & output\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\n\"\"\"\nTrue\nTrue\nTrue\nTrue\n\"\"\"\n```\n\nAs the result of the above code shown   \n1. the output contains all outputs of each iteration.\n2. the output is the collection of hidden state of each iteration\n3. from the last layer of the LSTM (in much layer network, see the official document)\n\n\n### Batch Processing\nIn fact, pytorch handle data in batch.  \nIt's more quickly, and save time.\n\n```python\nimport torch\n\ninput_size = 10\nhidden_size = 20\nnum_layers = 1\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\n\n# data\ninput = torch.randn(4, 4, 10)\n\n# option1: sequence\noutput, hidden = model(input)\n\n# option2: one by one\ninput_0 = input[:, 0, :].view(4,1,10)\ninput_1 = input[:, 1, :].view(4,1,10)\ninput_2 = input[:, 2, :].view(4,1,10)\ninput_3 = input[:, 3, :].view(4,1,10)\n\noutput_0, hidden_0 = model(input_0)\noutput_1, hidden_1 = model(input_1)\noutput_2, hidden_2 = model(input_2)\noutput_3, hidden_3 = model(input_3)\n\n#compare\nprint((output[-1][0]- output_0[-1][0]).sum())\nprint((output[-1][1]- output_1[-1][0]).sum())\nprint((output[-1][2]- output_2[-1][0]).sum())\nprint((output[-1][3]- output_3[-1][0]).sum())\n\"\"\"\ntensor(8.1956e-08, grad_fn=<SumBackward0>)\ntensor(3.2596e-09, grad_fn=<SumBackward0>)\ntensor(9.1270e-08, grad_fn=<SumBackward0>)\ntensor(5.1223e-08, grad_fn=<SumBackward0>)\n\"\"\"\n\n```\nThe above code process inputs in batch. \nOutput of shape is (seq_len, batch, num_directions * hidden_size)   \nWe can get the output according to the seq and batch    \noutput[i,j,:]: get the i iteration output of the sample j   \noutput[-1,j,:] get the last output of the sample j  \n\n### Batch processing with variable length sequences\nMany real cases need to handle variable length sequences.\nE.g. in nlp, the sentence length is variable, the character count of a word is variable.\n\nPytorch introduce several helper functions to handle this.\n\nhelper functions\n* torch.nn.utils.rnn.pack_sequence\n* torch.nn.utils.rnn.pad_sequence\n* torch.nn.utils.rnn.pad_packed_sequence\n* torch.nn.utils.rnn.pack_padded_sequence\n\nhelper structure\n* PackedSequence\n  \n```python\nimport torch\nimport numpy as np\n\nfrom torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n\ninput_size = 2\nhidden_size= 5\nnum_layers = 1\nnClasses = 10\nnSamples = 10\n\na = torch.ones(3, input_size)\nb = torch.ones(5, input_size)\nc = torch.ones(7, input_size)\n\n# pad\npad = pad_sequence([c,b,a])\nprint(\"pad result\", pad.size())\n\n# pack\npack = pack_sequence([c,b,a])\nprint(\"pack result:\", pack.data.size(), pack.batch_sizes)\n\n# pack_padded\npack_padded = pack_padded_sequence(pad, [7,5,3])\nprint(\"pack_padded result:\", pack_padded.data.size(), pack_padded.batch_sizes)\n\n# pad_packed\npad_packed_data, pad_packed_lengths = pad_packed_sequence(pack)\nprint(\"pad_packed result:\", pad_packed_data.size() ,pad_packed_lengths)\n\n# pattern\n\n\"\"\"\nprepare data/model/indices\n\"\"\"\n\n# data\ninputs = []\ntargets = []\nfor idx in range(nSamples):\n    # set random len of input , and set the len as target\n    # input: ones(len, input_size)\n    # target: len\n    len = np.random.randint(nSamples)+1\n    sample = torch.ones(len, input_size)\n    inputs.append(sample)\n    targets.append(len)\n\n# model\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\ndemo = torch.ones(10,1,  input_size)\nprint(\"sample sequence result\", model(demo)[0])\n\n# indices\nsample_length = [x.size(0) for x in inputs]\n_, indices_sorted = torch.sort(torch.LongTensor(sample_length), descending=True)\n_, indices_restore = torch.sort(indices_sorted)\n\nprint(\"sample length:\", sample_length)\n\n\"\"\"\noption1:\npre-process inputs\nsort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)\n\ntargets <-> outputs  \n\"\"\"\nprint(\"option1\")\n\n \n\n# sort inputs\ninputs_sorted = [inputs[x] for x in indices_sorted]\n\n# pack inputs\npack = pack_sequence(inputs_sorted)\n\n# rnn ...\noutputs, hidden = model(pack)\n\n# unpack\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\n\n# unsort\nunsorted_last_state = last_state[indices_restore,:]\nprint([(tup[0].size(0), tup[1], tup[2]) for tup in   zip(inputs, targets, unsorted_last_state)])\n\n\"\"\"\noption2 \npre-process (inputs, targets)\nsort (inputs, targets)-> pack(inputs) -> rnn -> unpack\n\ntargets(sorted) <--> outputs  \n\"\"\"\n\nprint(\"option2\")\nbatch = list(zip(inputs, targets))\n\n# sort inputs\nbatch_sorted = [batch[x] for x in indices_sorted]\n\n# pack inputs\npack = pack_sequence([tup[0] for tup in batch_sorted])\n\n# rnn ...\noutputs, hidden = model(pack)\n\n# unpack\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\n\nprint([(tup[0][0].size(0), tup[0][1], tup[1]) for tup in zip(batch_sorted, last_state)])\n\n\"\"\"\npad result torch.Size([7, 3, 2])\npack result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\npack_padded result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\npad_packed result: torch.Size([7, 3, 2]) tensor([7, 5, 3])\nsample sequence result tensor(\n\t\t[[[ 0.0121,  0.0403, -0.0511, -0.0392,  0.2119]],\n        [[ 0.0203,  0.0604, -0.0728, -0.0546,  0.2866]],\n        [[ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169]],\n        [[ 0.0271,  0.0784, -0.0860, -0.0653,  0.3302]],\n        [[ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362]],\n        [[ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390]],\n        [[ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403]],\n        [[ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409]],\n        [[ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412]],\n        [[ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413]]],\n       grad_fn=<CatBackward>)\nsample length: [9, 7, 10, 8, 6, 8, 5, 3, 5, 5]\noption1\n[(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \n(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), 0\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>))]\n\noption2\n[(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \n(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>))]\n\"\"\"\n```\nThe sample sequence result show the result of sample[1,1,1,1,1,1,1,1,1,1], so we can get the output of each iteration .     \nIn option 1: the outputs' order are restored, and it's the same as the orgin data           \nIn option 2: the outputs' order are sorted(not restored), and it's the same as the sorted data      \n\n# refs\n\nhttps://www.pythonlikeyoumeanit.com/intro.html\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558"},"fields":{"slug":"/series/pytorch/rnn/1-Fundamental/","category":"series"},"frontmatter":{"title":"Fundamental","date":"2018-09-07T10:16:47Z","tags":["machine learning","pytorch","rnn"],"desc":null}}}}