{"data":{"site":{"siteMetadata":{"title":"Mingjun Zhou's blog","author":"Mingjun Zhou","sourceUrl":"https://github.com/zhoumingjun/zhoumingjun.github.io/blob/source/","siteUrl":"https://zhoumingjun.github.io","disqusShortname":"zhoumingjun"}},"markdownRemark":{"id":"7eb33909-ca3d-5f8d-9833-9118852cbe23","html":"<h1>Pytorch</h1>","fileAbsolutePath":"/home/zhoumingjun/github.com/zhoumingjun/zhoumingjun.github.io/content/series/pytorch/index.md","fields":{"slug":"/series/pytorch/","category":"series"},"frontmatter":{"title":"Pytorch","date":"September 07, 2018"}}},"pageContext":{"slug":"/series/pytorch/","toc":"{\"children\":{\"rnn\":{\"children\":{\"2-charnn\":{\"post\":{\"internal\":{\"content\":\"\\n# Introduction\\nThe pytorch's official tutorial introduces the character-level RNN\\nhttps://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\\n\\nHere I will implement it by using lstm\\n\\n# Basic Points\\n1. Classifying Names is a kind of many-to-one RNN task\\n2. The Inputs' length are variable  \\n3. It's better to handle the data in batch to boost the training\\n4. Use dataset and dataloader to handle the raw data\\n   \\n\\n# Details\\n\\n## Raw data\\nDownload data from https://download.pytorch.org/tutorial/data.zip\\n\\n## Unicode & ascii\\nplease read my previous post about [unicode](https://zhoumingjun.github.io/note/unicode)    \\nwe will transform the names into NFD (Normalization Form Decomposition) and filter all non-ascii characters\\n\\n## Dataset and dataloader\\nThe dataset is a helper class to load data.\\n\\n```python\\nclass NamesDataset(Dataset):\\n    def __init__(self, data_dir, transforms=[]):\\n        self.data_dir = data_dir\\n\\n        all_langs = []\\n        inputs = []\\n        labels = []\\n\\n        for filepath in glob.glob(path.join(data_dir, \\\"*\\\")):\\n            lang = os.path.splitext(os.path.basename(filepath))[0]\\n            if not lang in all_langs:\\n                all_langs.append(lang)\\n\\n            label = all_langs.index(lang)\\n\\n            with open(filepath) as f:\\n                lines = f.readlines()\\n                inputs += [line.strip() for line in lines]\\n                labels += [label] * len(lines)\\n\\n        self.all_langs = all_langs\\n        self.inputs = inputs\\n        self.labels = labels\\n        self.transforms = transforms\\n\\n    def __len__(self):\\n        return len(self.inputs)\\n\\n    def __getitem__(self, idx):\\n\\n        item = self.inputs[idx]\\n        for transform in self.transforms:\\n            item = transform(item)\\n\\n        return item, self.labels[idx]\\n\\n    def getLangs(self):\\n        return self.all_langs\\n```\\n\\n## Model\\nThe model is straitforward.\\n1. use lstm to compute the output\\n2. use linear to map the output's feature to language classes\\n3. use crossentrophy(log_softmax + NLLLoss) to do classification\\n   \\n*attention, here we use packed sequece as the input* \\n\\n```python\\nclass NamesClassifier(torch.nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super(NamesClassifier, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_size = output_size\\n\\n        self.lstm = torch.nn.LSTM(input_size, hidden_size, 1)\\n        self.linear = torch.nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, x):\\n        x, hidden = self.lstm(x)\\n        output_unpacked, unpack_outputs_length = pad_packed_sequence(x)\\n\\n        seqs = unpack_outputs_length - 1\\n        batch = [x for x in range(len(unpack_outputs_length))]\\n        last_state = output_unpacked[seqs, batch, :].view(-1, self.hidden_size)\\n\\n        x = self.linear(last_state)\\n        return F.log_softmax(x, dim=1)\\n```\\n\\n## Training\\nHere we follow the pattern sort-> pack -> rnn to train the model in batch \\nrefer to [fundamental](https://zhoumingjun.github.io/series/pytorch/rnn/1-Fundamental/)\\n\\n```python\\nfor epoch in range(10):\\n    \\n    # train\\n    loss_sum = 0;\\n    nRound = 0\\n    for i_batch, batch in enumerate(dataloader):\\n\\n        # zero\\n        optimizer.zero_grad()\\n\\n        inputs, labels = batch\\n        # pre-process\\n        inputs = [name2tensor(name) for name in inputs]\\n\\n        inputs_length = [x.size(0) for x in inputs]\\n        _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\\n        _, indices_restore = torch.sort(indices_sorted)\\n\\n        # sort\\n        inputs_sorted = [inputs[x] for x in indices_sorted]\\n        labels_sorted = labels[indices_sorted]\\n\\n        # pack inputs\\n        pack = pack_sequence(inputs_sorted)\\n\\n        # rnn\\n        outputs = model(pack)\\n\\n        # loss/bp/step\\n        loss = criterion(outputs, labels_sorted)\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        loss_sum += loss\\n        nRound += 1\\n        if i_batch % 50 == 0:\\n            print(\\\"epoch {} i_batch {} loss {}\\\".format(epoch, i_batch, loss_sum / nRound))\\n            \\n    # validate\\n    with torch.no_grad():\\n        acc = 0\\n        for i_batch, batch in enumerate(dataloader):\\n            inputs, labels = batch\\n            # pre-process\\n            inputs = [name2tensor(name) for name in inputs]\\n\\n            inputs_length = [x.size(0) for x in inputs]\\n            _, indices_sorted = torch.sort(torch.LongTensor(inputs_length), descending=True)\\n            _, indices_restore = torch.sort(indices_sorted)\\n\\n            # sort\\n            inputs_sorted = [inputs[x] for x in indices_sorted]\\n            labels_sorted = labels[indices_sorted]\\n\\n            # pack inputs\\n            pack = pack_sequence(inputs_sorted)\\n\\n            # rnn\\n            outputs = model(pack)\\n\\n            top_v, topi = torch.topk(outputs, 1)\\n            acc += (topi.view(1, -1) == labels_sorted).sum().item()\\n```\\n\\nHere is the training output\\n\\n```\\nepoch 0 i_batch 0 loss 2.9128077030181885\\nepoch 0 i_batch 50 loss 1.6704312562942505\\nepoch 0 i_batch 100 loss 1.440101981163025\\nepoch 0 i_batch 150 loss 1.3050810098648071\\nepoch 0 acc:12182/16406 \\nepoch 1 i_batch 0 loss 0.8428565859794617\\nepoch 1 i_batch 50 loss 0.8336539268493652\\nepoch 1 i_batch 100 loss 0.7949255108833313\\nepoch 1 i_batch 150 loss 0.7666547894477844\\nepoch 1 acc:13227/16406 \\nepoch 2 i_batch 0 loss 0.6228170990943909\\nepoch 2 i_batch 50 loss 0.6274971961975098\\nepoch 2 i_batch 100 loss 0.6237800121307373\\nepoch 2 i_batch 150 loss 0.6122254729270935\\nepoch 2 acc:13737/16406 \\nepoch 3 i_batch 0 loss 0.5636349320411682\\nepoch 3 i_batch 50 loss 0.5250101685523987\\nepoch 3 i_batch 100 loss 0.5150720477104187\\nepoch 3 i_batch 150 loss 0.5117108225822449\\nepoch 3 acc:14008/16406 \\nepoch 4 i_batch 0 loss 0.48472052812576294\\nepoch 4 i_batch 50 loss 0.44357186555862427\\nepoch 4 i_batch 100 loss 0.4550507664680481\\nepoch 4 i_batch 150 loss 0.4460631012916565\\nepoch 4 acc:14387/16406 \\nepoch 5 i_batch 0 loss 0.225796639919281\\nepoch 5 i_batch 50 loss 0.39754441380500793\\nepoch 5 i_batch 100 loss 0.4006364941596985\\nepoch 5 i_batch 150 loss 0.4059876799583435\\nepoch 5 acc:14484/16406 \\nepoch 6 i_batch 0 loss 0.36545485258102417\\nepoch 6 i_batch 50 loss 0.3650299608707428\\nepoch 6 i_batch 100 loss 0.3646186292171478\\nepoch 6 i_batch 150 loss 0.36661288142204285\\nepoch 6 acc:14699/16406 \\nepoch 7 i_batch 0 loss 0.42295753955841064\\nepoch 7 i_batch 50 loss 0.3185514807701111\\nepoch 7 i_batch 100 loss 0.3292921483516693\\nepoch 7 i_batch 150 loss 0.3317612409591675\\nepoch 7 acc:14859/16406 \\nepoch 8 i_batch 0 loss 0.40569090843200684\\nepoch 8 i_batch 50 loss 0.28691983222961426\\nepoch 8 i_batch 100 loss 0.2941666543483734\\nepoch 8 i_batch 150 loss 0.30278101563453674\\nepoch 8 acc:14966/16406 \\nepoch 9 i_batch 0 loss 0.24612517654895782\\nepoch 9 i_batch 50 loss 0.2607012093067169\\nepoch 9 i_batch 100 loss 0.2631416916847229\\nepoch 9 i_batch 150 loss 0.27081555128097534\\nepoch 9 acc:15141/16406 \\n```\\n\\n## Predict\\n\\nAfter training, we can predict names' language using the model.\\n``` python\\n# do some preidct\\nfor i_batch, batch in enumerate(dataloader):\\n    input, label = batch\\n    for idx, input in enumerate(input):\\n        lang, lang_id = predict(input)\\n        print(\\\"input {}, label {}, predict {}, result: {}\\\".format(\\n            input, all_langs[label[idx].item()],\\n            lang,\\n            lang_id == label[idx].item()))\\n\\n    break\\n```\\n\\nHere is the result\\n```\\ninput Talypin, label Russian, predict Russian, result: True\\ninput Hamada, label Japanese, predict Japanese, result: True\\ninput Schermer, label Dutch, predict German, result: False\\ninput Kabachev, label Russian, predict Russian, result: True\\ninput Maroun, label Arabic, predict Arabic, result: True\\ninput Close, label Greek, predict Greek, result: True\\ninput Jabykin, label Russian, predict Russian, result: True\\ninput Handal, label Arabic, predict Arabic, result: True\\ninput Veltistov, label Russian, predict Russian, result: True\\ninput Pavlinsky, label Russian, predict Russian, result: True\\ninput Djumabaev, label Russian, predict Russian, result: True\\ninput Balavensky, label Russian, predict Russian, result: True\\ninput O'Meara, label Irish, predict Irish, result: True\\ninput Kalogeria, label Greek, predict Greek, result: True\\ninput Paisar, label Czech, predict Czech, result: True\\ninput Sfakianos, label Greek, predict Greek, result: True\\ninput Jeryapin, label Russian, predict Russian, result: True\\ninput Agoev, label Russian, predict Russian, result: True\\ninput Daher, label Arabic, predict Arabic, result: True\\ninput Barabolya, label Russian, predict Russian, result: True\\ninput Zasuhin, label Russian, predict Russian, result: True\\ninput Zhevlakov, label Russian, predict Russian, result: True\\ninput Babinoff, label Russian, predict Russian, result: True\\ninput Villevalde, label Russian, predict Italian, result: False\\ninput Tchartorizhsky, label Russian, predict Russian, result: True\\ninput Behmetiev, label Russian, predict Russian, result: True\\ninput Plank, label German, predict German, result: True\\ninput Almondinov, label Russian, predict Russian, result: True\\ninput Cuocco, label Italian, predict Italian, result: True\\ninput Pyanov, label Russian, predict Russian, result: True\\ninput Bakhmatov, label Russian, predict Russian, result: True\\ninput Nizhegorodov, label Russian, predict Russian, result: True\\ninput Basara, label Arabic, predict Arabic, result: True\\ninput Haitsin, label Russian, predict Russian, result: True\\ninput Agliullin, label Russian, predict Russian, result: True\\ninput Denzel, label German, predict Russian, result: False\\ninput Jivotovsky, label Russian, predict Russian, result: True\\ninput Lichman, label Russian, predict Russian, result: True\\ninput Awetyan, label Russian, predict Russian, result: True\\ninput Akutagawa, label Japanese, predict Japanese, result: True\\ninput Averkiev, label Russian, predict Russian, result: True\\ninput Luzzatto, label Italian, predict Italian, result: True\\ninput Homa, label Russian, predict Japanese, result: False\\ninput Vilonov, label Russian, predict Russian, result: True\\ninput Avalyan, label Russian, predict Russian, result: True\\ninput Jordan, label German, predict Russian, result: False\\ninput Xing, label Chinese, predict Chinese, result: True\\ninput Tulub, label Russian, predict Russian, result: True\\ninput Gluhov, label Russian, predict Russian, result: True\\ninput Korycan, label Czech, predict Czech, result: True\\ninput Matocha, label Czech, predict Czech, result: True\\ninput Pak, label Korean, predict Korean, result: True\\ninput Hamikoev, label Russian, predict Russian, result: True\\ninput Ichikawa, label Japanese, predict Japanese, result: True\\ninput Balasoglo, label Russian, predict Russian, result: True\\ninput Grossman, label Russian, predict Russian, result: True\\ninput Jaruev, label Russian, predict Russian, result: True\\ninput Gartner, label German, predict Russian, result: False\\ninput Likhodeev, label Russian, predict Russian, result: True\\ninput Bitar, label Arabic, predict Arabic, result: True\\ninput Caro, label Italian, predict Italian, result: True\\ninput Hou, label Chinese, predict Chinese, result: True\\ninput Zhulebin, label Russian, predict Russian, result: True\\ninput Marmazov, label Russian, predict Russian, result: True\\ninput Abdrahimov, label Russian, predict Russian, result: True\\ninput Mar, label Chinese, predict Chinese, result: True\\ninput Accursio, label Italian, predict Italian, result: True\\ninput Aliberti, label Italian, predict Italian, result: True\\ninput Raimondi, label Italian, predict Italian, result: True\\ninput Gulkevich, label Russian, predict Russian, result: True\\ninput Andruhovich, label Russian, predict Russian, result: True\\ninput Kataoka, label Japanese, predict Japanese, result: True\\ninput Adabash, label Russian, predict Russian, result: True\\ninput Zou, label Chinese, predict Chinese, result: True\\ninput Gudoshin, label Russian, predict Russian, result: True\\ninput Marievsky, label Russian, predict Russian, result: True\\ninput Jeltov, label Russian, predict Russian, result: True\\ninput Jukovets, label Russian, predict Russian, result: True\\ninput Hofer, label German, predict German, result: True\\ninput Elensky, label Russian, predict Russian, result: True\\ninput Comtois, label French, predict French, result: True\\ninput Saliba, label Arabic, predict Arabic, result: True\\ninput Deeb, label Arabic, predict Arabic, result: True\\ninput Otyaev, label Russian, predict Russian, result: True\\ninput Fadzaev, label Russian, predict Russian, result: True\\ninput Shalhoub, label Arabic, predict Arabic, result: True\\ninput Jalovenko, label Russian, predict Russian, result: True\\ninput Kruger, label German, predict German, result: True\\ninput Cham, label Arabic, predict Arabic, result: True\\ninput Rojo, label Spanish, predict Spanish, result: True\\ninput Zhen, label Chinese, predict Chinese, result: True\\ninput Lapkin, label Russian, predict Russian, result: True\\ninput Fakhoury, label Arabic, predict Arabic, result: True\\ninput Dehanov, label Russian, predict Russian, result: True\\ninput Grobivker, label Russian, predict Russian, result: True\\ninput Avinovitski, label Russian, predict Russian, result: True\\ninput Likutov, label Russian, predict Russian, result: True\\ninput Valkin, label Russian, predict Russian, result: True\\ninput Raikhert, label Russian, predict Russian, result: True\\ninput Andel, label Dutch, predict Dutch, result: True\\n```\\n\\n# The source code \\n\\nThe soure code can be found on [github](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/4.char-rnn.py)\"},\"fields\":{\"slug\":\"/series/pytorch/rnn/2-charnn/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Character-Level RNN\",\"date\":\"2018-09-10T08:47:59Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}},\"1-Fundamental\":{\"post\":{\"internal\":{\"content\":\"\\n\\n# python3\\n* star operator\\n* map zip lambda\\n* NDArray Indexing\\nhttps://docs.scipy.org/doc/numpy/user/basics.indexing.html\\n\\n# pytorch\\n\\n\\n## RNN\\nRefer to https://pytorch.org/docs/stable/nn.html#lstm\\n\\nAndrej Karpathy’s diagram shows the different pattern in RNN        \\n![RNN](rnn.jpg)\\n\\n### Sequence \\nThe following code show the concept about sequence. \\ntorch.nn.LSTM can handle the sequence automatically, but we can feed it step-by-step also.  \\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.ones(4, 1, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: step by step\\ninput_0 = input[0,:,:].view(1,1,10)\\ninput_1 = input[1,:,:].view(1,1,10)\\ninput_2 = input[2,:,:].view(1,1,10)\\ninput_3 = input[3,:,:].view(1,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1, hidden_0)\\noutput_2, hidden_2 = model(input_2, hidden_1)\\noutput_3, hidden_3 = model(input_3, hidden_2)\\n\\n\\nprint(hidden)\\nprint(output)\\nprint(hidden_0, hidden_1, hidden_2,hidden_3)\\nprint(output_0, output_1, output_2,output_3)\\n\\n\\n# compare option1 & option2\\nprint ((output[0]==output_0).sum().item() == hidden_size)\\nprint ((output[1]==output_1).sum().item() == hidden_size)\\nprint ((output[2]==output_2).sum().item() == hidden_size)\\nprint ((output[3]==output_3).sum().item() == hidden_size)\\n\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n# relation between hidden & output\\nprint ((output[0]==hidden_0[0][-1]).sum().item() == hidden_size)\\nprint ((output[1]==hidden_1[0][-1]).sum().item() == hidden_size)\\nprint ((output[2]==hidden_2[0][-1]).sum().item() == hidden_size)\\nprint ((output[3]==hidden_3[0][-1]).sum().item() == hidden_size)\\n\\\"\\\"\\\"\\nTrue\\nTrue\\nTrue\\nTrue\\n\\\"\\\"\\\"\\n```\\n\\nAs the result of the above code shown   \\n1. the output contains all outputs of each iteration.\\n2. the output is the collection of hidden state of each iteration\\n3. from the last layer of the LSTM (in much layer network, see the official document)\\n\\n\\n### Batch Processing\\nIn fact, pytorch handle data in batch.  \\nIt's more quickly, and save time.\\n\\n```python\\nimport torch\\n\\ninput_size = 10\\nhidden_size = 20\\nnum_layers = 1\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\n\\n# data\\ninput = torch.randn(4, 4, 10)\\n\\n# option1: sequence\\noutput, hidden = model(input)\\n\\n# option2: one by one\\ninput_0 = input[:, 0, :].view(4,1,10)\\ninput_1 = input[:, 1, :].view(4,1,10)\\ninput_2 = input[:, 2, :].view(4,1,10)\\ninput_3 = input[:, 3, :].view(4,1,10)\\n\\noutput_0, hidden_0 = model(input_0)\\noutput_1, hidden_1 = model(input_1)\\noutput_2, hidden_2 = model(input_2)\\noutput_3, hidden_3 = model(input_3)\\n\\n#compare\\nprint((output[-1][0]- output_0[-1][0]).sum())\\nprint((output[-1][1]- output_1[-1][0]).sum())\\nprint((output[-1][2]- output_2[-1][0]).sum())\\nprint((output[-1][3]- output_3[-1][0]).sum())\\n\\\"\\\"\\\"\\ntensor(8.1956e-08, grad_fn=<SumBackward0>)\\ntensor(3.2596e-09, grad_fn=<SumBackward0>)\\ntensor(9.1270e-08, grad_fn=<SumBackward0>)\\ntensor(5.1223e-08, grad_fn=<SumBackward0>)\\n\\\"\\\"\\\"\\n\\n```\\nThe above code process inputs in batch. \\nOutput of shape is (seq_len, batch, num_directions * hidden_size)   \\nWe can get the output according to the seq and batch    \\noutput[i,j,:]: get the i iteration output of the sample j   \\noutput[-1,j,:] get the last output of the sample j  \\n\\n### Batch processing with variable length sequences\\nMany real cases need to handle variable length sequences.\\nE.g. in nlp, the sentence length is variable, the character count of a word is variable.\\n\\nPytorch introduce several helper functions to handle this.\\n\\nhelper functions\\n* torch.nn.utils.rnn.pack_sequence\\n* torch.nn.utils.rnn.pad_sequence\\n* torch.nn.utils.rnn.pad_packed_sequence\\n* torch.nn.utils.rnn.pack_padded_sequence\\n\\nhelper structure\\n* PackedSequence\\n  \\n```python\\nimport torch\\nimport numpy as np\\n\\nfrom torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\\n\\ninput_size = 2\\nhidden_size= 5\\nnum_layers = 1\\nnClasses = 10\\nnSamples = 10\\n\\na = torch.ones(3, input_size)\\nb = torch.ones(5, input_size)\\nc = torch.ones(7, input_size)\\n\\n# pad\\npad = pad_sequence([c,b,a])\\nprint(\\\"pad result\\\", pad.size())\\n\\n# pack\\npack = pack_sequence([c,b,a])\\nprint(\\\"pack result:\\\", pack.data.size(), pack.batch_sizes)\\n\\n# pack_padded\\npack_padded = pack_padded_sequence(pad, [7,5,3])\\nprint(\\\"pack_padded result:\\\", pack_padded.data.size(), pack_padded.batch_sizes)\\n\\n# pad_packed\\npad_packed_data, pad_packed_lengths = pad_packed_sequence(pack)\\nprint(\\\"pad_packed result:\\\", pad_packed_data.size() ,pad_packed_lengths)\\n\\n# pattern\\n\\n\\\"\\\"\\\"\\nprepare data/model/indices\\n\\\"\\\"\\\"\\n\\n# data\\ninputs = []\\ntargets = []\\nfor idx in range(nSamples):\\n    # set random len of input , and set the len as target\\n    # input: ones(len, input_size)\\n    # target: len\\n    len = np.random.randint(nSamples)+1\\n    sample = torch.ones(len, input_size)\\n    inputs.append(sample)\\n    targets.append(len)\\n\\n# model\\nmodel = torch.nn.LSTM(input_size, hidden_size, num_layers)\\ndemo = torch.ones(10,1,  input_size)\\nprint(\\\"sample sequence result\\\", model(demo)[0])\\n\\n# indices\\nsample_length = [x.size(0) for x in inputs]\\n_, indices_sorted = torch.sort(torch.LongTensor(sample_length), descending=True)\\n_, indices_restore = torch.sort(indices_sorted)\\n\\nprint(\\\"sample length:\\\", sample_length)\\n\\n\\\"\\\"\\\"\\noption1:\\npre-process inputs\\nsort (inputs)-> pack(inputs) -> rnn -> unpack -> unsort(outputs)\\n\\ntargets <-> outputs  \\n\\\"\\\"\\\"\\nprint(\\\"option1\\\")\\n\\n \\n\\n# sort inputs\\ninputs_sorted = [inputs[x] for x in indices_sorted]\\n\\n# pack inputs\\npack = pack_sequence(inputs_sorted)\\n\\n# rnn ...\\noutputs, hidden = model(pack)\\n\\n# unpack\\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\\n\\n# unsort\\nunsorted_last_state = last_state[indices_restore,:]\\nprint([(tup[0].size(0), tup[1], tup[2]) for tup in   zip(inputs, targets, unsorted_last_state)])\\n\\n\\\"\\\"\\\"\\noption2 \\npre-process (inputs, targets)\\nsort (inputs, targets)-> pack(inputs) -> rnn -> unpack\\n\\ntargets(sorted) <--> outputs  \\n\\\"\\\"\\\"\\n\\nprint(\\\"option2\\\")\\nbatch = list(zip(inputs, targets))\\n\\n# sort inputs\\nbatch_sorted = [batch[x] for x in indices_sorted]\\n\\n# pack inputs\\npack = pack_sequence([tup[0] for tup in batch_sorted])\\n\\n# rnn ...\\noutputs, hidden = model(pack)\\n\\n# unpack\\noutput_unpacked, unpack_outputs_length = pad_packed_sequence(outputs)\\nlast_state = output_unpacked[unpack_outputs_length-1, [x for x in range(10)] ,:]\\n\\nprint([(tup[0][0].size(0), tup[0][1], tup[1]) for tup in zip(batch_sorted, last_state)])\\n\\n\\\"\\\"\\\"\\npad result torch.Size([7, 3, 2])\\npack result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\\npack_padded result: torch.Size([15, 2]) tensor([3, 3, 3, 2, 2, 1, 1])\\npad_packed result: torch.Size([7, 3, 2]) tensor([7, 5, 3])\\nsample sequence result tensor(\\n\\t\\t[[[ 0.0121,  0.0403, -0.0511, -0.0392,  0.2119]],\\n        [[ 0.0203,  0.0604, -0.0728, -0.0546,  0.2866]],\\n        [[ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169]],\\n        [[ 0.0271,  0.0784, -0.0860, -0.0653,  0.3302]],\\n        [[ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362]],\\n        [[ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390]],\\n        [[ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403]],\\n        [[ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409]],\\n        [[ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412]],\\n        [[ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413]]],\\n       grad_fn=<CatBackward>)\\nsample length: [9, 7, 10, 8, 6, 8, 5, 3, 5, 5]\\noption1\\n[(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \\n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \\n(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), 0\\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>))]\\n\\noption2\\n[(10, 10, tensor([ 0.0294,  0.0876, -0.0887, -0.0704,  0.3413], grad_fn=<SelectBackward>)), \\n(9, 9, tensor([ 0.0294,  0.0873, -0.0887, -0.0701,  0.3412], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(8, 8, tensor([ 0.0293,  0.0868, -0.0887, -0.0698,  0.3409], grad_fn=<SelectBackward>)), \\n(7, 7, tensor([ 0.0291,  0.0860, -0.0887, -0.0693,  0.3403], grad_fn=<SelectBackward>)), \\n(6, 6, tensor([ 0.0288,  0.0847, -0.0884, -0.0685,  0.3390], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(5, 5, tensor([ 0.0283,  0.0823, -0.0877, -0.0673,  0.3362], grad_fn=<SelectBackward>)), \\n(3, 3, tensor([ 0.0248,  0.0718, -0.0820, -0.0617,  0.3169], grad_fn=<SelectBackward>))]\\n\\\"\\\"\\\"\\n```\\nThe sample sequence result show the result of sample[1,1,1,1,1,1,1,1,1,1], so we can get the output of each iteration .     \\nIn option 1: the outputs' order are restored, and it's the same as the orgin data           \\nIn option 2: the outputs' order are sorted(not restored), and it's the same as the sorted data      \\n\\n# The source code\\nThe soure code can be found on github\\n\\n[basic](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/1.basic.py) \\n\\n[batch](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/2.batch.py) \\n\\n[variable length](https://github.com/zhoumingjun/pytorch_learning/blob/master/rnn/3.variable%20length.py)   \\n\\n# refs\\n\\nhttps://www.pythonlikeyoumeanit.com/intro.html\\n https://djosix.github.io/Variable-Sequence-Lengths-for-PyTorch-RNNs/\\n https://medium.com/understand-the-python/understanding-the-asterisk-of-python-8b9daaa4a558\"},\"fields\":{\"slug\":\"/series/pytorch/rnn/1-Fundamental/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Fundamental\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}}},\"post\":{\"internal\":{\"content\":\"\\n# RNN\\n\\n \"},\"fields\":{\"slug\":\"/series/pytorch/rnn/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"RNN\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\",\"rnn\"],\"desc\":null}}}},\"post\":{\"internal\":{\"content\":\"\\n# Pytorch\"},\"fields\":{\"slug\":\"/series/pytorch/\",\"category\":\"series\"},\"frontmatter\":{\"title\":\"Pytorch\",\"date\":\"2018-09-07T10:16:47Z\",\"tags\":[\"machine learning\",\"pytorch\"],\"desc\":null}}}","next":{"internal":{"content":"\n# RNN\n\n "},"fields":{"slug":"/series/pytorch/rnn/","category":"series"},"frontmatter":{"title":"RNN","date":"2018-09-07T10:16:47Z","tags":["machine learning","pytorch","rnn"],"desc":null}},"prev":null}}