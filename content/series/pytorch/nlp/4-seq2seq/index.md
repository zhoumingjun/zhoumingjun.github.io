---
title: "seq2seq network and attention"
date: "2018-09-13T08:36:32Z"
tags: ["machine learning","pytorch", "seq2seq", "attention"]
draft: true
---

# Introduction
This is following the pytorch tutorial article
> https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

And for more, read the papers that introduced these topics:
> [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
>  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/abs/1406.1078)
> [Sequence to Sequence Learning with Neural Networks](http://arxiv.org/abs/1409.3215)
> [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
> [A Neural Conversational Model](http://arxiv.org/abs/1506.05869)


# Theory

# Attention
http://phontron.com/class/nn4nlp2017/assets/slides/nn4nlp-09-attention.pdf

attention in pytorch
https://github.com/thomlake/pytorch-attention

https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb
 
# seq2seq
https://github.com/keon/seq2seq/blob/master/model.py

# DataSet 
1. http://www.manythings.org/anki/ 



